{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_from_scratch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOB52I+qV+Zxi4RVVdMDE7h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/rnn/rnn_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzjc2dlH2GsQ"
      },
      "source": [
        "# Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUVIbX8I4_gR"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from numpy.random import randint\n",
        "from collections import OrderedDict\n",
        "from torch.utils import data\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uzAyRHP2U7H"
      },
      "source": [
        "## Representing text as tokens\n",
        "\n",
        "Let's define our dataset samples $x \\in \\mathrm{R}^d$, where $d$ is the feature space dimension.\n",
        "\n",
        "With time sequences our data can be represented as $x \\in \\mathrm{R}^{t \\, \\times \\, d}$, where $t$ is the sequence length. \n",
        "This emphasises sequence dependence and that the samples along the sequence are not independent and identically distributed (i.i.d.).\n",
        "We will model functions as $\\mathrm{R}^{t \\, \\times \\, d} \\rightarrow \\mathrm{R}^c$, where $c$ is the amount of classes in the output.\n",
        "\n",
        "There are several ways to represent sequences. With text, the challenge is how to represent a word as a feature vector in $d$ dimensions, as we are required to represent text with decimal numbers in order to apply neural networks to it.\n",
        "\n",
        "Initially, we will use a simple one-hot encoding but for categorical variables that can take on many values (e.g. words in the English language)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-DRDHhQ3zs7"
      },
      "source": [
        "### One-hot encoding over vocabulary\n",
        "\n",
        "One way to represent a fixed amount of words is by making a one-hot encoded vector, which consists of 0s in all cells with the exception of a single 1 in a cell used uniquely to identify each word.\n",
        "\n",
        "| vocabulary    | one-hot encoded vector   |\n",
        "| ------------- |--------------------------|\n",
        "| Paris         | $= [1, 0, 0, \\ldots, 0]$ |\n",
        "| Rome          | $= [0, 1, 0, \\ldots, 0]$ |\n",
        "| Copenhagen    | $= [0, 0, 1, \\ldots, 0]$ |\n",
        "\n",
        "Representing a large vocabulary with one-hot encodings often becomes inefficient because of the size of each sparse vector.\n",
        "To overcome this challenge it is common practice to truncate the vocabulary to contain the $k$ most used words and represent the rest with a special symbol, $\\mathtt{UNK}$, to define unknown/unimportant words.\n",
        "This often causes entities such as names to be represented with $\\mathtt{UNK}$ because they are rare.\n",
        "\n",
        "Consider the following text\n",
        "> I love the corny jokes in Spielberg's new movie.\n",
        "\n",
        "where an example result would be similar to\n",
        "> I love the corny jokes in $\\mathtt{UNK}$'s new movie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ag97wDL4Ubv"
      },
      "source": [
        "### Generating a dataset\n",
        "\n",
        "We generate sequences of the form:\n",
        "\n",
        "`a b EOS`,\n",
        "\n",
        "`a a b b EOS`,\n",
        "\n",
        "`a a a a a b b b b b EOS`\n",
        "\n",
        "where `EOS` is a special character denoting the end of a sequence. The task is to predict the next token $t_n$, i.e. `a`, `b`, `EOS` or the unknown token `UNK` given a sequence of tokens $\\{ t_{1}, t_{2}, \\dots , t_{n-1}\\}$, and we are to process sequences in a sequential manner. As such, the network will need to learn that e.g. 5 `b`s and an `EOS` token will be preceded by 5 `a`s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVm_7FMdRtWL"
      },
      "source": [
        "CHARS = ['a', 'b']\n",
        "UNKNOWN = 'U'\n",
        "EOS = 'E'\n",
        "VOCAB = [UNKNOWN, EOS] + CHARS\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "NUM_SENTENCES = 2**8\n",
        "P_TRAIN = int(NUM_SENTENCES * 0.8)\n",
        "P_VAL = int(NUM_SENTENCES * 0.1)\n",
        "P_TEST = int(NUM_SENTENCES * 0.1)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSNcnPbA2AEj"
      },
      "source": [
        "def generate_dataset(num_sequences):\n",
        "  \"\"\"\n",
        "  Generated a number of sequences as out dataset.\n",
        "  \"\"\"\n",
        "  generate_random_token = lambda num_tokens: (\n",
        "      ''.join([c * num_tokens for c in CHARS]) + EOS\n",
        "  )\n",
        "  return [generate_random_token(randint(1, 12)) for _ in range(num_sequences)]"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLtzsitq7Ekw",
        "outputId": "8bd61409-6d05-4e66-c0ae-4cbdad1f71c1"
      },
      "source": [
        "sequences = generate_dataset(NUM_SENTENCES)\n",
        "sequences[:5]"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aaaaabbbbbE',\n",
              " 'aaaaabbbbbE',\n",
              " 'aaaaaaaaaaabbbbbbbbbbbE',\n",
              " 'aaaaaaabbbbbbbE',\n",
              " 'aaaaaaaaabbbbbbbbbE']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGpn9GHR9IHR"
      },
      "source": [
        "## Representing tokens as indices\n",
        "\n",
        "To build a one-hot encoding, we need to assign each possible word in our vocabulary an index. We do that by creating two dictionaries: one that allows us to go from a given word to its corresponding index in our vocabulary, and one for the reverse direction. Let's call them `word_to_idx` and `idx_to_word`. The keyword `vocab_size` specifies the maximum size of our vocabulary. If we try to access a word that does not exist in our vocabulary, it is automatically replaced by the `UNK` token or its corresponding index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-ArjKF4BZJ8"
      },
      "source": [
        "word_to_idx = OrderedDict((word, index) for index, word in enumerate(VOCAB)) \n",
        "idx_to_word = OrderedDict((index, word) for index, word in enumerate(VOCAB))\n",
        "vocab_size = len(VOCAB)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuiEBnlwXe-U"
      },
      "source": [
        "## Partitioning the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VafQndpbXW46"
      },
      "source": [
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, inputs, targets):\n",
        "    self.inputs = inputs\n",
        "    self.targets = targets\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.targets)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    X = self.inputs[index]\n",
        "    y = self.targets[index]\n",
        "    return X, y"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGhRyIYrXW2G",
        "outputId": "b57a72aa-5546-4389-a7cf-5cf318a8d7fc"
      },
      "source": [
        "inputs = [sequences[i][:-1] for i in range(len(sequences))]\n",
        "targets = [sequences[i][1:] for i in range(len(sequences))]\n",
        "train_set = Dataset(inputs[:P_TRAIN], targets[:P_TRAIN])\n",
        "val_set = Dataset(inputs[P_TRAIN:P_TRAIN + P_VAL], targets[P_TRAIN:P_TRAIN + P_VAL])\n",
        "test_set = Dataset(inputs[-P_TEST:], targets[-P_TEST:])\n",
        "\n",
        "print(f'We have {len(train_set)} samples in the training set.')\n",
        "print(f'We have {len(val_set)} samples in the validation set.')\n",
        "print(f'We have {len(test_set)} samples in the test set.')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 204 samples in the training set.\n",
            "We have 25 samples in the validation set.\n",
            "We have 25 samples in the test set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQGXexzgkY0Y"
      },
      "source": [
        "## One-hot encodings\n",
        "We now create a simple function that returns the one-hot encoded representation of a given index of a word in our vocabulary. Notice that the shape of the one-hot encoding is equal to the entire vocabulary (which can be huge!). Additionally, we define a function to automatically one-hot encode a sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plzXFcMvXWze",
        "outputId": "0d312bc1-4b09-42d9-c972-807fe9be3807"
      },
      "source": [
        "def one_hot_encode(idx):\n",
        "    '''\n",
        "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
        "    \n",
        "    Args:\n",
        "     `idx`: the index of the given word\n",
        "     `vocab_size`: the size of the vocabulary\n",
        "    \n",
        "    Returns a 1-D numpy array of length `vocab_size`.\n",
        "    '''\n",
        "    # Initialize the encoded array\n",
        "    one_hot = np.zeros(VOCAB_SIZE)\n",
        "    \n",
        "    # Set the appropriate element to one\n",
        "    one_hot[idx] = 1.0\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "def one_hot_encode_sequence(sequence):\n",
        "    '''\n",
        "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
        "    \n",
        "    Args:\n",
        "     `sentence`: a list of words to encode\n",
        "     `vocab_size`: the size of the vocabulary\n",
        "     \n",
        "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
        "    '''\n",
        "    # Encode each word in the sentence\n",
        "    encoding = np.array(\n",
        "        [one_hot_encode(word_to_idx[word]) for word in sequence]\n",
        "    )\n",
        "\n",
        "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
        "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
        "    \n",
        "    return encoding\n",
        "\n",
        "test_word = one_hot_encode(word_to_idx['a'])\n",
        "print(f'One-hot encoding of \"a\" has shape {test_word.shape} -> {test_word}')\n",
        "\n",
        "test_sent = one_hot_encode_sequence(['a', 'b'])\n",
        "print(f'One-hot encoding of \"a b\" has shape {test_sent.shape} -> {test_sent}')"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One-hot encoding of \"a\" has shape (4,) -> [0. 0. 1. 0.]\n",
            "One-hot encoding of \"a b\" has shape (2, 4, 1) -> [[[0.]\n",
            "  [0.]\n",
            "  [1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [1.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kafwb0TEo4Nv"
      },
      "source": [
        "# Recurrent Neural Networks (RNN)\n",
        "___\n",
        "\n",
        "A RNN performs its computations in a cyclic manner, where the same computation is applied to every sample of a given sequence.\n",
        "The idea is that the network should be able to use the previous computations as some form of memory and apply this to future computations.\n",
        "\n",
        "![rnn-unroll image](https://github.com/martin-fabbri/colab-notebooks/raw/master/rnn/images/rnn-folded-unfolded.png))\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "- $X$ is the input sequence of samples, \n",
        "- $U$ is a weight matrix applied to the given input sample,\n",
        "- $V$ is a weight matrix used for the recurrent computation in order to pass memory along the sequence,\n",
        "- $W$ is a weight matrix used to compute the output of the every timestep (given that every timestep requires an output),\n",
        "- $h$ is the hidden state (the network's memory) for a given time step, and\n",
        "- $o$ is the resulting output.\n",
        "\n",
        "When the network is unrolled as shown, it is easier to refer to a timestep, $t$.\n",
        "We have the following computations through the network:\n",
        "\n",
        "- $h_t = f(U\\,{x_t} + V\\,{h_{t-1}})$, where $f$ is a non-linear activation function, e.g. $\\mathrm{tanh}$.\n",
        "- $o_t = W\\,{h_t}$\n",
        "\n",
        "When we are doing language modelling using a cross-entropy loss, we additionally apply the softmax function to the output $o_{t}$:\n",
        "\n",
        "- $\\hat{y}_t = \\mathrm{softmax}(o_{t})$\n",
        "\n",
        "\n",
        "### Backpropagation through time\n",
        "\n",
        "We define a loss function\n",
        "\n",
        "- $E = \\sum_t E_t  = \\sum_t E_t(y_t ,\\hat{y}_t ) \\ , $\n",
        "\n",
        "where $E_t(y_t ,\\hat{y}_t )$ is the cross-entropy function.\n",
        "\n",
        "Backpropagation through time amounts to computing the gradients of the loss using the same type of clever bookkeeping we applied to the feed-forward network in week 1. This you will do in Exercise D."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTrF5TmXekFa",
        "outputId": "1d8959c5-c6ba-4f2f-db54-eb0a2306995d"
      },
      "source": [
        "one_hot_encode_sequence(sequences[0])"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.]],\n",
              "\n",
              "       [[0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.]],\n",
              "\n",
              "       [[0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.]],\n",
              "\n",
              "       [[0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.]],\n",
              "\n",
              "       [[0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.]],\n",
              "\n",
              "       [[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.]],\n",
              "\n",
              "       [[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.]],\n",
              "\n",
              "       [[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.]],\n",
              "\n",
              "       [[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.]],\n",
              "\n",
              "       [[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.]],\n",
              "\n",
              "       [[0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4Ukog4IfO7d"
      },
      "source": [
        ""
      ],
      "execution_count": 105,
      "outputs": []
    }
  ]
}