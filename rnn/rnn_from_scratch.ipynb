{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_from_scratch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM8IdvZgfRVrvIcQhQdsX2j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/rnn/rnn_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzjc2dlH2GsQ"
      },
      "source": [
        "# Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUVIbX8I4_gR"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from numpy.random import randint\n",
        "from collections import OrderedDict\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uzAyRHP2U7H"
      },
      "source": [
        "## Representing text as tokens\n",
        "\n",
        "Let's define our dataset samples $x \\in \\mathrm{R}^d$, where $d$ is the feature space dimension.\n",
        "\n",
        "With time sequences our data can be represented as $x \\in \\mathrm{R}^{t \\, \\times \\, d}$, where $t$ is the sequence length. \n",
        "This emphasises sequence dependence and that the samples along the sequence are not independent and identically distributed (i.i.d.).\n",
        "We will model functions as $\\mathrm{R}^{t \\, \\times \\, d} \\rightarrow \\mathrm{R}^c$, where $c$ is the amount of classes in the output.\n",
        "\n",
        "There are several ways to represent sequences. With text, the challenge is how to represent a word as a feature vector in $d$ dimensions, as we are required to represent text with decimal numbers in order to apply neural networks to it.\n",
        "\n",
        "Initially, we will use a simple one-hot encoding but for categorical variables that can take on many values (e.g. words in the English language)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-DRDHhQ3zs7"
      },
      "source": [
        "### One-hot encoding over vocabulary\n",
        "\n",
        "One way to represent a fixed amount of words is by making a one-hot encoded vector, which consists of 0s in all cells with the exception of a single 1 in a cell used uniquely to identify each word.\n",
        "\n",
        "| vocabulary    | one-hot encoded vector   |\n",
        "| ------------- |--------------------------|\n",
        "| Paris         | $= [1, 0, 0, \\ldots, 0]$ |\n",
        "| Rome          | $= [0, 1, 0, \\ldots, 0]$ |\n",
        "| Copenhagen    | $= [0, 0, 1, \\ldots, 0]$ |\n",
        "\n",
        "Representing a large vocabulary with one-hot encodings often becomes inefficient because of the size of each sparse vector.\n",
        "To overcome this challenge it is common practice to truncate the vocabulary to contain the $k$ most used words and represent the rest with a special symbol, $\\mathtt{UNK}$, to define unknown/unimportant words.\n",
        "This often causes entities such as names to be represented with $\\mathtt{UNK}$ because they are rare.\n",
        "\n",
        "Consider the following text\n",
        "> I love the corny jokes in Spielberg's new movie.\n",
        "\n",
        "where an example result would be similar to\n",
        "> I love the corny jokes in $\\mathtt{UNK}$'s new movie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ag97wDL4Ubv"
      },
      "source": [
        "### Generating a dataset\n",
        "\n",
        "We generate sequences of the form:\n",
        "\n",
        "`a b EOS`,\n",
        "\n",
        "`a a b b EOS`,\n",
        "\n",
        "`a a a a a b b b b b EOS`\n",
        "\n",
        "where `EOS` is a special character denoting the end of a sequence. The task is to predict the next token $t_n$, i.e. `a`, `b`, `EOS` or the unknown token `UNK` given a sequence of tokens $\\{ t_{1}, t_{2}, \\dots , t_{n-1}\\}$, and we are to process sequences in a sequential manner. As such, the network will need to learn that e.g. 5 `b`s and an `EOS` token will be preceded by 5 `a`s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVm_7FMdRtWL"
      },
      "source": [
        "CHARS = ['a', 'b']\n",
        "VOCAB = ['<UNK>', '<EOS>'] + CHARS\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "NUM_SENTENCES = 2**8"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSNcnPbA2AEj"
      },
      "source": [
        "def generate_dataset(num_sequences):\n",
        "  \"\"\"\n",
        "  Generated a number of sequences as out dataset.\n",
        "  \"\"\"\n",
        "  generate_random_token = lambda num_tokens: (\n",
        "      ''.join([c * num_tokens for c in CHARS]) + '<EOS>'\n",
        "  )\n",
        "  return [generate_random_token(randint(1, 12)) for _ in range(num_sequences)]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLtzsitq7Ekw",
        "outputId": "ce19f7fa-92ca-4252-e347-66104d1d7e71"
      },
      "source": [
        "sequences = generate_dataset(NUM_SENTENCES)\n",
        "sequences[:5]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aabb<EOS>',\n",
              " 'aaaaaaabbbbbbb<EOS>',\n",
              " 'aaaaabbbbb<EOS>',\n",
              " 'aaaaaaaaaaabbbbbbbbbbb<EOS>',\n",
              " 'aaaaaabbbbbb<EOS>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGpn9GHR9IHR"
      },
      "source": [
        "## Representing tokens as indices\n",
        "\n",
        "To build a one-hot encoding, we need to assign each possible word in our vocabulary an index. We do that by creating two dictionaries: one that allows us to go from a given word to its corresponding index in our vocabulary, and one for the reverse direction. Let's call them `word_to_idx` and `idx_to_word`. The keyword `vocab_size` specifies the maximum size of our vocabulary. If we try to access a word that does not exist in our vocabulary, it is automatically replaced by the `UNK` token or its corresponding index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-ArjKF4BZJ8"
      },
      "source": [
        "word_to_idx = OrderedDict((word, index) for index, word in enumerate(VOCAB)) \n",
        "idx_to_word = OrderedDict((index, word) for index, word in enumerate(VOCAB))\n",
        "vocab_size = len(VOCAB)"
      ],
      "execution_count": 53,
      "outputs": []
    }
  ]
}