{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_from_scratch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPmcpE6R6igk4NESx6w+Ahb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/rnn/rnn_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzjc2dlH2GsQ"
      },
      "source": [
        "# Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUVIbX8I4_gR"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from numpy.random import randint\n",
        "from collections import OrderedDict\n",
        "from torch.utils import data\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uzAyRHP2U7H"
      },
      "source": [
        "## Representing text as tokens\n",
        "\n",
        "Let's define our dataset samples $x \\in \\mathrm{R}^d$, where $d$ is the feature space dimension.\n",
        "\n",
        "With time sequences our data can be represented as $x \\in \\mathrm{R}^{t \\, \\times \\, d}$, where $t$ is the sequence length. \n",
        "This emphasises sequence dependence and that the samples along the sequence are not independent and identically distributed (i.i.d.).\n",
        "We will model functions as $\\mathrm{R}^{t \\, \\times \\, d} \\rightarrow \\mathrm{R}^c$, where $c$ is the amount of classes in the output.\n",
        "\n",
        "There are several ways to represent sequences. With text, the challenge is how to represent a word as a feature vector in $d$ dimensions, as we are required to represent text with decimal numbers in order to apply neural networks to it.\n",
        "\n",
        "Initially, we will use a simple one-hot encoding but for categorical variables that can take on many values (e.g. words in the English language)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-DRDHhQ3zs7"
      },
      "source": [
        "### One-hot encoding over vocabulary\n",
        "\n",
        "One way to represent a fixed amount of words is by making a one-hot encoded vector, which consists of 0s in all cells with the exception of a single 1 in a cell used uniquely to identify each word.\n",
        "\n",
        "| vocabulary    | one-hot encoded vector   |\n",
        "| ------------- |--------------------------|\n",
        "| Paris         | $= [1, 0, 0, \\ldots, 0]$ |\n",
        "| Rome          | $= [0, 1, 0, \\ldots, 0]$ |\n",
        "| Copenhagen    | $= [0, 0, 1, \\ldots, 0]$ |\n",
        "\n",
        "Representing a large vocabulary with one-hot encodings often becomes inefficient because of the size of each sparse vector.\n",
        "To overcome this challenge it is common practice to truncate the vocabulary to contain the $k$ most used words and represent the rest with a special symbol, $\\mathtt{UNK}$, to define unknown/unimportant words.\n",
        "This often causes entities such as names to be represented with $\\mathtt{UNK}$ because they are rare.\n",
        "\n",
        "Consider the following text\n",
        "> I love the corny jokes in Spielberg's new movie.\n",
        "\n",
        "where an example result would be similar to\n",
        "> I love the corny jokes in $\\mathtt{UNK}$'s new movie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ag97wDL4Ubv"
      },
      "source": [
        "### Generating a dataset\n",
        "\n",
        "We generate sequences of the form:\n",
        "\n",
        "`a b EOS`,\n",
        "\n",
        "`a a b b EOS`,\n",
        "\n",
        "`a a a a a b b b b b EOS`\n",
        "\n",
        "where `EOS` is a special character denoting the end of a sequence. The task is to predict the next token $t_n$, i.e. `a`, `b`, `EOS` or the unknown token `UNK` given a sequence of tokens $\\{ t_{1}, t_{2}, \\dots , t_{n-1}\\}$, and we are to process sequences in a sequential manner. As such, the network will need to learn that e.g. 5 `b`s and an `EOS` token will be preceded by 5 `a`s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVm_7FMdRtWL"
      },
      "source": [
        "CHARS = ['a', 'b']\n",
        "UNKNOWN = 'U'\n",
        "EOS = 'E'\n",
        "VOCAB = [UNKNOWN, EOS] + CHARS\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "NUM_SENTENCES = 2**8\n",
        "HIDDEN_SIZE = 50\n",
        "P_TRAIN = int(NUM_SENTENCES * 0.8)\n",
        "P_VAL = int(NUM_SENTENCES * 0.1)\n",
        "P_TEST = int(NUM_SENTENCES * 0.1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSNcnPbA2AEj"
      },
      "source": [
        "def generate_dataset(num_sequences):\n",
        "  \"\"\"\n",
        "  Generated a number of sequences as out dataset.\n",
        "  \"\"\"\n",
        "  generate_random_token = lambda num_tokens: (\n",
        "      ''.join([c * num_tokens for c in CHARS]) + EOS\n",
        "  )\n",
        "  return [generate_random_token(randint(1, 12)) for _ in range(num_sequences)]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLtzsitq7Ekw",
        "outputId": "652b0cf9-ac95-4a30-e955-0bdea4e375c4"
      },
      "source": [
        "sequences = generate_dataset(NUM_SENTENCES)\n",
        "sequences[:5]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aaaaaaabbbbbbbE',\n",
              " 'aaaabbbbE',\n",
              " 'aaaaaaaaaaabbbbbbbbbbbE',\n",
              " 'aaaaaaaabbbbbbbbE',\n",
              " 'aaaaabbbbbE']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGpn9GHR9IHR"
      },
      "source": [
        "## Representing tokens as indices\n",
        "\n",
        "To build a one-hot encoding, we need to assign each possible word in our vocabulary an index. We do that by creating two dictionaries: one that allows us to go from a given word to its corresponding index in our vocabulary, and one for the reverse direction. Let's call them `word_to_idx` and `idx_to_word`. The keyword `vocab_size` specifies the maximum size of our vocabulary. If we try to access a word that does not exist in our vocabulary, it is automatically replaced by the `UNK` token or its corresponding index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-ArjKF4BZJ8"
      },
      "source": [
        "word_to_idx = OrderedDict((word, index) for index, word in enumerate(VOCAB)) \n",
        "idx_to_word = OrderedDict((index, word) for index, word in enumerate(VOCAB))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuiEBnlwXe-U"
      },
      "source": [
        "## Partitioning the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VafQndpbXW46"
      },
      "source": [
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, inputs, targets):\n",
        "    self.inputs = inputs\n",
        "    self.targets = targets\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.targets)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    X = self.inputs[index]\n",
        "    y = self.targets[index]\n",
        "    return X, y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGhRyIYrXW2G",
        "outputId": "834aec4e-1eb8-49a2-8ad9-47712ffeea92"
      },
      "source": [
        "inputs = [sequences[i][:-1] for i in range(len(sequences))]\n",
        "targets = [sequences[i][1:] for i in range(len(sequences))]\n",
        "train_set = Dataset(inputs[:P_TRAIN], targets[:P_TRAIN])\n",
        "val_set = Dataset(inputs[P_TRAIN:P_TRAIN + P_VAL], targets[P_TRAIN:P_TRAIN + P_VAL])\n",
        "test_set = Dataset(inputs[-P_TEST:], targets[-P_TEST:])\n",
        "\n",
        "print(f'We have {len(train_set)} samples in the training set.')\n",
        "print(f'We have {len(val_set)} samples in the validation set.')\n",
        "print(f'We have {len(test_set)} samples in the test set.')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 204 samples in the training set.\n",
            "We have 25 samples in the validation set.\n",
            "We have 25 samples in the test set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQGXexzgkY0Y"
      },
      "source": [
        "## One-hot encodings\n",
        "We now create a simple function that returns the one-hot encoded representation of a given index of a word in our vocabulary. Notice that the shape of the one-hot encoding is equal to the entire vocabulary (which can be huge!). Additionally, we define a function to automatically one-hot encode a sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plzXFcMvXWze",
        "outputId": "d733dd4b-44e9-42b1-cef3-6066580b2b15"
      },
      "source": [
        "def one_hot_encode(idx):\n",
        "    '''\n",
        "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
        "    \n",
        "    Args:\n",
        "     `idx`: the index of the given word\n",
        "     `vocab_size`: the size of the vocabulary\n",
        "    \n",
        "    Returns a 1-D numpy array of length `vocab_size`.\n",
        "    '''\n",
        "    # Initialize the encoded array\n",
        "    one_hot = np.zeros(VOCAB_SIZE)\n",
        "    \n",
        "    # Set the appropriate element to one\n",
        "    one_hot[idx] = 1.0\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "def one_hot_encode_sequence(sequence):\n",
        "    '''\n",
        "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
        "    \n",
        "    Args:\n",
        "     `sentence`: a list of words to encode\n",
        "     `vocab_size`: the size of the vocabulary\n",
        "     \n",
        "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
        "    '''\n",
        "    # Encode each word in the sentence\n",
        "    encoding = np.array(\n",
        "        [one_hot_encode(word_to_idx[word]) for word in sequence]\n",
        "    )\n",
        "\n",
        "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
        "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
        "    \n",
        "    return encoding\n",
        "\n",
        "test_word = one_hot_encode(word_to_idx['a'])\n",
        "print(f'One-hot encoding of \"a\" has shape {test_word.shape} -> {test_word}')\n",
        "\n",
        "test_sent = one_hot_encode_sequence(['a', 'b'])\n",
        "print(f'One-hot encoding of \"a b\" has shape {test_sent.shape} -> {test_sent}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One-hot encoding of \"a\" has shape (4,) -> [0. 0. 1. 0.]\n",
            "One-hot encoding of \"a b\" has shape (2, 4, 1) -> [[[0.]\n",
            "  [0.]\n",
            "  [1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [1.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kafwb0TEo4Nv"
      },
      "source": [
        "# Recurrent Neural Networks (RNN)\n",
        "___\n",
        "\n",
        "A RNN performs its computations in a cyclic manner, where the same computation is applied to every sample of a given sequence.\n",
        "The idea is that the network should be able to use the previous computations as some form of memory and apply this to future computations.\n",
        "\n",
        "![rnn-unroll image](https://github.com/martin-fabbri/colab-notebooks/raw/master/rnn/images/rnn-folded-unfolded.png)\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "- $X$ is the input sequence of samples, \n",
        "- $U$ is a weight matrix applied to the given input sample,\n",
        "- $V$ is a weight matrix used for the recurrent computation in order to pass memory along the sequence,\n",
        "- $W$ is a weight matrix used to compute the output of the every timestep (given that every timestep requires an output),\n",
        "- $h$ is the hidden state (the network's memory) for a given time step, and\n",
        "- $o$ is the resulting output.\n",
        "\n",
        "When the network is unrolled as shown, it is easier to refer to a timestep, $t$.\n",
        "We have the following computations through the network:\n",
        "\n",
        "- $h_t = f(U\\,{x_t} + V\\,{h_{t-1}})$, where $f$ is a non-linear activation function, e.g. $\\mathrm{tanh}$.\n",
        "- $o_t = W\\,{h_t}$\n",
        "\n",
        "When we are doing language modelling using a cross-entropy loss, we additionally apply the softmax function to the output $o_{t}$:\n",
        "\n",
        "- $\\hat{y}_t = \\mathrm{softmax}(o_{t})$\n",
        "\n",
        "\n",
        "### Backpropagation through time\n",
        "\n",
        "We define a loss function\n",
        "\n",
        "- $E = \\sum_t E_t  = \\sum_t E_t(y_t ,\\hat{y}_t ) \\ , $\n",
        "\n",
        "where $E_t(y_t ,\\hat{y}_t )$ is the cross-entropy function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a89XCuTm1YZi"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "We will implement the forward pass, backward pass, optimization and training loop for an RNN in numpy so that you can get familiar with the recurrent nature of RNNs. Later, we will go back to PyTorch and appreciate how convenient the implementation becomes!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTrF5TmXekFa",
        "outputId": "362b7378-07d3-4f80-9bc8-f66f312876fd"
      },
      "source": [
        "def init_orthogonal(param):\n",
        "    \"\"\"\n",
        "    Initializes weight parameters orthogonally.\n",
        "    This is a common initiailization for recurrent neural networks.\n",
        "    \n",
        "    Refer to this paper for an explanation of this initialization:\n",
        "    https://arxiv.org/abs/1312.6120\n",
        "    \"\"\"\n",
        "    if param.ndim < 2:\n",
        "        raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
        "\n",
        "    rows, cols = param.shape\n",
        "    \n",
        "    new_param = np.random.randn(rows, cols)\n",
        "    \n",
        "    if rows < cols:\n",
        "        new_param = new_param.T\n",
        "    \n",
        "    # Compute QR factorization\n",
        "    q, r = np.linalg.qr(new_param)\n",
        "    \n",
        "    # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n",
        "    d = np.diag(r, 0)\n",
        "    ph = np.sign(d)\n",
        "    q *= ph\n",
        "\n",
        "    if rows < cols:\n",
        "        q = q.T\n",
        "    \n",
        "    new_param = q\n",
        "    \n",
        "    return new_param\n",
        "\n",
        "\n",
        "def init_rnn(hidden_size, vocab_size):\n",
        "    \"\"\"\n",
        "    Initializes our recurrent neural network.\n",
        "    \n",
        "    Args:\n",
        "     `hidden_size`: the dimensions of the hidden state\n",
        "     `vocab_size`: the dimensions of our vocabulary\n",
        "    \"\"\"\n",
        "    # Weight matrix (input to hidden state)\n",
        "    # YOUR CODE HERE!\n",
        "    U = np.zeros((hidden_size, vocab_size))\n",
        "\n",
        "    # Weight matrix (recurrent computation)\n",
        "    # YOUR CODE HERE!\n",
        "    V = np.zeros((hidden_size, hidden_size))\n",
        "\n",
        "    # Weight matrix (hidden state to output)\n",
        "    # YOUR CODE HERE!\n",
        "    W = np.zeros((vocab_size, hidden_size))\n",
        "\n",
        "    # Bias (hidden state)\n",
        "    # YOUR CODE HERE!\n",
        "    b_hidden = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # Bias (output)\n",
        "    # YOUR CODE HERE!\n",
        "    b_out = np.zeros((vocab_size, 1))\n",
        "    \n",
        "    # Initialize weights\n",
        "    U = init_orthogonal(U)\n",
        "    V = init_orthogonal(V)\n",
        "    W = init_orthogonal(W)\n",
        "    \n",
        "    # Return parameters as a tuple\n",
        "    return U, V, W, b_hidden, b_out\n",
        "\n",
        "\n",
        "params = init_rnn(hidden_size=HIDDEN_SIZE, vocab_size=VOCAB_SIZE)\n",
        "print('U:', params[0].shape)\n",
        "print('V:', params[1].shape)\n",
        "print('W:', params[2].shape)\n",
        "print('b_hidden:', params[3].shape)\n",
        "print('b_out:', params[4].shape)\n",
        "\n",
        "for param in params:\n",
        "    assert param.ndim == 2, \\\n",
        "        'all parameters should be 2-dimensional '\\\n",
        "        '(hint: a dimension can simply have size 1)'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "U: (50, 4)\n",
            "V: (50, 50)\n",
            "W: (4, 50)\n",
            "b_hidden: (50, 1)\n",
            "b_out: (4, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUycuHlB54ux"
      },
      "source": [
        "#### Sigmoid activation\n",
        "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4Ukog4IfO7d"
      },
      "source": [
        "def sigmoid(x, derivative=False):\n",
        "    '''\n",
        "    Computes the element-wise sigmoid activation function for an array x.\n",
        "\n",
        "    Args:\n",
        "     `x`: the array where the function is applied\n",
        "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
        "    '''\n",
        "    x_safe = x + 1e-12\n",
        "    f = 1 / (1 + np.exp(-x_safe))\n",
        "    \n",
        "    if derivative: # Return the derivative of the function evaluated at x\n",
        "        return f * (1 - f)\n",
        "    else: # Return the forward pass of the function at x\n",
        "        return f"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MzBPta86Kl0"
      },
      "source": [
        "#### Hyperbolic Tangent activation\n",
        "\n",
        "$\\tanh{x} = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C-tPo7d6IM6"
      },
      "source": [
        "def tanh(x, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the element-wise tanh activation function for an array x.\n",
        "\n",
        "    Args:\n",
        "     `x`: the array where the function is applied\n",
        "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
        "    \"\"\"\n",
        "    x_safe = x + 1e-12\n",
        "    f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
        "    \n",
        "    if derivative: # Return the derivative of the function evaluated at x\n",
        "        return 1-f**2\n",
        "    else: # Return the forward pass of the function at x\n",
        "        return f"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9w3J78d6Ppl"
      },
      "source": [
        "#### Softmax\n",
        "\n",
        "$\\mathrm{softmax}(x) = \\frac{e^{x_{i}}}{\\sum_{j=1}^{K} e^{x_{j}}} \\text { for } i=1, \\ldots, K \\text { and } \\mathbf{x}=\\left(x_{1}, \\ldots, x_{K}\\right) \\in \\mathbb{R}^{K}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avSkIazH6PZ9"
      },
      "source": [
        "def softmax(x, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the softmax for an array x.\n",
        "    \n",
        "    Args:\n",
        "     `x`: the array where the function is applied\n",
        "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
        "    \"\"\"\n",
        "    x_safe = x + 1e-12\n",
        "    f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
        "    \n",
        "    if derivative: # Return the derivative of the function evaluated at x\n",
        "        pass # We will not need this one\n",
        "    else: # Return the forward pass of the function at x\n",
        "        return f"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khzkiR7r6bpe"
      },
      "source": [
        "### Implement the forward \n",
        "\n",
        "Now that we have all the definitions in place, we can start to implement a forward pass.\n",
        "\n",
        "- $h_t = f(U\\,{x_t} + V\\,{h_{t-1}})$, where $f$ is a non-linear activation function, e.g. $\\mathrm{tanh}$.\n",
        "- $o_t = W\\,{h_t}$\n",
        "- $\\hat{y}_t = \\mathrm{softmax}(o_{t})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpaUtjW76PXK",
        "outputId": "ca9dd0c7-1926-4d6f-e05f-b36a7a974290"
      },
      "source": [
        "def forward_pass(inputs, hidden_state, params, activation_func=tanh):\n",
        "    \"\"\"\n",
        "    Computes the forward pass of a vanilla RNN.\n",
        "    \n",
        "    Args:\n",
        "     `inputs`: sequence of inputs to be processed\n",
        "     `hidden_state`: an already initialized hidden state\n",
        "     `params`: the parameters of the RNN\n",
        "    \"\"\"\n",
        "    # First we unpack our parameters\n",
        "    U, V, W, b_hidden, b_out = params\n",
        "    \n",
        "    # Create a list to store outputs and hidden states\n",
        "    outputs, hidden_states = [], []\n",
        "    \n",
        "    # For each element in input sequence\n",
        "    for t in range(len(inputs)):\n",
        "\n",
        "        # Compute new hidden state\n",
        "        # YOUR CODE HERE!\n",
        "        hidden_state = activation_func(\n",
        "            np.dot(U, inputs[t]) + np.dot(V, hidden_state) + b_hidden\n",
        "        )\n",
        "\n",
        "        # Compute output\n",
        "        # YOUR CODE HERE!\n",
        "        out = softmax(np.dot(W, hidden_state) + b_out)\n",
        "        \n",
        "        # Save results and continue\n",
        "        outputs.append(out)\n",
        "        hidden_states.append(hidden_state.copy())\n",
        "    \n",
        "    return outputs, hidden_states\n",
        "\n",
        "\n",
        "# Get first sequence in training set\n",
        "test_input_sequence, test_target_sequence = train_set[0]\n",
        "\n",
        "# One-hot encode input and target sequence\n",
        "test_input = one_hot_encode_sequence(test_input_sequence)\n",
        "test_target = one_hot_encode_sequence(test_target_sequence)\n",
        "\n",
        "# Initialize hidden state as zeros\n",
        "hidden_state = np.zeros((HIDDEN_SIZE, 1))\n",
        "\n",
        "# Now let's try out our new function\n",
        "outputs, hidden_states = forward_pass(test_input, hidden_state, params)\n",
        "\n",
        "print('Input sequence:')\n",
        "print(test_input_sequence)\n",
        "\n",
        "# print('\\nTarget sequence:')\n",
        "# print(test_target_sequence)\n",
        "\n",
        "print('\\nPredicted sequence:')\n",
        "print([idx_to_word[np.argmax(output)] for output in outputs])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence:\n",
            "aaaaaaabbbbbbb\n",
            "\n",
            "Predicted sequence:\n",
            "['E', 'b', 'a', 'U', 'a', 'a', 'a', 'b', 'a', 'b', 'b', 'b', 'b', 'b']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JazWWv3Nb5SP"
      },
      "source": [
        "___\n",
        "U: (50, 4)   V: (50, 50)   W: (4, 50)   b_hidden: (50, 1)   b_out: (4, 1)\n",
        "___\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTcdFX1DadG_"
      },
      "source": [
        "## Backward pass\n",
        "\n",
        "Implement the missing code in the backward pass code given below using a cross-entropy loss and $\\tanh{x}$ as non-linear activation function $f$.\n",
        "\n",
        "To complete the implementation, we need to compute the partial derivatives\n",
        "$\n",
        "\\frac{\\partial E}{\\partial W},~\\frac{\\partial E}{\\partial U},~\\frac{\\partial E}{\\partial V}\n",
        "$. \n",
        "We repeat the definition of the RNN forward pass from above:\n",
        "\n",
        "- $h_t = f(U\\,{x_t} + V\\,{h_{t-1}})$, where $f$ usually is an activation function, e.g. $\\mathrm{tanh}$.\n",
        "- $o_t = W\\,{h_t}$\n",
        "- $\\hat{y}_t = \\mathrm{softmax}(o_{t})$\n",
        "\n",
        "where\n",
        "- $U$ is a weight matrix applied to the given input sample,\n",
        "- $V$ is a weight matrix used for the recurrent computation in order to pass memory along the sequence,\n",
        "- $W$ is a weight matrix used to compute the output of the every timestep (given that every timestep requires an output), and\n",
        "- $h$ is the hidden state (the network's memory) for a given time step.\n",
        "\n",
        "Recall though, that RNNs are recurrent and the weights $W,~U,~V$ are shared across time, i.e. we do not have separate weights for each time step. Therefore, to compute e.g. the partial derivative $\\frac{\\partial E}{\\partial W}$, we need to 1) sum up across time, and 2) apply the chain rule:\n",
        "\n",
        "$$\\frac{\\partial E}{\\partial W} = \\sum_{t} \\frac{\\partial E}{\\partial o_{t}} \\frac{\\partial o_{t}}{\\partial W}\\,.$$\n",
        "To compute$\\frac{\\partial o_{t}}{\\partial W}$ we use the definition of $o_t$ above.\n",
        "From week 1 (exercise i) we have that\n",
        "$$\\delta_{o,t} \\equiv \\frac{\\partial E}{\\partial o_{t}} = \\frac{\\partial E_t}{\\partial o_{t}} = \\hat{y}_{t} - y_{t}\\,,$$\n",
        "where $\\hat{y}_{t}$ is a softmax distribution over model outputs $o_{t}$ at time $t$, and $y_{t}$ is the target label at time $t$. In the above code, this corresponds to\n",
        "```\n",
        "d_o = outputs[t].copy()\n",
        "d_o[np.argmax(targets[t])] -= 1\n",
        "```\n",
        "where `outputs[t]` is the output $\\hat{y}_{t}$, and `targets[t]` is a one-hot encoded target.\n",
        "\n",
        "To compute $\\frac{\\partial E}{\\partial U}$ and $\\frac{\\partial E}{\\partial V}$ we again sum over time and use the chain rule:\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial U} = \\sum_{t} \\frac{\\partial E}{\\partial h_{t}} \\frac{\\partial h_{t}}{\\partial U} \\ . \n",
        "$$\n",
        "This leads us to introduce\n",
        "$$\n",
        "\\delta_{h,t} \\equiv \\frac{\\partial E}{\\partial h_{t}} \\ .\n",
        "$$\n",
        "The backpropagation through time recursion is derived by realising that a variation of $h_t$ affects 1) the loss at time step $t$ through the feed-forward connection to the output and 2) the future losses through the $h_{t+1}$ dependence of $h_t$. Mathematically, we write this through the chain rule:\n",
        "\n",
        "$$\n",
        "\\delta_{h,t} \\equiv \\frac{\\partial E}{\\partial h_{t}} =  \\frac{\\partial E}{\\partial o_{t}} \\frac{\\partial o_t}{\\partial h_{t}} + \\frac{\\partial E}{\\partial h_{t+1}}\n",
        "\\frac{\\partial h_{t+1}}{\\partial h_{t}} = \\delta_{o,t} \\frac{\\partial o_t}{\\partial h_{t}} + \\delta_{h,t+1}\n",
        "\\frac{\\partial h_{t+1}}{\\partial h_{t}} \\ . \n",
        "$$\n",
        "\n",
        "Like above we can compute $\\frac{\\partial h_{t+1}}{\\partial h_{t}}$ using the definition of the network (shifted one time step). In the code the intermediate steps to compute the $\\delta$ recursions have been precomputed for you. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_lSF0Fc6PUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b774269-b0d1-49c9-bf2a-4c6344eb4c2e"
      },
      "source": [
        "params[2].shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MNlc6uv6PRx"
      },
      "source": [
        "def clip_gradient_norm(grads, max_norm=0.25):\n",
        "    \"\"\"\n",
        "    Clips gradients to have a maximum norm of `max_norm`.\n",
        "    This is to prevent the exploding gradients problem.\n",
        "    \"\"\" \n",
        "    # Set the maximum of the norm to be of type float\n",
        "    max_norm = float(max_norm)\n",
        "    total_norm = 0\n",
        "    \n",
        "    # Calculate the L2 norm squared for each gradient and add them to the total norm\n",
        "    for grad in grads:\n",
        "        grad_norm = np.sum(np.power(grad, 2))\n",
        "        total_norm += grad_norm\n",
        "    \n",
        "    total_norm = np.sqrt(total_norm)\n",
        "    \n",
        "    # Calculate clipping coeficient\n",
        "    clip_coef = max_norm / (total_norm + 1e-6)\n",
        "    \n",
        "    # If the total norm is larger than the maximum allowable norm, then clip the gradient\n",
        "    if clip_coef < 1:\n",
        "        for grad in grads:\n",
        "            grad *= clip_coef\n",
        "    \n",
        "    return grads"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQWcdFRH6POl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efbaf49e-b223-46d1-a33a-762ef50bcbeb"
      },
      "source": [
        "def backward_pass(inputs, outputs, hidden_states, targets, params):\n",
        "    \"\"\"\n",
        "    Computes the backward pass of a vanilla RNN.\n",
        "    \n",
        "    Args:\n",
        "     `inputs`: sequence of inputs to be processed\n",
        "     `outputs`: sequence of outputs from the forward pass\n",
        "     `hidden_states`: sequence of hidden_states from the forward pass\n",
        "     `targets`: sequence of targets\n",
        "     `params`: the parameters of the RNN\n",
        "    \"\"\"\n",
        "    # First we unpack our parameters\n",
        "    U, V, W, b_hidden, b_out = params\n",
        "    \n",
        "    # Initialize gradients as zero\n",
        "    d_U, d_V, d_W = np.zeros_like(U), np.zeros_like(V), np.zeros_like(W)\n",
        "    d_b_hidden, d_b_out = np.zeros_like(b_hidden), np.zeros_like(b_out)\n",
        "    \n",
        "    # Keep track of hidden state derivative and loss\n",
        "    d_h_next = np.zeros_like(hidden_states[0])\n",
        "    loss = 0\n",
        "    \n",
        "    # For each element in output sequence\n",
        "    # NB: We iterate backwards s.t. t = N, N-1, ... 1, 0\n",
        "    for t in reversed(range(len(outputs))):\n",
        "\n",
        "        # Compute the cross-entropy loss E (as a scalar)\n",
        "        # When taking logarithms, it's a good idea to add a small constant (e.g. 1e-9)\n",
        "        # YOUR CODE HERE!\n",
        "        loss += -np.mean(targets[t] * np.log(outputs[t] + 1e-9))\n",
        "        \n",
        "        # Backpropagate into output o (derivative of cross-entropy)\n",
        "        # If you're confused about this step, see this link for an explanation:\n",
        "        # http://cs231n.github.io/neural-networks-case-study/#grad\n",
        "        d_o = outputs[t].copy()\n",
        "        d_o[np.argmax(targets[t])] -= 1\n",
        "        \n",
        "        # Backpropagate into weights W\n",
        "        # YOUR CODE HERE!\n",
        "        d_W += np.dot(d_o, hidden_states[t].T)\n",
        "        d_b_out += d_o\n",
        "        \n",
        "        # Backpropagate into hidden state h\n",
        "        d_h = np.dot(W.T, d_o) + d_h_next\n",
        "        \n",
        "        # Backpropagate through non-linearity f\n",
        "        # (we assume tanh is used here)\n",
        "        d_f = (1 - hidden_states[t]**2) * d_h\n",
        "        d_b_hidden += d_f\n",
        "        \n",
        "        # Backpropagate into weights U\n",
        "        # YOUR CODE HERE!\n",
        "        d_U += np.dot(d_f, inputs[t].T)\n",
        "        \n",
        "        # Backpropagate into weights V\n",
        "        # YOUR CODE HERE!\n",
        "        d_V += np.dot(d_f, hidden_states[t-1].T)\n",
        "        d_h_next = np.dot(V.T, d_f)\n",
        "    \n",
        "    # Pack gradients\n",
        "    grads = d_U, d_V, d_W, d_b_hidden, d_b_out    \n",
        "    \n",
        "    # Clip gradients\n",
        "    grads = clip_gradient_norm(grads)\n",
        "    \n",
        "    return loss, grads\n",
        "\n",
        "\n",
        "loss, grads = backward_pass(test_input, outputs, hidden_states, test_target, params)\n",
        "\n",
        "print('We get a loss of:')\n",
        "print(loss)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We get a loss of:\n",
            "4.343361021444801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bm1b3HX7JcW"
      },
      "source": [
        "### Optimization\n",
        "\n",
        "Now that we can do forward passes and compute gradients with backpropagation, we're ready to train our network. For that we will need an optimizer. A common and easy to implement optimization method is stochastic gradient descent (SGD), which has the update rule: $\\theta_{n+1} = \\theta_{n} - \\eta \\frac{\\partial E}{\\partial \\theta_{n}}$, where $\\eta$ is the learning rate and $E$ is our cost function. This is essentially what's going on behind the scenes when you run `optimizer.step()` in PyTorch using the SGD optimizer. If you want to learn more about optimization in a deep learning context, [this is a great starting point](https://arxiv.org/abs/1609.04747)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f1Wpb-47J-Y"
      },
      "source": [
        "def update_parameters(params, grads, lr=1e-3):\n",
        "    # Take a step\n",
        "    for param, grad in zip(params, grads):\n",
        "        param -= lr * grad\n",
        "    \n",
        "    return params"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRh9jnM4FVrs"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "We need to define a full training loop with a forward pass, backward pass, optimization step and validation. Training will take approximately 5 minutes, so you might want to read on while the notebook is running."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "3cUuP7kUFP2W",
        "outputId": "b7ef1f89-76e1-4271-d6fc-4d1fd2b95ffb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 1000\n",
        "\n",
        "# Initialize a new network\n",
        "params = init_rnn(hidden_size=HIDDEN_SIZE, vocab_size=VOCAB_SIZE)\n",
        "\n",
        "# Initialize hidden state as zeros\n",
        "hidden_state = np.zeros((HIDDEN_SIZE, 1))\n",
        "\n",
        "# Track loss\n",
        "training_loss, validation_loss = [], []\n",
        "\n",
        "# For each epoch\n",
        "for i in range(num_epochs):\n",
        "    \n",
        "    # Track loss\n",
        "    epoch_training_loss = 0\n",
        "    epoch_validation_loss = 0\n",
        "    \n",
        "     # For each sentence in validation set\n",
        "    for inputs, targets in val_set:\n",
        "        \n",
        "        # One-hot encode input and target sequence\n",
        "        inputs_one_hot = one_hot_encode_sequence(inputs)\n",
        "        targets_one_hot = one_hot_encode_sequence(targets)\n",
        "        \n",
        "        # Re-initialize hidden state\n",
        "        hidden_state = np.zeros_like(hidden_state)\n",
        "\n",
        "        # Forward pass\n",
        "        # YOUR CODE HERE!\n",
        "        outputs, hidden_states = forward_pass(\n",
        "            inputs_one_hot,\n",
        "            hidden_state,\n",
        "            params\n",
        "        )\n",
        "\n",
        "        # Backward pass\n",
        "        # YOUR CODE HERE!\n",
        "        loss, _ = backward_pass(\n",
        "            inputs_one_hot,\n",
        "            outputs,\n",
        "            hidden_states,\n",
        "            targets_one_hot,\n",
        "            params\n",
        "        )\n",
        "        \n",
        "        # Update loss\n",
        "        epoch_validation_loss += loss\n",
        "    \n",
        "    # For each sentence in training set\n",
        "    for inputs, targets in train_set:\n",
        "        \n",
        "        # One-hot encode input and target sequence\n",
        "        inputs_one_hot = one_hot_encode_sequence(inputs)\n",
        "        targets_one_hot = one_hot_encode_sequence(targets)\n",
        "        \n",
        "        # Re-initialize hidden state\n",
        "        hidden_state = np.zeros_like(hidden_state)\n",
        "\n",
        "        # Forward pass\n",
        "        # YOUR CODE HERE!\n",
        "        outputs, hidden_states = forward_pass(\n",
        "            inputs_one_hot,\n",
        "            hidden_state,\n",
        "            params\n",
        "        )\n",
        "\n",
        "        # Backward pass\n",
        "        # YOUR CODE HERE!\n",
        "        loss, grads = backward_pass(\n",
        "            inputs_one_hot,\n",
        "            outputs,\n",
        "            hidden_states,\n",
        "            targets_one_hot,\n",
        "            params\n",
        "        )\n",
        "        \n",
        "        if np.isnan(loss):\n",
        "            raise ValueError('Gradients have vanished/exploded!')\n",
        "        \n",
        "        # Update parameters\n",
        "        # YOUR CODE HERE!\n",
        "        params = update_parameters(params, grads, lr=1e-3)\n",
        "        \n",
        "        # Update loss\n",
        "        epoch_training_loss += loss\n",
        "        \n",
        "    # Save loss for plot\n",
        "    training_loss.append(epoch_training_loss/len(train_set))\n",
        "    validation_loss.append(epoch_validation_loss/len(val_set))\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if i % 100 == 0:\n",
        "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
        "\n",
        "\n",
        "# Get first sentence in test set\n",
        "inputs, targets = test_set[1]\n",
        "\n",
        "# One-hot encode input and target sequence\n",
        "inputs_one_hot = one_hot_encode_sequence(inputs)\n",
        "targets_one_hot = one_hot_encode_sequence(targets)\n",
        "\n",
        "# Initialize hidden state as zeros\n",
        "hidden_state = np.zeros((HIDDEN_SIZE, 1))\n",
        "\n",
        "# Forward pass\n",
        "outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params)\n",
        "output_sentence = [idx_to_word[np.argmax(output)] for output in outputs]\n",
        "print('Input sentence:')\n",
        "print(inputs)\n",
        "\n",
        "print('\\nTarget sequence:')\n",
        "print(targets)\n",
        "\n",
        "print('\\nPredicted sequence:')\n",
        "print([idx_to_word[np.argmax(output)] for output in outputs])\n",
        "\n",
        "# Plot training and validation loss\n",
        "epoch = np.arange(len(training_loss))\n",
        "plt.figure()\n",
        "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
        "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, training loss: 4.533073143254339, validation loss: 4.803672524872286\n",
            "Epoch 100, training loss: 1.4988155705003372, validation loss: 1.582402058607385\n",
            "Epoch 200, training loss: 1.0702117369140383, validation loss: 1.140875855944345\n",
            "Epoch 300, training loss: 1.0195988311873245, validation loss: 1.1383936781951538\n",
            "Epoch 400, training loss: 1.0206038315120267, validation loss: 1.1715226369805374\n",
            "Epoch 500, training loss: 1.0212138262451527, validation loss: 1.197553568370234\n",
            "Epoch 600, training loss: 1.0522706610511043, validation loss: 1.2305981092291913\n",
            "Epoch 700, training loss: 1.2041279011896884, validation loss: 1.3795271029694354\n",
            "Epoch 800, training loss: 1.5784430455774539, validation loss: 1.7381703010302614\n",
            "Epoch 900, training loss: 1.967908911383222, validation loss: 2.2458325199415303\n",
            "Input sentence:\n",
            "aaaabbbb\n",
            "\n",
            "Target sequence:\n",
            "aaabbbbE\n",
            "\n",
            "Predicted sequence:\n",
            "['b', 'b', 'b', 'a', 'b', 'b', 'b', 'E']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj4N_YkTFPzY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKIqBCaTFPwx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ERzijUzFPuK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho-K7SGPFPrF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}