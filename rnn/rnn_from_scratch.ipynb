{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_from_scratch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMF0qDCF3G5lqRGUnG+BQoC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/rnn/rnn_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzjc2dlH2GsQ"
      },
      "source": [
        "# Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUVIbX8I4_gR"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from numpy.random import randint\n",
        "from collections import OrderedDict\n",
        "from torch.utils import data\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uzAyRHP2U7H"
      },
      "source": [
        "## Representing text as tokens\n",
        "\n",
        "Let's define our dataset samples $x \\in \\mathrm{R}^d$, where $d$ is the feature space dimension.\n",
        "\n",
        "With time sequences our data can be represented as $x \\in \\mathrm{R}^{t \\, \\times \\, d}$, where $t$ is the sequence length. \n",
        "This emphasises sequence dependence and that the samples along the sequence are not independent and identically distributed (i.i.d.).\n",
        "We will model functions as $\\mathrm{R}^{t \\, \\times \\, d} \\rightarrow \\mathrm{R}^c$, where $c$ is the amount of classes in the output.\n",
        "\n",
        "There are several ways to represent sequences. With text, the challenge is how to represent a word as a feature vector in $d$ dimensions, as we are required to represent text with decimal numbers in order to apply neural networks to it.\n",
        "\n",
        "Initially, we will use a simple one-hot encoding but for categorical variables that can take on many values (e.g. words in the English language)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-DRDHhQ3zs7"
      },
      "source": [
        "### One-hot encoding over vocabulary\n",
        "\n",
        "One way to represent a fixed amount of words is by making a one-hot encoded vector, which consists of 0s in all cells with the exception of a single 1 in a cell used uniquely to identify each word.\n",
        "\n",
        "| vocabulary    | one-hot encoded vector   |\n",
        "| ------------- |--------------------------|\n",
        "| Paris         | $= [1, 0, 0, \\ldots, 0]$ |\n",
        "| Rome          | $= [0, 1, 0, \\ldots, 0]$ |\n",
        "| Copenhagen    | $= [0, 0, 1, \\ldots, 0]$ |\n",
        "\n",
        "Representing a large vocabulary with one-hot encodings often becomes inefficient because of the size of each sparse vector.\n",
        "To overcome this challenge it is common practice to truncate the vocabulary to contain the $k$ most used words and represent the rest with a special symbol, $\\mathtt{UNK}$, to define unknown/unimportant words.\n",
        "This often causes entities such as names to be represented with $\\mathtt{UNK}$ because they are rare.\n",
        "\n",
        "Consider the following text\n",
        "> I love the corny jokes in Spielberg's new movie.\n",
        "\n",
        "where an example result would be similar to\n",
        "> I love the corny jokes in $\\mathtt{UNK}$'s new movie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ag97wDL4Ubv"
      },
      "source": [
        "### Generating a dataset\n",
        "\n",
        "We generate sequences of the form:\n",
        "\n",
        "`a b EOS`,\n",
        "\n",
        "`a a b b EOS`,\n",
        "\n",
        "`a a a a a b b b b b EOS`\n",
        "\n",
        "where `EOS` is a special character denoting the end of a sequence. The task is to predict the next token $t_n$, i.e. `a`, `b`, `EOS` or the unknown token `UNK` given a sequence of tokens $\\{ t_{1}, t_{2}, \\dots , t_{n-1}\\}$, and we are to process sequences in a sequential manner. As such, the network will need to learn that e.g. 5 `b`s and an `EOS` token will be preceded by 5 `a`s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVm_7FMdRtWL"
      },
      "source": [
        "CHARS = ['a', 'b']\n",
        "UNKNOWN = 'U'\n",
        "EOS = 'E'\n",
        "VOCAB = [UNKNOWN, EOS] + CHARS\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "NUM_SENTENCES = 2**8\n",
        "HIDDEN_SIZE = 50\n",
        "vocab_size = len(word_to_idx)\n",
        "P_TRAIN = int(NUM_SENTENCES * 0.8)\n",
        "P_VAL = int(NUM_SENTENCES * 0.1)\n",
        "P_TEST = int(NUM_SENTENCES * 0.1)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSNcnPbA2AEj"
      },
      "source": [
        "def generate_dataset(num_sequences):\n",
        "  \"\"\"\n",
        "  Generated a number of sequences as out dataset.\n",
        "  \"\"\"\n",
        "  generate_random_token = lambda num_tokens: (\n",
        "      ''.join([c * num_tokens for c in CHARS]) + EOS\n",
        "  )\n",
        "  return [generate_random_token(randint(1, 12)) for _ in range(num_sequences)]"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLtzsitq7Ekw",
        "outputId": "8bd61409-6d05-4e66-c0ae-4cbdad1f71c1"
      },
      "source": [
        "sequences = generate_dataset(NUM_SENTENCES)\n",
        "sequences[:5]"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aaaaabbbbbE',\n",
              " 'aaaaabbbbbE',\n",
              " 'aaaaaaaaaaabbbbbbbbbbbE',\n",
              " 'aaaaaaabbbbbbbE',\n",
              " 'aaaaaaaaabbbbbbbbbE']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGpn9GHR9IHR"
      },
      "source": [
        "## Representing tokens as indices\n",
        "\n",
        "To build a one-hot encoding, we need to assign each possible word in our vocabulary an index. We do that by creating two dictionaries: one that allows us to go from a given word to its corresponding index in our vocabulary, and one for the reverse direction. Let's call them `word_to_idx` and `idx_to_word`. The keyword `vocab_size` specifies the maximum size of our vocabulary. If we try to access a word that does not exist in our vocabulary, it is automatically replaced by the `UNK` token or its corresponding index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-ArjKF4BZJ8"
      },
      "source": [
        "word_to_idx = OrderedDict((word, index) for index, word in enumerate(VOCAB)) \n",
        "idx_to_word = OrderedDict((index, word) for index, word in enumerate(VOCAB))\n",
        "vocab_size = len(VOCAB)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuiEBnlwXe-U"
      },
      "source": [
        "## Partitioning the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VafQndpbXW46"
      },
      "source": [
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, inputs, targets):\n",
        "    self.inputs = inputs\n",
        "    self.targets = targets\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.targets)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    X = self.inputs[index]\n",
        "    y = self.targets[index]\n",
        "    return X, y"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGhRyIYrXW2G",
        "outputId": "b57a72aa-5546-4389-a7cf-5cf318a8d7fc"
      },
      "source": [
        "inputs = [sequences[i][:-1] for i in range(len(sequences))]\n",
        "targets = [sequences[i][1:] for i in range(len(sequences))]\n",
        "train_set = Dataset(inputs[:P_TRAIN], targets[:P_TRAIN])\n",
        "val_set = Dataset(inputs[P_TRAIN:P_TRAIN + P_VAL], targets[P_TRAIN:P_TRAIN + P_VAL])\n",
        "test_set = Dataset(inputs[-P_TEST:], targets[-P_TEST:])\n",
        "\n",
        "print(f'We have {len(train_set)} samples in the training set.')\n",
        "print(f'We have {len(val_set)} samples in the validation set.')\n",
        "print(f'We have {len(test_set)} samples in the test set.')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 204 samples in the training set.\n",
            "We have 25 samples in the validation set.\n",
            "We have 25 samples in the test set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQGXexzgkY0Y"
      },
      "source": [
        "## One-hot encodings\n",
        "We now create a simple function that returns the one-hot encoded representation of a given index of a word in our vocabulary. Notice that the shape of the one-hot encoding is equal to the entire vocabulary (which can be huge!). Additionally, we define a function to automatically one-hot encode a sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plzXFcMvXWze",
        "outputId": "0d312bc1-4b09-42d9-c972-807fe9be3807"
      },
      "source": [
        "def one_hot_encode(idx):\n",
        "    '''\n",
        "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
        "    \n",
        "    Args:\n",
        "     `idx`: the index of the given word\n",
        "     `vocab_size`: the size of the vocabulary\n",
        "    \n",
        "    Returns a 1-D numpy array of length `vocab_size`.\n",
        "    '''\n",
        "    # Initialize the encoded array\n",
        "    one_hot = np.zeros(VOCAB_SIZE)\n",
        "    \n",
        "    # Set the appropriate element to one\n",
        "    one_hot[idx] = 1.0\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "def one_hot_encode_sequence(sequence):\n",
        "    '''\n",
        "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
        "    \n",
        "    Args:\n",
        "     `sentence`: a list of words to encode\n",
        "     `vocab_size`: the size of the vocabulary\n",
        "     \n",
        "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
        "    '''\n",
        "    # Encode each word in the sentence\n",
        "    encoding = np.array(\n",
        "        [one_hot_encode(word_to_idx[word]) for word in sequence]\n",
        "    )\n",
        "\n",
        "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
        "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
        "    \n",
        "    return encoding\n",
        "\n",
        "test_word = one_hot_encode(word_to_idx['a'])\n",
        "print(f'One-hot encoding of \"a\" has shape {test_word.shape} -> {test_word}')\n",
        "\n",
        "test_sent = one_hot_encode_sequence(['a', 'b'])\n",
        "print(f'One-hot encoding of \"a b\" has shape {test_sent.shape} -> {test_sent}')"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One-hot encoding of \"a\" has shape (4,) -> [0. 0. 1. 0.]\n",
            "One-hot encoding of \"a b\" has shape (2, 4, 1) -> [[[0.]\n",
            "  [0.]\n",
            "  [1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [1.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kafwb0TEo4Nv"
      },
      "source": [
        "# Recurrent Neural Networks (RNN)\n",
        "___\n",
        "\n",
        "A RNN performs its computations in a cyclic manner, where the same computation is applied to every sample of a given sequence.\n",
        "The idea is that the network should be able to use the previous computations as some form of memory and apply this to future computations.\n",
        "\n",
        "![rnn-unroll image](https://github.com/martin-fabbri/colab-notebooks/raw/master/rnn/images/rnn-folded-unfolded.png)\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "- $X$ is the input sequence of samples, \n",
        "- $U$ is a weight matrix applied to the given input sample,\n",
        "- $V$ is a weight matrix used for the recurrent computation in order to pass memory along the sequence,\n",
        "- $W$ is a weight matrix used to compute the output of the every timestep (given that every timestep requires an output),\n",
        "- $h$ is the hidden state (the network's memory) for a given time step, and\n",
        "- $o$ is the resulting output.\n",
        "\n",
        "When the network is unrolled as shown, it is easier to refer to a timestep, $t$.\n",
        "We have the following computations through the network:\n",
        "\n",
        "- $h_t = f(U\\,{x_t} + V\\,{h_{t-1}})$, where $f$ is a non-linear activation function, e.g. $\\mathrm{tanh}$.\n",
        "- $o_t = W\\,{h_t}$\n",
        "\n",
        "When we are doing language modelling using a cross-entropy loss, we additionally apply the softmax function to the output $o_{t}$:\n",
        "\n",
        "- $\\hat{y}_t = \\mathrm{softmax}(o_{t})$\n",
        "\n",
        "\n",
        "### Backpropagation through time\n",
        "\n",
        "We define a loss function\n",
        "\n",
        "- $E = \\sum_t E_t  = \\sum_t E_t(y_t ,\\hat{y}_t ) \\ , $\n",
        "\n",
        "where $E_t(y_t ,\\hat{y}_t )$ is the cross-entropy function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a89XCuTm1YZi"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "We will implement the forward pass, backward pass, optimization and training loop for an RNN in numpy so that you can get familiar with the recurrent nature of RNNs. Later, we will go back to PyTorch and appreciate how convenient the implementation becomes!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTrF5TmXekFa",
        "outputId": "b988ac54-6d3e-4dfe-bdb3-0bf7cc3ca26f"
      },
      "source": [
        "def init_orthogonal(param):\n",
        "    \"\"\"\n",
        "    Initializes weight parameters orthogonally.\n",
        "    This is a common initiailization for recurrent neural networks.\n",
        "    \n",
        "    Refer to this paper for an explanation of this initialization:\n",
        "    https://arxiv.org/abs/1312.6120\n",
        "    \"\"\"\n",
        "    if param.ndim < 2:\n",
        "        raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
        "\n",
        "    rows, cols = param.shape\n",
        "    \n",
        "    new_param = np.random.randn(rows, cols)\n",
        "    \n",
        "    if rows < cols:\n",
        "        new_param = new_param.T\n",
        "    \n",
        "    # Compute QR factorization\n",
        "    q, r = np.linalg.qr(new_param)\n",
        "    \n",
        "    # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n",
        "    d = np.diag(r, 0)\n",
        "    ph = np.sign(d)\n",
        "    q *= ph\n",
        "\n",
        "    if rows < cols:\n",
        "        q = q.T\n",
        "    \n",
        "    new_param = q\n",
        "    \n",
        "    return new_param\n",
        "\n",
        "\n",
        "def init_rnn(hidden_size, vocab_size):\n",
        "    \"\"\"\n",
        "    Initializes our recurrent neural network.\n",
        "    \n",
        "    Args:\n",
        "     `hidden_size`: the dimensions of the hidden state\n",
        "     `vocab_size`: the dimensions of our vocabulary\n",
        "    \"\"\"\n",
        "    # Weight matrix (input to hidden state)\n",
        "    # YOUR CODE HERE!\n",
        "    U = np.zeros((hidden_size, vocab_size))\n",
        "\n",
        "    # Weight matrix (recurrent computation)\n",
        "    # YOUR CODE HERE!\n",
        "    V = np.zeros((hidden_size, hidden_size))\n",
        "\n",
        "    # Weight matrix (hidden state to output)\n",
        "    # YOUR CODE HERE!\n",
        "    W = np.zeros((vocab_size, hidden_size))\n",
        "\n",
        "    # Bias (hidden state)\n",
        "    # YOUR CODE HERE!\n",
        "    b_hidden = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # Bias (output)\n",
        "    # YOUR CODE HERE!\n",
        "    b_out = np.zeros((vocab_size, 1))\n",
        "    \n",
        "    # Initialize weights\n",
        "    U = init_orthogonal(U)\n",
        "    V = init_orthogonal(V)\n",
        "    W = init_orthogonal(W)\n",
        "    \n",
        "    # Return parameters as a tuple\n",
        "    return U, V, W, b_hidden, b_out\n",
        "\n",
        "\n",
        "params = init_rnn(hidden_size=hidden_size, vocab_size=vocab_size)\n",
        "print('U:', params[0].shape)\n",
        "print('V:', params[1].shape)\n",
        "print('W:', params[2].shape)\n",
        "print('b_hidden:', params[3].shape)\n",
        "print('b_out:', params[4].shape)\n",
        "\n",
        "for param in params:\n",
        "    assert param.ndim == 2, \\\n",
        "        'all parameters should be 2-dimensional '\\\n",
        "        '(hint: a dimension can simply have size 1)'"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "U: (50, 4)\n",
            "V: (50, 50)\n",
            "W: (4, 50)\n",
            "b_hidden: (50, 1)\n",
            "b_out: (4, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUycuHlB54ux"
      },
      "source": [
        "#### Sigmoid activation\n",
        "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4Ukog4IfO7d"
      },
      "source": [
        "def sigmoid(x, derivative=False):\n",
        "    '''\n",
        "    Computes the element-wise sigmoid activation function for an array x.\n",
        "\n",
        "    Args:\n",
        "     `x`: the array where the function is applied\n",
        "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
        "    '''\n",
        "    x_safe = x + 1e-12\n",
        "    f = 1 / (1 + np.exp(-x_safe))\n",
        "    \n",
        "    if derivative: # Return the derivative of the function evaluated at x\n",
        "        return f * (1 - f)\n",
        "    else: # Return the forward pass of the function at x\n",
        "        return f"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MzBPta86Kl0"
      },
      "source": [
        "#### Hyperbolic Tangent activation\n",
        "\n",
        "$\\tanh{x} = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C-tPo7d6IM6"
      },
      "source": [
        "def tanh(x, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the element-wise tanh activation function for an array x.\n",
        "\n",
        "    Args:\n",
        "     `x`: the array where the function is applied\n",
        "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
        "    \"\"\"\n",
        "    x_safe = x + 1e-12\n",
        "    f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
        "    \n",
        "    if derivative: # Return the derivative of the function evaluated at x\n",
        "        return 1-f**2\n",
        "    else: # Return the forward pass of the function at x\n",
        "        return f"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9w3J78d6Ppl"
      },
      "source": [
        "#### Softmax\n",
        "\n",
        "$\\mathrm{softmax}(x) = \\frac{e^{x_{i}}}{\\sum_{j=1}^{K} e^{x_{j}}} \\text { for } i=1, \\ldots, K \\text { and } \\mathbf{x}=\\left(x_{1}, \\ldots, x_{K}\\right) \\in \\mathbb{R}^{K}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avSkIazH6PZ9"
      },
      "source": [
        "def softmax(x, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the softmax for an array x.\n",
        "    \n",
        "    Args:\n",
        "     `x`: the array where the function is applied\n",
        "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
        "    \"\"\"\n",
        "    x_safe = x + 1e-12\n",
        "    f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
        "    \n",
        "    if derivative: # Return the derivative of the function evaluated at x\n",
        "        pass # We will not need this one\n",
        "    else: # Return the forward pass of the function at x\n",
        "        return f"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khzkiR7r6bpe"
      },
      "source": [
        "### Implement the forward \n",
        "\n",
        "Now that we have all the definitions in place, we can start to implement a forward pass.\n",
        "\n",
        "- $h_t = f(U\\,{x_t} + V\\,{h_{t-1}})$, where $f$ is a non-linear activation function, e.g. $\\mathrm{tanh}$.\n",
        "- $o_t = W\\,{h_t}$\n",
        "- $\\hat{y}_t = \\mathrm{softmax}(o_{t})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpaUtjW76PXK",
        "outputId": "b53b41e4-2dd9-4d61-ff58-15b45fbd0dd3"
      },
      "source": [
        "def forward_pass(inputs, hidden_state, params, activation_func=tanh):\n",
        "    \"\"\"\n",
        "    Computes the forward pass of a vanilla RNN.\n",
        "    \n",
        "    Args:\n",
        "     `inputs`: sequence of inputs to be processed\n",
        "     `hidden_state`: an already initialized hidden state\n",
        "     `params`: the parameters of the RNN\n",
        "    \"\"\"\n",
        "    # First we unpack our parameters\n",
        "    U, V, W, b_hidden, b_out = params\n",
        "    \n",
        "    # Create a list to store outputs and hidden states\n",
        "    outputs, hidden_states = [], []\n",
        "    \n",
        "    # For each element in input sequence\n",
        "    for t in range(len(inputs)):\n",
        "\n",
        "        # Compute new hidden state\n",
        "        # YOUR CODE HERE!\n",
        "        hidden_state = activation_func(\n",
        "            np.dot(U, inputs[t]) + np.dot(V, hidden_state) + b_hidden\n",
        "        )\n",
        "\n",
        "        # Compute output\n",
        "        # YOUR CODE HERE!\n",
        "        out = softmax(np.dot(W, hidden_state) + b_out)\n",
        "        \n",
        "        # Save results and continue\n",
        "        outputs.append(out)\n",
        "        hidden_states.append(hidden_state.copy())\n",
        "    \n",
        "    return outputs, hidden_states\n",
        "\n",
        "\n",
        "# Get first sequence in training set\n",
        "test_input_sequence, test_target_sequence = train_set[0]\n",
        "\n",
        "# One-hot encode input and target sequence\n",
        "test_input = one_hot_encode_sequence(test_input_sequence)\n",
        "test_target = one_hot_encode_sequence(test_target_sequence)\n",
        "\n",
        "# Initialize hidden state as zeros\n",
        "hidden_state = np.zeros((hidden_size, 1))\n",
        "\n",
        "# Now let's try out our new function\n",
        "outputs, hidden_states = forward_pass(test_input, hidden_state, params)\n",
        "\n",
        "print('Input sequence:')\n",
        "print(test_input_sequence)\n",
        "\n",
        "print('\\nTarget sequence:')\n",
        "print(test_target_sequence)\n",
        "\n",
        "print('\\nPredicted sequence:')\n",
        "print([idx_to_word[np.argmax(output)] for output in outputs])"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence:\n",
            "aaaaabbbbb\n",
            "\n",
            "Target sequence:\n",
            "aaaabbbbbE\n",
            "\n",
            "Predicted sequence:\n",
            "['a', 'b', 'b', 'a', 'b', 'b', 'E', 'E', 'E', 'E']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_lSF0Fc6PUX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MNlc6uv6PRx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQWcdFRH6POl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}