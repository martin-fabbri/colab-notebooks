{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "subword_tokenization_sentencepiece.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO9Wt4r04un7EfWwwX3cv1J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/subword_tokenization/subword_tokenization_sentencepiece.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6Rka-bbxf0w"
      },
      "source": [
        "## Install and data preparation\n",
        "\n",
        "We use the small training data (botchan.txt) in this example. \n",
        "([Botchan](https://en.wikipedia.org/wiki/Botchan) is a novel written by Natsume Sōseki in 1906.  The sample is English-translated one.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUjuQ1IlerGi"
      },
      "source": [
        "!pip install sentencepiece -Uqq\n",
        "!wget -q https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4FAsE-ByJNV"
      },
      "source": [
        "## Basic end-to-end example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl9PD__6zOyz"
      },
      "source": [
        "import sentencepiece as spm\n",
        "import tensorflow as tf"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xbz_QhVBxtgm",
        "outputId": "aa8ba59e-b2ec-4615-a55b-fe976dcc8b68"
      },
      "source": [
        "# Train sentencepiece model from 'botchan.txt' and makes 'm.model' and 'm.vocab'\n",
        "# 'm.vocab' is just a reference. Not used in the segmentation.\n",
        "train_args = '--input=botchan.txt --model_prefix=m --vocab_size=2000'\n",
        "spm.SentencePieceTrainer.train(train_args)\n",
        "\n",
        "# makes segmenter instance and loads the model file(m.model)\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# encode: text => id\n",
        "print(sp.encode_as_pieces('This is a test'))\n",
        "print(sp.encode_as_ids('This is a test'))\n",
        "\n",
        "# decode: id => text\n",
        "print(sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))\n",
        "print(sp.decode_ids([212, 32, 10, 587, 446]))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁This', '▁is', '▁a', '▁t', 'est']\n",
            "[212, 32, 10, 587, 446]\n",
            "This is a test\n",
            "This is a test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyCl1PgYz3Ux",
        "outputId": "e3ea99a1-5e20-46ed-df0a-6a733a23bbb4"
      },
      "source": [
        "# returns vocab size\n",
        "print(sp.get_piece_size())\n",
        "\n",
        "# is <=> piece conversion\n",
        "print(sp.id_to_piece(229))\n",
        "print(sp.piece_to_id('_This'))\n",
        "\n",
        "# returns 0 for unknown tokens (we can change the id for UNK)\n",
        "print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\n",
        "\n",
        "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
        "# <s> and </s> are defined as 'control' symbol.\n",
        "for id in range(3):\n",
        "  print(sp.id_to_piece(id), sp.is_control(id))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n",
            "▁W\n",
            "0\n",
            "0\n",
            "<unk> False\n",
            "<s> True\n",
            "</s> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fasXL72t5ZeM"
      },
      "source": [
        "## Loads model from byte stream\n",
        "\n",
        "Sentencepiece's model file is just a serialized [protocol buffer](https://developers.google.com/protocol-buffers/). We can instantiate sentencepiece processor from byte object with **load_from_serialized_proto** method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVF5zRg216ct",
        "outputId": "d1a949f8-5228-4e50-a441-2e3707f37d0e"
      },
      "source": [
        "# assumens that m.model is stored in non_posix file system.\n",
        "serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load_from_serialized_proto(serialized_model_proto)\n",
        "\n",
        "print(sp.encode_as_pieces('this is a test'))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁this', '▁is', '▁a', '▁t', 'est']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2Mo39QW9rXp"
      },
      "source": [
        "## User defined and control symbols\n",
        "\n",
        "We can define special tokens (symbols) to tweak the DNN behavior through the tokens.   Typical examples are  [BERT](https://arxiv.org/abs/1810.04805)'s special symbols., e.g., [SEP] and [CLS].\n",
        "\n",
        "There are two types of special tokens:\n",
        "\n",
        "- **user defined symbols**: Always treated as one token in any context. These symbols can appear in the input sentence. \n",
        "- **control symbol**:  We only reserve ids for these tokens. Even if these tokens appear in the input text, they are not handled as one token. User needs to insert ids explicitly after encoding.\n",
        "\n",
        "For experimental purpose, user defined symbols are easier to use since user can change the behavior just by modifying the input text. However,  we want to use control symbols in the production setting in order to avoid users from tweaking the behavior by feeding these special symbols in their input text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxfxpMfi89_x",
        "outputId": "571b527e-a7c6-4b4b-9c60-10256cd78072"
      },
      "source": [
        "cs = '--user_defined_symbols=<sep>,<cls>'\n",
        "train_args = f'--input=botchan.txt --model_prefix=m_ctrl --vocab_size=2000 {cs}'\n",
        "spm.SentencePieceTrainer.train(train_args)\n",
        "\n",
        "sp_ctrl = spm.SentencePieceProcessor()\n",
        "sp_ctrl.load('m_ctrl.model')\n",
        "\n",
        "# control symbols just reserve ids.\n",
        "print(sp_ctrl.encode_as_pieces('this is a test<sep> hello world<cls>'))\n",
        "print(sp_ctrl.piece_to_id('<sep>'))  # 3\n",
        "print(sp_ctrl.piece_to_id('<cls>'))  # 4\n",
        "print('3=', sp_ctrl.decode_ids([3]))  # decoded to empty\n",
        "print('4=', sp_ctrl.decode_ids([4]))  # decoded to empty"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁this', '▁is', '▁a', '▁t', 'est', '<sep>', '▁he', 'll', 'o', '▁world', '<cls>']\n",
            "3\n",
            "4\n",
            "3= <sep>\n",
            "4= <cls>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "BJVgey16-yor",
        "outputId": "541aa39e-0b45-4134-d6ef-8951dcb05326"
      },
      "source": [
        "print(train_args)\n",
        "'--input=botchan.txt --model_prefix=m_ctrl --vocab_size=2000 --control_symbols=<sep>,<cls>'"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--input=botchan.txt --model_prefix=m_ctrl --vocab_size=2000 --control_symbols=<sep>,<cls>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'--input=botchan.txt --model_prefix=m_ctrl --vocab_size=2000 --control_symbols=<sep>,<cls>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-TTuvo9_N8p",
        "outputId": "07c16f51-0e87-4398-8683-11532db2486c"
      },
      "source": [
        "## Example of user defined symbols\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_user2 --user_defined_symbols=<sep>,<cls> --vocab_size=2000')\n",
        "\n",
        "sp_user = spm.SentencePieceProcessor()\n",
        "sp_user.load('m_user2.model')\n",
        "\n",
        "# ids are reserved in both mode.\n",
        "# <unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
        "# user defined symbols allow these symbol to apper in the text.\n",
        "print(sp_user.encode_as_pieces('this is a test<sep> hello world<cls>'))\n",
        "print(sp_user.piece_to_id('<sep>'))  # 3\n",
        "print(sp_user.piece_to_id('<cls>'))  # 4\n",
        "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>\n",
        "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁this', '▁is', '▁a', '▁t', 'est', '<sep>', '▁he', 'll', 'o', '▁world', '<cls>']\n",
            "3\n",
            "4\n",
            "3= <sep>\n",
            "4= <cls>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrP434LBAo2y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}