{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pytorch: 1.7.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, math, sys\n",
    "import glob, itertools\n",
    "import argparse, random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import vgg19\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"pytorch: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"images\", exist_ok=True)\n",
    "os.makedirs(\"saved_models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Is CUDA available? True\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(f\"Is CUDA available? {cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "n_epochs = 2\n",
    "dataset_path = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\"\n",
    "batch_size = 16\n",
    "lr = 0.00008\n",
    "b1 = 0.05\n",
    "b2 = 0.999\n",
    "\n",
    "decay_epochs = 100\n",
    "n_cpu = 8\n",
    "hr_height = 256\n",
    "hr_width = 256\n",
    "channels = 3\n",
    "\n",
    "hr_shape = (hr_height, hr_width)\n",
    "hr_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, files, hr_shape):\n",
    "        hr_height, hr_width = hr_shape\n",
    "        self.lr_transform = transforms.Compose([\n",
    "            transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "        self.hr_transform = transforms.Compose([\n",
    "            transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "        self.files = files\n",
    "\n",
    "    def __getitems__(self, index):\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        img_lr = self.lr_transform(img)\n",
    "        img_hr = self.hr_transform(img)\n",
    "        return {\"lr\": img_lr, \"hr\": img_hr}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, test_paths = train_test_split(\n",
    "    sorted(glob.glob(f\"{dataset_path}/*.*\")), test_size=0.02, random_state=42\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    ImageDataset(train_paths, hr_shape=hr_shape),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=n_cpu,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    ImageDataset(test_paths, hr_shape=hr_shape),\n",
    "    batch_size=int(batch_size * 0.75),\n",
    "    shuffle=True,\n",
    "    num_workers=n_cpu,\n",
    ")"
   ]
  },
  {
   "source": [
    "![srgan](https://miro.medium.com/max/1000/1*zsiBj3IL4ALeLgsCeQ3lyA.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        vgg19_model = vgg19(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            *list(vgg19_model.features.children())[:18]\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.feature_extractor(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_features, 0.8),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_features, 0.8)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "        res_blocks = []\n",
    "        for _ in range(n_residual_blocks):\n",
    "            res_blocks.append(ResidualBlock(64))\n",
    "        self.res_blocks = nn.Sequential(*res_blocks)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "        )\n",
    "        upsampling = []\n",
    "        for out_features in range(2):\n",
    "            upsampling += [\n",
    "                nn.Conv2d(64, 256, 3, 1, 1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.PixelShuffle(upscale_factor=2),\n",
    "                nn.PReLU(),\n",
    "            ]\n",
    "        self.upsampling = nn.Sequential(*upsampling)\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1(x)\n",
    "        out = self.res_blocks(out1)\n",
    "        out2 = self.conv2(out)\n",
    "        out = torch.add(out1, out2)\n",
    "        out = self.upsampling(out)\n",
    "        out = self.conv3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "        res_blocks = []\n",
    "        for _ in range(n_residual_blocks):\n",
    "            res_blocks.append(ResidualBlock(64))\n",
    "        self.res_blocks = nn.Sequential(*res_blocks)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "        )\n",
    "        upsampling = []\n",
    "        for out_features in range(2):\n",
    "            upsampling += [\n",
    "                nn.Conv2d(64, 256, 3, 1, 1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.PixelShuffle(upscale_factor=2),\n",
    "                nn.PReLU(),\n",
    "            ]\n",
    "        self.upsampling = nn.Sequential(*upsampling)\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1(x)\n",
    "        out = self.res_blocks(out1)\n",
    "        out2 = self.conv2(out)\n",
    "        out = torch.add(out1, out2)\n",
    "        out = self.upsampling(out)\n",
    "        out = self.conv3(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        in_channels, in_height, in_width = self.input_shape\n",
    "        patch_h, patch_w = int(in_height / 2 ** 3), int(in_width / 2 ** 4)\n",
    "        self.output_shape = (1, patch_h, patch_w)\n",
    "        layers = []\n",
    "        in_filters = in_channels\n",
    "        for i, out_filters in enumerate([64, 128, 256, 512]):\n",
    "            layers.extend(self.discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
    "            in_filters = out_filters\n",
    "        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def discriminator_block(self, in_filters, out_filters, first_block=False):\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n",
    "        if not first_block:\n",
    "            layers.append(nn.BatchNorm2d(out_filters))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
    "        layers.append(nn.BatchNorm2d(out_filters))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "FeatureExtractor(\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "feature_extrator = FeatureExtractor()\n",
    "feature_extrator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GeneratorResNet()\n",
    "discriminator = Discriminator(input_shape=(channels, *hr_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_gan = torch.nn.MSELoss()\n",
    "criterion_content = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    feature_extrator = feature_extrator.cuda()\n",
    "    criterion_gan = criterion_gan.cuda()\n",
    "    criterion_content = criterion_content.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained_models = False\n",
    "if load_pretrained_models:\n",
    "    generator.load_state_dict(torch.load(\"/kaggle/input/single-image-super-resolution-gan-srgan-pytorch/saved_models/generator.pth\"))\n",
    "    discriminator.load_state_dict(torch.load(\"/kaggle/input/single-image-super-resolution-gan-srgan-pytorch/saved_models/discriminator.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_gan = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_losses, train_disc_losses, train_counter = [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}