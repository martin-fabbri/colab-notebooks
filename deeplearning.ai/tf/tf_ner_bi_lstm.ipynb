{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tf_ner_bi_lstm.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/tf/tf_ner_bi_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cx0HfnFKVPY8","cellView":"form","executionInfo":{"status":"ok","timestamp":1609254635888,"user_tz":480,"elapsed":36752,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"e9f6b6c3-88ee-47b9-96d6-5863158cb42f"},"source":["#@title Download Kaggle Dataset\r\n","#@markdown Dataset: Annotated Corpus for Named Entity Recognition <br>\r\n","#@markdown https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus\r\n","#@markdown ___\r\n","\r\n","kaggle_dataset_id = \"abhinavwalia95/entity-annotated-corpus\" #@param {type:\"string\"}\r\n","\r\n","!pip install -q kaggle\r\n","from google.colab import drive\r\n","drive.mount('/content/gdrive')\r\n","\r\n","!mkdir -p ~/.kaggle\r\n","!cp /content/gdrive/My\\ Drive/kaggle/kaggle.json ~/.kaggle/kaggle.json\r\n","!chmod 600 ~/.kaggle/kaggle.json\r\n","!kaggle datasets download -d {kaggle_dataset_id}\r\n","!ls -l /content\r\n","!unzip -o /content/entity-annotated-corpus\r\n","\r\n","#@markdown ___\r\n","#@markdown Install dependencies<br>\r\n","#@markdown - seqeval: Sequence labeling evaluation (F1, precision, etc).\r\n","#@markdown - fastprogress: Progress bar for Jupyter notebooks.\r\n","\r\n","!pip install -Uqq seqeval\r\n","!pip install -Uqq fastprogress"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","Downloading entity-annotated-corpus.zip to /content\n"," 34% 9.00M/26.4M [00:01<00:02, 7.57MB/s]\n","100% 26.4M/26.4M [00:01<00:00, 23.7MB/s]\n","total 27064\n","-rw-r--r-- 1 root root 27703149 Dec 29 15:10 entity-annotated-corpus.zip\n","drwx------ 5 root root     4096 Dec 29 15:10 gdrive\n","drwxr-xr-x 1 root root     4096 Dec 21 17:29 sample_data\n","Archive:  /content/entity-annotated-corpus.zip\n","  inflating: ner.csv                 \n","  inflating: ner_dataset.csv         \n","\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JiVNesPUlvmY","executionInfo":{"status":"ok","timestamp":1609254640409,"user_tz":480,"elapsed":605,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"6fb630f7-70d1-4f95-a80d-c345c67ce78a"},"source":["!nvidia-smi -L"],"execution_count":2,"outputs":[{"output_type":"stream","text":["GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-6d46fd1e-71f5-1920-f427-f626e0bf9d75)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cCcbeu8PWUTt","executionInfo":{"status":"ok","timestamp":1609256516459,"user_tz":480,"elapsed":663,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["import math\r\n","import pathlib\r\n","import shutil\r\n","import tempfile\r\n","\r\n","import numpy as np\r\n","import pandas as pd\r\n","import tensorflow as tf\r\n","import tensorflow.keras.backend as K\r\n","from fastprogress.fastprogress import master_bar, progress_bar\r\n","from numpy.random import seed\r\n","from seqeval.metrics import classification_report, f1_score\r\n","from sklearn.model_selection import train_test_split\r\n","from tensorflow.keras import Model, Sequential\r\n","from tensorflow.keras.callbacks import TensorBoard\r\n","from tensorflow.keras.layers import (\r\n","    LSTM,\r\n","    Bidirectional,\r\n","    Dense,\r\n","    Dropout,\r\n","    Embedding,\r\n","    InputSpec,\r\n","    Layer,\r\n","    SpatialDropout1D,\r\n","    TimeDistributed,\r\n",")\r\n","from tensorflow.keras.optimizers import Adam\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n","from tensorflow.keras.utils import to_categorical\r\n","from tensorflow.random import set_seed\r\n","from tensorflow_addons.text import crf_decode, crf_log_likelihood\r\n","\r\n","set_seed(42)\r\n","seed(42)\r\n","\r\n","logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorflow_logs\"\r\n","shutil.rmtree(logdir, ignore_errors=True)"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"BfoVNEISAdqT","executionInfo":{"status":"ok","timestamp":1609254649045,"user_tz":480,"elapsed":1219,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["PAD_TOKEN = \"PAD\"\r\n","PAD_INDEX = 0\r\n","PAD_TAG = \"O\""],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"xWqVJaak23jf","cellView":"form","executionInfo":{"status":"ok","timestamp":1609254908518,"user_tz":480,"elapsed":568,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["#@title Utils\r\n","#@markdown ```\r\n","#@markdown - build_vocab(): Extracts unique tokens and tags\r\n","#@markdown - build_indexes(): Builds the tokens and tags mapping indexes\r\n","#@markdown - decode_one_hot_tags_sequence():\r\n","#@markdown - decode_tags_batch()\r\n","#@markdown - test_inference()\r\n","#@markdown ```\r\n","\r\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n","\r\n","def configure_dataset(dataset):\r\n","    return dataset.cache().prefetch(buffer_size=AUTOTUNE)\r\n","\r\n","def build_vocab(data):\r\n","    tokens = {token for token in data[\"word\"]}\r\n","    tokens = {\"unk\" if t is math.nan or isinstance(t, float) else t for t in tokens}\r\n","\r\n","    tags = {tag for tag in data[\"tag\"]}\r\n","    tags = {\"O\" if t is math.nan or isinstance(t, float) else t for t in tags}\r\n","    return tokens, tags\r\n","\r\n","def build_tagged_senteces(data):\r\n","    agg_func = lambda s: [(w, t) for w, t in zip(s[\"word\"], s[\"tag\"])]\r\n","    grouped = data.groupby(\"sentence_idx\").apply(agg_func)\r\n","    sentences = [s for s in grouped]\r\n","    return sentences\r\n","\r\n","def build_indexes(tokens, tags):\r\n","    # token[0] is reserved for padding\r\n","    token2idx = {token: idx + 1 for idx, token in enumerate(tokens)}\r\n","    idx2token = {idx + 1: token for idx, token in enumerate(tokens)}\r\n","    idx2token[PAD_INDEX] = PAD_TOKEN\r\n","    token2idx[PAD_TOKEN] = PAD_INDEX \r\n","\r\n","    tag2idx = {tag: idx for idx, tag in enumerate(tags)}\r\n","    idx2tag = {idx: tag for idx, tag in enumerate(tags)}\r\n","    return token2idx, idx2token, tag2idx, idx2tag\r\n","\r\n","def tokenize(sentences, token2idx, tag2idx, one_hot_encode_tags=True):\r\n","    X = [[token2idx[t] for t, _ in s] for s in sentences]\r\n","    X = pad_sequences(X, maxlen=maxlen, padding=\"post\", value=PAD_INDEX)\r\n","\r\n","    y = [[tag2idx[t] for _, t in s] for s in sentences]\r\n","    y = pad_sequences(y, maxlen=maxlen, padding=\"post\", value=tag2idx[PAD_TAG])\r\n","    if one_hot_encode_tags:\r\n","        y = [to_categorical(tag_idx, num_classes=num_tags) for tag_idx in y]\r\n","    return X, np.array(y)\r\n","\r\n","def decode_one_hot_tags_sequence(tags_sequence):\r\n","    idx_tags = np.argmax(tags_sequence, axis=-1)\r\n","    return [idx2tag[idx] for idx in idx_tags]\r\n","\r\n","def decode_tags_batch(encoded_tags_sequences):\r\n","    return [decode_one_hot_tags_sequence(seq) for seq in encoded_tags_sequences]\r\n","\r\n","def test_inference(inference_model, test_dataset):\r\n","    true_labels = []\r\n","    pred_labels = []\r\n","\r\n","    num_test_batches = test_dataset.cardinality().numpy()\r\n","    for X_batch, y_true in progress_bar(test_dataset, total=num_test_batches):\r\n","        y_pred = inference_model.predict(X_batch)\r\n","        pred_labels.append(decode_tags_batch(y_pred))\r\n","        true_labels.append(decode_tags_batch(y_true.numpy()))\r\n","\r\n","    true_labels = np.array(true_labels)\r\n","    num_batches, num_samples, sentence_lenght = true_labels.shape\r\n","    true_labels = true_labels.reshape(num_batches * num_samples, sentence_lenght)\r\n","    true_labels = true_labels.tolist()\r\n","\r\n","    pred_labels = np.array(pred_labels)\r\n","    pred_labels = pred_labels.reshape(num_batches * num_samples, sentence_lenght)\r\n","    pred_labels = pred_labels.tolist()\r\n","\r\n","    return true_labels, pred_labels"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L1JpDNgHWNt4"},"source":["## Load the dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":266},"id":"GTYrpk2FVbAI","executionInfo":{"status":"ok","timestamp":1609256568306,"user_tz":480,"elapsed":4441,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"c2f40013-2724-4147-a275-d8110e0dd482"},"source":["df = pd.read_csv(\"ner.csv\", encoding=\"ISO-8859-1\", error_bad_lines=False)\r\n","df.head()"],"execution_count":41,"outputs":[{"output_type":"stream","text":["b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>lemma</th>\n","      <th>next-lemma</th>\n","      <th>next-next-lemma</th>\n","      <th>next-next-pos</th>\n","      <th>next-next-shape</th>\n","      <th>next-next-word</th>\n","      <th>next-pos</th>\n","      <th>next-shape</th>\n","      <th>next-word</th>\n","      <th>pos</th>\n","      <th>prev-iob</th>\n","      <th>prev-lemma</th>\n","      <th>prev-pos</th>\n","      <th>prev-prev-iob</th>\n","      <th>prev-prev-lemma</th>\n","      <th>prev-prev-pos</th>\n","      <th>prev-prev-shape</th>\n","      <th>prev-prev-word</th>\n","      <th>prev-shape</th>\n","      <th>prev-word</th>\n","      <th>sentence_idx</th>\n","      <th>shape</th>\n","      <th>word</th>\n","      <th>tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>thousand</td>\n","      <td>of</td>\n","      <td>demonstr</td>\n","      <td>NNS</td>\n","      <td>lowercase</td>\n","      <td>demonstrators</td>\n","      <td>IN</td>\n","      <td>lowercase</td>\n","      <td>of</td>\n","      <td>NNS</td>\n","      <td>__START1__</td>\n","      <td>__start1__</td>\n","      <td>__START1__</td>\n","      <td>__START2__</td>\n","      <td>__start2__</td>\n","      <td>__START2__</td>\n","      <td>wildcard</td>\n","      <td>__START2__</td>\n","      <td>wildcard</td>\n","      <td>__START1__</td>\n","      <td>1.0</td>\n","      <td>capitalized</td>\n","      <td>Thousands</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>of</td>\n","      <td>demonstr</td>\n","      <td>have</td>\n","      <td>VBP</td>\n","      <td>lowercase</td>\n","      <td>have</td>\n","      <td>NNS</td>\n","      <td>lowercase</td>\n","      <td>demonstrators</td>\n","      <td>IN</td>\n","      <td>O</td>\n","      <td>thousand</td>\n","      <td>NNS</td>\n","      <td>__START1__</td>\n","      <td>__start1__</td>\n","      <td>__START1__</td>\n","      <td>wildcard</td>\n","      <td>__START1__</td>\n","      <td>capitalized</td>\n","      <td>Thousands</td>\n","      <td>1.0</td>\n","      <td>lowercase</td>\n","      <td>of</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>demonstr</td>\n","      <td>have</td>\n","      <td>march</td>\n","      <td>VBN</td>\n","      <td>lowercase</td>\n","      <td>marched</td>\n","      <td>VBP</td>\n","      <td>lowercase</td>\n","      <td>have</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","      <td>of</td>\n","      <td>IN</td>\n","      <td>O</td>\n","      <td>thousand</td>\n","      <td>NNS</td>\n","      <td>capitalized</td>\n","      <td>Thousands</td>\n","      <td>lowercase</td>\n","      <td>of</td>\n","      <td>1.0</td>\n","      <td>lowercase</td>\n","      <td>demonstrators</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>have</td>\n","      <td>march</td>\n","      <td>through</td>\n","      <td>IN</td>\n","      <td>lowercase</td>\n","      <td>through</td>\n","      <td>VBN</td>\n","      <td>lowercase</td>\n","      <td>marched</td>\n","      <td>VBP</td>\n","      <td>O</td>\n","      <td>demonstr</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","      <td>of</td>\n","      <td>IN</td>\n","      <td>lowercase</td>\n","      <td>of</td>\n","      <td>lowercase</td>\n","      <td>demonstrators</td>\n","      <td>1.0</td>\n","      <td>lowercase</td>\n","      <td>have</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>march</td>\n","      <td>through</td>\n","      <td>london</td>\n","      <td>NNP</td>\n","      <td>capitalized</td>\n","      <td>London</td>\n","      <td>IN</td>\n","      <td>lowercase</td>\n","      <td>through</td>\n","      <td>VBN</td>\n","      <td>O</td>\n","      <td>have</td>\n","      <td>VBP</td>\n","      <td>O</td>\n","      <td>demonstr</td>\n","      <td>NNS</td>\n","      <td>lowercase</td>\n","      <td>demonstrators</td>\n","      <td>lowercase</td>\n","      <td>have</td>\n","      <td>1.0</td>\n","      <td>lowercase</td>\n","      <td>marched</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0     lemma next-lemma  ...        shape           word tag\n","0           0  thousand         of  ...  capitalized      Thousands   O\n","1           1        of   demonstr  ...    lowercase             of   O\n","2           2  demonstr       have  ...    lowercase  demonstrators   O\n","3           3      have      march  ...    lowercase           have   O\n","4           4     march    through  ...    lowercase        marched   O\n","\n","[5 rows x 25 columns]"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":494},"id":"wG4MpL6Nt0cx","executionInfo":{"status":"ok","timestamp":1609256571171,"user_tz":480,"elapsed":805,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"7689435a-5f35-46b4-896a-b439c4ab0721"},"source":["data = df[[\"sentence_idx\", \"word\", \"tag\"]]\r\n","data.head(15)"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence_idx</th>\n","      <th>word</th>\n","      <th>tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>Thousands</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>of</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>demonstrators</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>have</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>marched</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1.0</td>\n","      <td>through</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1.0</td>\n","      <td>London</td>\n","      <td>B-geo</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1.0</td>\n","      <td>to</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1.0</td>\n","      <td>protest</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1.0</td>\n","      <td>the</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1.0</td>\n","      <td>war</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1.0</td>\n","      <td>in</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1.0</td>\n","      <td>Iraq</td>\n","      <td>B-geo</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1.0</td>\n","      <td>and</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>1.0</td>\n","      <td>demand</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    sentence_idx           word    tag\n","0            1.0      Thousands      O\n","1            1.0             of      O\n","2            1.0  demonstrators      O\n","3            1.0           have      O\n","4            1.0        marched      O\n","5            1.0        through      O\n","6            1.0         London  B-geo\n","7            1.0             to      O\n","8            1.0        protest      O\n","9            1.0            the      O\n","10           1.0            war      O\n","11           1.0             in      O\n","12           1.0           Iraq  B-geo\n","13           1.0            and      O\n","14           1.0         demand      O"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"InVYe7a8ujFB","executionInfo":{"status":"ok","timestamp":1609256574245,"user_tz":480,"elapsed":837,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"3942b667-1279-4c1a-87c9-62e120ddbaf3"},"source":["data[\"tag\"].value_counts()"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["O        889973\n","B-geo     37525\n","B-tim     20193\n","B-org     20184\n","I-per     17382\n","B-per     17011\n","I-org     16537\n","B-gpe     16392\n","I-geo      7409\n","I-tim      6298\n","B-art       434\n","B-eve       348\n","I-eve       297\n","I-art       280\n","I-gpe       229\n","B-nat       226\n","I-nat        76\n","Name: tag, dtype: int64"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"TuQNAILwBXpb"},"source":["### Build vocab"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o6FTBOvszaBu","executionInfo":{"status":"ok","timestamp":1609256582451,"user_tz":480,"elapsed":3838,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"317bd671-0971-4d1f-bf3c-db13d0430406"},"source":["tagged_sentences = build_tagged_senteces(data)\r\n","print(\"Sample tagged sentence\")\r\n","print(repr(tagged_sentences[0][:4]), \"...\")\r\n","\r\n","tokens, tags = build_vocab(data)\r\n","num_tokens, num_tags = len(tokens), len(tags)\r\n","print(\"\\nStats\")\r\n","print(f\"Num tokens: {num_tokens:,}\")\r\n","print(f\"Num tags: {num_tags}\")\r\n","\r\n","maxlen = max([len(t) for t in tokens])\r\n","print(f\"maxlen: {maxlen}\")\r\n"],"execution_count":44,"outputs":[{"output_type":"stream","text":["Sample tagged sentence\n","[('Thousands', 'O'), ('of', 'O'), ('demonstrators', 'O'), ('have', 'O')] ...\n","\n","Stats\n","Num tokens: 30,173\n","Num tags: 17\n","maxlen: 64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZyTO50ql71OJ"},"source":["### Tokenize sentence and label sequences"]},{"cell_type":"code","metadata":{"id":"CAP5-ttp3U9F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609256586329,"user_tz":480,"elapsed":1725,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"eb789d1b-14d5-4210-823a-8f25cd256b33"},"source":["token2idx, idx2token, tag2idx, idx2tag = build_indexes(tokens, tags)\r\n","X, y = tokenize(tagged_sentences, token2idx, tag2idx)\r\n","\r\n","print(f\"Sentences dimension: {X.shape}\")\r\n","print(f\"Labels dimension: {y.shape}\")"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Sentences dimension: (35177, 64)\n","Labels dimension: (35177, 64, 17)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j0wSHkWELH2r"},"source":["### Split the dataset into train and test"]},{"cell_type":"code","metadata":{"id":"wS_NRQOX3UzR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609256590483,"user_tz":480,"elapsed":1294,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"62aee664-8d41-4740-accf-dce4301b3aa3"},"source":["VALIDATION_SIZE = int(len(X) * 0.1)\r\n","BUFFER_SIZE = 50000\r\n","\r\n","dataset = tf.data.Dataset.from_tensor_slices((X, y))\r\n","train_dataset = dataset.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE).batch(64, drop_remainder=True)\r\n","train_dataset = configure_dataset(train_dataset)\r\n","\r\n","test_dataset = dataset.take(VALIDATION_SIZE)\r\n","test_dataset = configure_dataset(test_dataset).batch(64, drop_remainder=True)\r\n","\r\n","train_dataset.cardinality(), test_dataset.cardinality()"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(), dtype=int64, numpy=494>,\n"," <tf.Tensor: shape=(), dtype=int64, numpy=54>)"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5V1GW2jvBdfD","executionInfo":{"status":"ok","timestamp":1609256697852,"user_tz":480,"elapsed":51963,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"8444d35e-a0c9-487a-f658-f87f717c0eb2"},"source":["# @title Baseline: Bilateral LSTM model\r\n","EPOCHS = 3  # @param {type:\"number\"}\r\n","LEARNING_RATE = 1e-3  # @param {type:\"number\"}\r\n","EMBEDDING_DROPOUT_RATE = 0.5  # @param {type:\"number\"}\r\n","RECURRENT_DROPOUT_RATE = 0.5  # @param {type:\"number\"}\r\n","\r\n","# embeddings input_dim add one extra input accounting for the padding\r\n","model = Sequential([\r\n","    Embedding(input_dim=num_tokens + 1, output_dim=64, mask_zero=True),\r\n","    SpatialDropout1D(EMBEDDING_DROPOUT_RATE),\r\n","    Bidirectional(\r\n","        LSTM(units=100, return_sequences=True, dropout=RECURRENT_DROPOUT_RATE)\r\n","    ),\r\n","    TimeDistributed(Dense(num_tags, activation=\"softmax\")),\r\n","])\r\n","# model.summary()\r\n","adam_optimizer = Adam(learning_rate=LEARNING_RATE)\r\n","model.compile(\r\n","    optimizer=adam_optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\r\n",")\r\n","history = model.fit(train_dataset, epochs=EPOCHS, verbose=1)"],"execution_count":48,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","494/494 [==============================] - 20s 30ms/step - loss: 0.3892 - accuracy: 0.8388\n","Epoch 2/3\n","494/494 [==============================] - 15s 30ms/step - loss: 0.0973 - accuracy: 0.9375\n","Epoch 3/3\n","494/494 [==============================] - 15s 30ms/step - loss: 0.0608 - accuracy: 0.9599\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6tIbuOmiQ9BS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609256704685,"user_tz":480,"elapsed":2384,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"56395d0b-a1c0-46f7-f10a-32a356e0d176"},"source":["model.evaluate(test_dataset)"],"execution_count":49,"outputs":[{"output_type":"stream","text":["54/54 [==============================] - 2s 7ms/step - loss: 0.1127 - accuracy: 0.9558\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.11273986101150513, 0.9557585120201111]"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"jFRd9filUScz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609256719911,"user_tz":480,"elapsed":2073,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"dfa3e6b6-bdf3-4d2e-ed23-b03cb00a7f18"},"source":["X_test, y_test = next(test_dataset.take(1).as_numpy_iterator())\r\n","sample_idx = np.random.randint(0, len(X_test))\r\n","\r\n","X_test = X_test[sample_idx]\r\n","y_pred = model.predict(X_test)\r\n","pred_tags = decode_tags_batch(y_pred)\r\n","pred_tags = np.squeeze(pred_tags)\r\n","\r\n","y_test = y_test[sample_idx]\r\n","true_tags = decode_tags_batch([y_test])\r\n","true_tags = np.squeeze(true_tags)\r\n","\r\n","print(f\"{'Word':15}{'True':5}\\t {'Pred'}\\n\")\r\n","print(\"_\"*30)\r\n","for token_idx, true_tag, pred_tag in zip(X_test, true_tags, pred_tags):\r\n","    print(f\"{idx2token[token_idx]:15}{true_tag}\\t{pred_tag}\")"],"execution_count":50,"outputs":[{"output_type":"stream","text":["Word           True \t Pred\n","\n","______________________________\n","Pacific        I-org\tI-geo\n","Economic       I-org\tI-org\n","Cooperation    I-org\tB-gpe\n","Business       I-org\tI-org\n","Advisory       I-org\tB-gpe\n","Council        I-org\tI-org\n","are            O\tO\n","holding        O\tO\n","meetings       O\tO\n","this           O\tO\n","week           O\tO\n","to             O\tO\n","finalize       O\tO\n","their          O\tO\n","annual         O\tO\n","report         O\tO\n","for            O\tO\n","APEC           B-org\tB-gpe\n","leaders        O\tO\n","who            O\tO\n","will           O\tO\n","hold           O\tO\n","a              O\tO\n","summit         O\tO\n","on             O\tO\n","September      B-tim\tI-tim\n","8              I-tim\tI-tim\n","and            O\tO\n","9              B-tim\tI-tim\n",".              O\tO\n","RepresentativesO\tI-org\n","from           O\tO\n","the            O\tO\n","Asia           B-org\tI-geo\n","Pacific        I-org\tI-geo\n","Economic       I-org\tI-org\n","Cooperation    I-org\tB-gpe\n","Business       I-org\tI-org\n","Advisory       I-org\tB-gpe\n","Council        I-org\tI-org\n","are            O\tO\n","holding        O\tO\n","meetings       O\tO\n","this           O\tO\n","week           O\tO\n","to             O\tO\n","finalize       O\tO\n","their          O\tO\n","annual         O\tO\n","report         O\tO\n","for            O\tO\n","APEC           B-org\tB-gpe\n","leaders        O\tO\n","who            O\tO\n","will           O\tO\n","hold           O\tO\n","a              O\tO\n","summit         O\tO\n","on             O\tO\n","September      B-tim\tI-tim\n","8              I-tim\tI-tim\n","and            O\tO\n","9              B-tim\tI-tim\n",".              O\tO\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d6VLMrKvWeqS"},"source":["## Baseline Model Evaluation\r\n","\r\n","Achieves f1-score=0.72"]},{"cell_type":"code","metadata":{"id":"t-PJqEmVp4St","colab":{"base_uri":"https://localhost:8080/","height":341},"executionInfo":{"status":"ok","timestamp":1609256732873,"user_tz":480,"elapsed":8355,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"f5c43ffd-d5a6-4bde-fcc0-d5d209869dde"},"source":["true_labels, pred_labels = test_inference(model, test_dataset)\r\n","print(classification_report(true_labels, pred_labels))"],"execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='54' class='' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [54/54 00:04<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         art       0.00      0.00      0.00       121\n","         eve       0.00      0.00      0.00        93\n","         geo       0.71      0.84      0.77      4770\n","         gpe       0.92      0.75      0.82      2716\n","         nat       0.00      0.00      0.00        46\n","         org       0.58      0.53      0.55      2704\n","         per       0.68      0.65      0.66      2403\n","         tim       0.82      0.83      0.82      2658\n","\n","   micro avg       0.73      0.73      0.73     15511\n","   macro avg       0.46      0.45      0.45     15511\n","weighted avg       0.72      0.73      0.72     15511\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UnTjlUnGuLa1"},"source":["# Bidirectional LSTM CRF"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sde1HgV8Z3Kq","executionInfo":{"status":"ok","timestamp":1609218894560,"user_tz":480,"elapsed":1405,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"719aeda8-c9e4-4c9c-ffae-a746d2fe9815"},"source":["#@title Train/Test dataset split\r\n","batch_size = 64 #@param {type:\"number\"}\r\n","\r\n","X, y = tokenize(tagged_sentences, token2idx, tag2idx, one_hot_encode_tags=False)\r\n","\r\n","print(f\"Sentences dimension: {X.shape}\")\r\n","print(f\"Labels dimension: {y.shape}\")\r\n","\r\n","dataset = tf.data.Dataset.from_tensor_slices((X, y))\r\n","train_dataset = (\r\n","    dataset.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)\r\n",")\r\n","train_dataset = configure_dataset(train_dataset)\r\n","\r\n","test_dataset = dataset.take(VALIDATION_SIZE)\r\n","test_dataset = configure_dataset(test_dataset).batch(batch_size, drop_remainder=True)\r\n","\r\n","print(\"-\" * 10)\r\n","train_batches_size = train_dataset.cardinality().numpy()\r\n","print(\"Train batches size:\", train_batches_size)\r\n","\r\n","test_batches_size = test_dataset.cardinality().numpy()\r\n","print(\"Test batches size:\", test_batches_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sentences dimension: (35177, 64)\n","Labels dimension: (35177, 64)\n","----------\n","Train batches size: 494\n","Test batches size: 54\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XkuRy1mtOHdI"},"source":["SPATIAL_DROPOUT_RATE = 0.5\r\n","RECURRENT_DROPOUT_RATE = 0.5\r\n","\r\n","\r\n","class BiLstmCRF(Model):\r\n","    def __init__(\r\n","        self,\r\n","        vocab_dim,\r\n","        tag_dim,\r\n","        max_seq_len,\r\n","        embedding_dim=128,\r\n","        lstm_men_dim=200,\r\n","        name=\"BiLstmCRF\",\r\n","        **kwargs\r\n","    ):\r\n","        super(NerBiLstmCRF, self).__init__(name=name, **kwargs)\r\n","        self.embedding = Embedding(vocab_dim, embedding_dim)\r\n","        self.dropout = SpatialDropout1D(SPATIAL_DROPOUT_RATE)\r\n","        self.lstm = LSTM(\r\n","            lstm_men_dim,\r\n","            return_sequences=True,\r\n","            dropout=RECURRENT_DROPOUT_RATE\r\n","        )\r\n","        self.bi_lstm = Bidirectional(self.lstm)\r\n","        self.classifier = Dense(tag_dim, activation=\"softmax\")\r\n","        self.time_distributed_classifier = TimeDistributed(self.classifier)\r\n","        self.sequence_lengths = tf.expand_dims(max_seq_len, axis=0)\r\n","        # let the crf layer to initialize the transition_params for us\r\n","        self.transition_params = None \r\n","\r\n","    def call(self, inputs, labels=None, training=False):\r\n","        token_embeddings = self.embedding(inputs)\r\n","        token_embeddings = self.dropout(token_embeddings, training)\r\n","        logits = self.bi_lstm(token_embeddings)\r\n","        logits = self.time_distributed_classifier(logits)\r\n","\r\n","        if labels is not None:\r\n","            label_sequences = labels\r\n","            log_likelihood, self.transition_params = tf_ad.text.crf_log_likelihood(\r\n","                logits, labels, self.sequence_lengths, transition_params=self.transition_params\r\n","            )\r\n","            return logits, log_likelihood\r\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XWe6YbFjVd8-"},"source":["class CRF(Layer):\r\n","    \"\"\"\r\n","    Conditional Random Field loss layer\r\n","    \"\"\"\r\n","\r\n","    def __init__(self, output_dim, sparse_target=True, name=\"crf_loss\", **kwargs):\r\n","        \"\"\"\r\n","        Args:\r\n","            output_dim (int): number of labels to tag\r\n","            sparse_target (bool): is ground-truth label one-hot encoded\r\n","            Input shape:\r\n","                (batch_size, sentence_lenght, output_dim)\r\n","            Output shape:\r\n","                (batch_size, sentence_lenght, output_dim)\r\n","        \"\"\"\r\n","        super(CRF, self).__init__(name=name, **kwargs)\r\n","        self.output_dim = output_dim\r\n","        self.sparse_target = sparse_target\r\n","        self.input_spec = InputSpec(min_ndim=3)\r\n","        self.supports_masking = False\r\n","        self.sequence_lenghts = None\r\n","        self.transitions = None\r\n","\r\n","    def build(self, input_shape):\r\n","        assert len(input_shape) == 3\r\n","        f_shape = tf.TensorShape(input_shape)\r\n","        input_spec = InputSpec(min_ndim=3, axes={-1: f_shape[-1]})\r\n","\r\n","        if f_shape[-1] is None:\r\n","            raise ValueError(\"CRF missing dimession mismatch.\")\r\n","        if f_shape[-1] != self.output_dim:\r\n","            raise ValueError(\"Last dimession should be equal to output_dim.\")\r\n","        self.input_spec = input_spec\r\n","        self.transitions = self.add_weight(\r\n","            name=\"transitions\",\r\n","            shape=[self.output_dim, self.output_dim],\r\n","            initializer=\"glorot_uniform\",\r\n","            trainable=True,\r\n","        )\r\n","        self.build = True\r\n","\r\n","    def compute_mask(self, inputs, mask=None):\r\n","        return mask\r\n","\r\n","    def call(self, inputs, training=False, **kwargs):\r\n","        print(\"inputs!!!\", inputs)\r\n","        sequences = tf.convert_to_tensor(inputs, dtype=self.dtype)\r\n","        if sequence_lengths is not None:\r\n","            assert len(sequence_lengths.shape) == 2\r\n","            assert tf.convert_to_tensor(sequence_lengths).dtype == \"int32\"\r\n","            seq_len_shape = tf.convert_to_tensor(sequence_lengths).get_shape().as_list()\r\n","            assert seq_len_shape[1] == 1\r\n","            self.sequence_lengths = K.flatten(sequence_lengths)\r\n","        else:\r\n","            self.sequence_lengths = tf.ones(tf.shape(inputs)[0], dtype=tf.int32) * (\r\n","                tf.shape(inputs)[1]\r\n","            )\r\n","        \r\n","        viterbi_sequence, _ = crf_decode(sequences, self.transitions, self.sequence_lenghts)\r\n","        output = K.one_hot(viterbi_sequence, self.output_dim)\r\n","        return K.in_train_phase(sequences, output)\r\n","\r\n","    @property\r\n","    def loss(self):\r\n","        def crf_loss(y_true, y_pred):\r\n","            y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\r\n","            log_likelihood, self.transitions = crf_log_likelihood(y_pred, tf.cast(K.argmax(y_true), dtype=tf.int32) if self.sparse_target else y_true, self.sequence_lenghts, transition_params=self.transitions)\r\n","            return tf.reduce_mean(-log_likelihood)\r\n","        return crf_loss\r\n","\r\n","    @property\r\n","    def accuracy(self):\r\n","        def viterbi_accuracy(y_true, y_pred):\r\n","            mask = K.cast(K.all(K.greater(y_pred, -1e10), axis=2), K.floatx())\r\n","            shape = tf.shape(y_pred)\r\n","            sequence_lenghts = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\r\n","            y_pred, _ = crf_decode(y_pred, self.transitions, )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"B6-5YNEqmGvU","executionInfo":{"status":"error","timestamp":1609223308204,"user_tz":480,"elapsed":4401019,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"5ce524fa-463d-4c9f-bc92-1a602349319d"},"source":["#@title Train Bilateral LSTM model with CRF\r\n","EPOCHS  =  60#@param {type:\"number\"}\r\n","LEARNING_RATE = 1e-3 #@param {type:\"number\"}\r\n","\r\n","bi_lstm_crf_model = BiLstmCRF(num_tokens, num_tags, max_seq_len=maxlen)\r\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n","\r\n","train_loss_metric = tf.keras.metrics.Mean(\"training_loss\", dtype=tf.float32)\r\n","\r\n","best_acc = 0\r\n","step = 0\r\n","\r\n","epoch_bar = master_bar(range(epochs))\r\n","for epoch in epoch_bar:\r\n","    for tokens_batch, labels_batch in progress_bar(train_dataset, total=train_batches_size, parent=epoch_bar):\r\n","        loss, logits = train_step_fn(\r\n","            bi_lstm_crf_model, optimizer, tokens_batch, labels_batch\r\n","        )\r\n","        train_loss_metric(loss)\r\n","        epoch_bar.child.comment = f\"training loss : {train_loss_metric.result():.3f}\"\r\n","        if step % 20 == 0:\r\n","            accuracy = 0\r\n","    tf.summary.scalar('training loss', train_loss_metric.result(), step=epoch)\r\n","    epoch_bar.write(f\"Epoch {epoch} - train loss: {train_loss_metric.result():.3f} valid loss 0 accuracy: 0%\")\r\n","    train_loss_metric.reset_states()\r\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='45' class='' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      75.00% [45/60 1:12:41<24:13]\n","    </div>\n","    \n","Epoch 0 - train loss: 151.571 valid loss 0 accuracy: 0%<p>Epoch 1 - train loss: 141.443 valid loss 0 accuracy: 0%<p>Epoch 2 - train loss: 135.957 valid loss 0 accuracy: 0%<p>Epoch 3 - train loss: 130.511 valid loss 0 accuracy: 0%<p>Epoch 4 - train loss: 125.104 valid loss 0 accuracy: 0%<p>Epoch 5 - train loss: 119.739 valid loss 0 accuracy: 0%<p>Epoch 6 - train loss: 114.414 valid loss 0 accuracy: 0%<p>Epoch 7 - train loss: 109.130 valid loss 0 accuracy: 0%<p>Epoch 8 - train loss: 103.885 valid loss 0 accuracy: 0%<p>Epoch 9 - train loss: 98.681 valid loss 0 accuracy: 0%<p>Epoch 10 - train loss: 93.521 valid loss 0 accuracy: 0%<p>Epoch 11 - train loss: 88.410 valid loss 0 accuracy: 0%<p>Epoch 12 - train loss: 83.357 valid loss 0 accuracy: 0%<p>Epoch 13 - train loss: 78.375 valid loss 0 accuracy: 0%<p>Epoch 14 - train loss: 73.477 valid loss 0 accuracy: 0%<p>Epoch 15 - train loss: 68.679 valid loss 0 accuracy: 0%<p>Epoch 16 - train loss: 64.001 valid loss 0 accuracy: 0%<p>Epoch 17 - train loss: 59.468 valid loss 0 accuracy: 0%<p>Epoch 18 - train loss: 55.108 valid loss 0 accuracy: 0%<p>Epoch 19 - train loss: 50.954 valid loss 0 accuracy: 0%<p>Epoch 20 - train loss: 47.042 valid loss 0 accuracy: 0%<p>Epoch 21 - train loss: 43.411 valid loss 0 accuracy: 0%<p>Epoch 22 - train loss: 40.096 valid loss 0 accuracy: 0%<p>Epoch 23 - train loss: 37.126 valid loss 0 accuracy: 0%<p>Epoch 24 - train loss: 34.519 valid loss 0 accuracy: 0%<p>Epoch 25 - train loss: 32.276 valid loss 0 accuracy: 0%<p>Epoch 26 - train loss: 30.381 valid loss 0 accuracy: 0%<p>Epoch 27 - train loss: 28.804 valid loss 0 accuracy: 0%<p>Epoch 28 - train loss: 27.506 valid loss 0 accuracy: 0%<p>Epoch 29 - train loss: 26.441 valid loss 0 accuracy: 0%<p>Epoch 30 - train loss: 25.568 valid loss 0 accuracy: 0%<p>Epoch 31 - train loss: 24.847 valid loss 0 accuracy: 0%<p>Epoch 32 - train loss: 24.245 valid loss 0 accuracy: 0%<p>Epoch 33 - train loss: 23.738 valid loss 0 accuracy: 0%<p>Epoch 34 - train loss: 23.305 valid loss 0 accuracy: 0%<p>Epoch 35 - train loss: 22.848 valid loss 0 accuracy: 0%<p>Epoch 36 - train loss: 20.283 valid loss 0 accuracy: 0%<p>Epoch 37 - train loss: 18.219 valid loss 0 accuracy: 0%<p>Epoch 38 - train loss: 16.143 valid loss 0 accuracy: 0%<p>Epoch 39 - train loss: 11.799 valid loss 0 accuracy: 0%<p>Epoch 40 - train loss: 7.736 valid loss 0 accuracy: 0%<p>Epoch 41 - train loss: 4.556 valid loss 0 accuracy: 0%<p>Epoch 42 - train loss: 1.949 valid loss 0 accuracy: 0%<p>Epoch 43 - train loss: 1.348 valid loss 0 accuracy: 0%<p>Epoch 44 - train loss: -0.540 valid loss 0 accuracy: 0%<p>\n","\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='193' class='' max='494' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      39.07% [193/494 00:37<00:58 training loss : -6.116]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-d4523cef05a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtokens_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_batches_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         loss, logits = train_step_fn(\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mbi_lstm_crf_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         )\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_loss_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-2ecf70574c49>\u001b[0m in \u001b[0;36mtrain_step_fn\u001b[0;34m(model, optimizer, tokens_batch, labels_batch)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         logits, log_likelihood = model(\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mtokens_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         )\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_likelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-bdeb19697ccd>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, labels, training)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mlabel_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             log_likelihood, self.transition_params = tf_ad.text.crf_log_likelihood(\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransition_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             )\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_addons/text/crf.py\u001b[0m in \u001b[0;36mcrf_log_likelihood\u001b[0;34m(inputs, tag_indices, sequence_lengths, transition_params)\u001b[0m\n\u001b[1;32m    194\u001b[0m     sequence_scores = crf_sequence_score(inputs, tag_indices, sequence_lengths,\n\u001b[1;32m    195\u001b[0m                                          transition_params)\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0mlog_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrf_log_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransition_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# Normalize the scores to get the log-likelihood per example.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_addons/text/crf.py\u001b[0m in \u001b[0;36mcrf_log_norm\u001b[0;34m(inputs, sequence_lengths, transition_params)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     return tf.cond(\n\u001b[0;32m--> 160\u001b[0;31m         tf.equal(tf.shape(inputs)[1], 1), _single_seq_fn, _multi_seq_fn)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mcond_for_tf_v2\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m   \"\"\"\n\u001b[0;32m-> 1396\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfalse_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m                 instructions)\n\u001b[0;32m--> 538\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mcond\u001b[0;34m(pred, true_fn, false_fn, strict, name, fn1, fn2)\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1210\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1211\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_UnpackIfSingleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_addons/text/crf.py\u001b[0m in \u001b[0;36m_multi_seq_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         alphas = crf_forward(rest_of_input, first_input, transition_params,\n\u001b[0;32m--> 151\u001b[0;31m                              sequence_lengths)\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mlog_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_logsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# Mask `log_norm` of the sequences with length <= zero.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_addons/text/crf.py\u001b[0m in \u001b[0;36mcrf_forward\u001b[0;34m(inputs, state, transition_params, sequence_lengths)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_alphas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0mall_alphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_scan_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0;31m# add first state for sequences of length 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0mall_alphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_alphas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 605\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36mscan_v2\u001b[0;34m(fn, elems, initializer, parallel_iterations, back_prop, swap_memory, infer_shape, reverse, name)\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0minfer_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfer_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m       \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36mscan\u001b[0;34m(fn, elems, initializer, parallel_iterations, back_prop, swap_memory, infer_shape, reverse, name)\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0mback_prop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m         maximum_iterations=n)\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresults_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2733\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2734\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2735\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2737\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   2724\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   2725\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 2726\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2727\u001b[0m       \u001b[0mtry_to_pack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(i, a_flat, tas)\u001b[0m\n\u001b[1;32m    645\u001b[0m       \u001b[0mpacked_elems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_ta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem_ta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melems_ta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mpacked_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m       \u001b[0ma_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacked_elems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m       nest.assert_same_structure(elems if initializer is None else initializer,\n\u001b[1;32m    649\u001b[0m                                  a_out)\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_addons/text/crf.py\u001b[0m in \u001b[0;36m_scan_fn\u001b[0;34m(_state, _inputs)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mtransition_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_state\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtransition_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mnew_alphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_inputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_logsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransition_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_alphas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m--> 469\u001b[0;31m         _ctx, \"AddV2\", name, x, y)\n\u001b[0m\u001b[1;32m    470\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"so6EGuW94Ur-"},"source":["tf.summary.flush()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"otSrTw4A6SLH"},"source":["m = NerBiLstmCRF(num_tokens, num_tags)\r\n","m.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\r\n","m.fit(train_dataset, epochs=3)\r\n","#m.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"10hvgCt3IJec"},"source":["m.evaluate(test_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LZjhHDDiJsEN"},"source":["true_labels, pred_labels = test_inference(m, test_dataset)\r\n","print(classification_report(true_labels, pred_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VOxdm2jDIQAE"},"source":["true_labels = []\r\n","pred_labels = []\r\n","\r\n","num_test_batches = test_dataset.cardinality().numpy()\r\n","for X_batch, y_true in progress_bar(test_dataset, total=num_test_batches):\r\n","    y_pred = model.predict(X_batch)\r\n","    pred_labels.append(decode_tags_batch(y_pred))\r\n","    true_labels.append(decode_tags_batch(y_true.numpy()))\r\n","\r\n","true_labels = np.array(true_labels)\r\n","num_batches, num_samples, sentence_lenght = true_labels.shape\r\n","true_labels = true_labels.reshape(num_batches * num_samples, sentence_lenght)\r\n","true_labels = true_labels.tolist()\r\n","\r\n","pred_labels = np.array(pred_labels)\r\n","pred_labels = pred_labels.reshape(num_batches * num_samples, sentence_lenght)\r\n","pred_labels = pred_labels.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xjr7--R1U0YO"},"source":["model_222 = Sequential([\r\n","    Embedding(input_dim=num_tokens, output_dim=64),\r\n","    SpatialDropout1D(0.5),\r\n","    Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.5)),\r\n","    LSTM(units=100, return_sequences=True, recurrent_dropout=0.5),\r\n","    TimeDistributed(Dense(num_tags))\r\n","])\r\n","\r\n","def loss(labels, logits):\r\n","    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\r\n","\r\n","model_222.compile(optimizer=\"adam\", loss=loss, metrics=[\"accuracy\"])\r\n","X = [[token2idx[t] for t, _ in s] for s in sentences]\r\n","X = pad_sequences(X, maxlen=maxlen, padding=\"post\", value=unk_token_idx)\r\n","\r\n","y = [[tag2idx[t] for _, t in s] for s in sentences]\r\n","y = pad_sequences(y, maxlen=maxlen, padding=\"post\", value=unk_tag_idx)\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\r\n","history = model_222.fit(X_train, np.array(y_train), validation_split=0.2, batch_size=32, epochs=3, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MJqUq-0oV3mH"},"source":["model_222.evaluate(X_test, np.array(y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Vuv5NN6ct4P"},"source":["sample_idx = np.random.randint(0, len(X_test))\r\n","pred = model_222(tf.expand_dims(X_test[sample_idx], 0))\r\n","pred = tf.squeeze(pred, 0)\r\n","pred = tf.random.categorical(pred, num_samples=1)\r\n","pred_tags = pred.numpy().flatten()\r\n","ground_truth = y_test[sample_idx]\r\n","\r\n","print(f\"{'Word':15}{'True':5}\\t {'Pred'}\\n\")\r\n","print(\"_\"*30)\r\n","for token, gt_tag, pred_tag in zip(X_test[sample_idx], ground_truth, pred_tags):\r\n","    print(f\"{idx2token[token]:15}{idx2tag[gt_tag]}\\t{idx2tag[pred_tag]}\")"],"execution_count":null,"outputs":[]}]}