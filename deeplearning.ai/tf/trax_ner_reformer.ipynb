{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ner_reformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1-dqTHDvTDZyae6FrK8bfxGj2yIcTnJsZ",
      "authorship_tag": "ABX9TyPnB7PK1Gi88WFf/epoGmpD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/tf/trax_ner_reformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1uTzGn4kInf"
      },
      "source": [
        "# NER Reformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7iJU0e6lBN_"
      },
      "source": [
        "## Named Entity Recognition\r\n",
        "\r\n",
        "Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG8jFYpkkFmV",
        "outputId": "ef76fc79-abe5-4086-c063-6eb17a710371"
      },
      "source": [
        "#@title ## Install Dependencies\r\n",
        "#@markdown - trax\r\n",
        "#@markdown - kaggle client: downloads dataset\r\n",
        "\r\n",
        "%%capture --no-stdout --no-stderr\r\n",
        "!pip install -Uqq trax \r\n",
        "!pip install -Uqq kaggle\r\n",
        "\r\n",
        "# %%python\r\n",
        "print(\"Dependencies successfully installed.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 522kB 4.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 8.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 33.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 29.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.7MB 46.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 6.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 49.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 50.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 43.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 47.9MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Dependencies successfully installed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8qpXStMm3Id",
        "cellView": "form",
        "outputId": "363f94fc-f478-413f-8073-183c0d4a77bd"
      },
      "source": [
        "#@title ## Download Kaggle Dataset\r\n",
        "#@markdown Dataset: Annotated Corpus for Named Entity Recognition <br>\r\n",
        "#@markdown [https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\r\n",
        "#@markdown <br><br>\r\n",
        "#@markdown This is the extract from GMB corpus which is tagged, annotated and built specifically to train the classifier to predict named entities such as name, location, etc.\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "!mkdir -p ~/.kaggle\r\n",
        "!cp /content/drive/MyDrive/kaggle/kaggle.json ~/.kaggle/kaggle.json\r\n",
        "!chmod 600 ~/.kaggle/kaggle.json\r\n",
        "!kaggle datasets download -d abhinavwalia95/entity-annotated-corpus\r\n",
        "!unzip -o /content/entity-annotated-corpus\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Downloading entity-annotated-corpus.zip to /content\n",
            " 34% 9.00M/26.4M [00:01<00:02, 8.78MB/s]\n",
            "100% 26.4M/26.4M [00:01<00:00, 20.3MB/s]\n",
            "Archive:  /content/entity-annotated-corpus.zip\n",
            "  inflating: ner.csv                 \n",
            "  inflating: ner_dataset.csv         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vootG507rX4r",
        "outputId": "97fc2402-1f2c-4bd2-c0c5-a046723e580f"
      },
      "source": [
        "#@title ## Import packages\r\n",
        "#@markdown DL framework: trax<br>\r\n",
        "#@markdown Data Manipulation: pandas<br>\r\n",
        "import random as rnd\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "\r\n",
        "#print('trax:', trax.__version__)\r\n",
        "print('numpy:', np.__version__)\r\n",
        "print('pandas:', pd.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numpy: 1.19.4\n",
            "pandas: 1.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ1XIryYu6w-"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "193kd0kpvFTb"
      },
      "source": [
        "Padding tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPRPJkPTvOHI"
      },
      "source": [
        "PAD_TOKEN = \"PAD\"\r\n",
        "PAD_INDEX = 0\r\n",
        "PAD_TAG = \"O\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjNWJQ275yuW"
      },
      "source": [
        "Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "KPQdUULntOM-",
        "outputId": "231837aa-e76e-43d0-9599-acad7906dc48"
      },
      "source": [
        "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"ISO-8859-1\", error_bad_lines=False)\r\n",
        "#data = data.rename(columns={\"Sentence #\": \"sentence_id\", \"Word\": \"word\", \"Tag\": \"tag\"})\r\n",
        "#data = data[[\"sentence_id\", \"word\", \"tag\"]]\r\n",
        "data = data.fillna(method= \"ffill\")\r\n",
        "data.head(3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Sentence #           Word  POS Tag\n",
              "0  Sentence: 1      Thousands  NNS   O\n",
              "1  Sentence: 1             of   IN   O\n",
              "2  Sentence: 1  demonstrators  NNS   O"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8dHELTM51wz"
      },
      "source": [
        "Tag Entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tqbbrExwD2k"
      },
      "source": [
        "#data[\"tag\"].value_counts()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grP8fwjl7SNY"
      },
      "source": [
        "Build Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f59_bEm8Aej",
        "outputId": "face549c-c22f-4725-d8cf-1107d10456d0"
      },
      "source": [
        "## Extract the 'Word' column from the dataframe\r\n",
        "words = data.loc[:, \"Word\"]\r\n",
        "!touch words.txt\r\n",
        "vocab = {}\r\n",
        "with open('words.txt') as f:\r\n",
        "  for i, l in enumerate(f.read().splitlines()):\r\n",
        "    vocab[l] = i\r\n",
        "  print(\"Number of words:\", len(vocab))\r\n",
        "  vocab['<PAD>'] = len(vocab)\r\n",
        "\r\n",
        "## Convert into a text file using the .savetxt() function\r\n",
        "np.savetxt(r'words.txt', words.values, fmt=\"%s\")\r\n",
        "\r\n",
        "class Get_sentence(object):\r\n",
        "    def __init__(self,data):\r\n",
        "        self.n_sent=1\r\n",
        "        self.data = data\r\n",
        "        agg_func = lambda s:[(w,p,t) for w,p,t in zip(s[\"Word\"].values.tolist(),\r\n",
        "                                                     s[\"POS\"].values.tolist(),\r\n",
        "                                                     s[\"Tag\"].values.tolist())]\r\n",
        "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\r\n",
        "        self.sentences = [s for s in self.grouped]\r\n",
        "\r\n",
        "getter = Get_sentence(data)\r\n",
        "sentence = getter.sentences\r\n",
        "\r\n",
        "words = list(set(data[\"Word\"].values))\r\n",
        "words_tag = list(set(data[\"Tag\"].values))\r\n",
        "\r\n",
        "word_idx = {w : i+1 for i ,w in enumerate(words)}\r\n",
        "tag_idx =  {t : i for i ,t in enumerate(words_tag)}\r\n",
        "\r\n",
        "X = [[word_idx[w[0]] for w in s] for s in sentence]\r\n",
        "y = [[tag_idx[w[2]] for w in s] for s in sentence]\r\n",
        "\r\n",
        "def data_generator(batch_size, x, y,pad, shuffle=False, verbose=False):\r\n",
        "\r\n",
        "    num_lines = len(x)\r\n",
        "    lines_index = [*range(num_lines)]\r\n",
        "    if shuffle:\r\n",
        "        rnd.shuffle(lines_index)\r\n",
        "    \r\n",
        "    index = 0 \r\n",
        "    while True:\r\n",
        "        buffer_x = [0] * batch_size \r\n",
        "        buffer_y = [0] * batch_size \r\n",
        "\r\n",
        "        max_len = 0\r\n",
        "        for i in range(batch_size):\r\n",
        "            if index >= num_lines:\r\n",
        "                index = 0\r\n",
        "                if shuffle:\r\n",
        "                    rnd.shuffle(lines_index)\r\n",
        "            \r\n",
        "            buffer_x[i] = x[lines_index[index]]\r\n",
        "            buffer_y[i] = y[lines_index[index]]\r\n",
        "            \r\n",
        "            lenx = len(x[lines_index[index]])    \r\n",
        "            if lenx > max_len:\r\n",
        "                max_len = lenx                  \r\n",
        "            \r\n",
        "            index += 1\r\n",
        "\r\n",
        "        X = np.full((batch_size, max_len), pad)\r\n",
        "        Y = np.full((batch_size, max_len), pad)\r\n",
        "\r\n",
        "\r\n",
        "        for i in range(batch_size):\r\n",
        "            x_i = buffer_x[i]\r\n",
        "            y_i = buffer_y[i]\r\n",
        "\r\n",
        "            for j in range(len(x_i)):\r\n",
        "\r\n",
        "                X[i, j] = x_i[j]\r\n",
        "                Y[i, j] = y_i[j]\r\n",
        "\r\n",
        "        if verbose: print(\"index=\", index)\r\n",
        "        yield((X,Y))\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.1,random_state=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg_Wpw1p9VJ0"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FjfBxJQMlfd",
        "outputId": "cab160f3-7d04-4ad5-948e-4334bed27ae0"
      },
      "source": [
        "!pip install --upgrade jax # install jax(base)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: jax in /usr/local/lib/python3.6/dist-packages (0.2.7)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from jax) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jax) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlgHrOiK8BDd"
      },
      "source": [
        "def NERmodel(tags, vocab_size=35178, d_model=50):\r\n",
        "    model = tl.Serial(\r\n",
        "        trax.models.reformer.Reformer(vocab_size, d_model, ff_activation=tl.LogSoftmax),\r\n",
        "        tl.Dense(tags),\r\n",
        "        tl.LogSoftmax()\r\n",
        "    )\r\n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeQ_jitd94IY"
      },
      "source": [
        "model = NERmodel(tags=17)\r\n",
        "#print(model)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs2CBZ2X99uJ"
      },
      "source": [
        "from trax.supervised import training\r\n",
        "\r\n",
        "rnd.seed(33)\r\n",
        "\r\n",
        "batch_size = 64\r\n",
        "\r\n",
        "train_generator = trax.data.inputs.add_loss_weights(\r\n",
        "    data_generator(batch_size, x_train, y_train,vocab['<PAD>'], True),\r\n",
        "    id_to_mask=vocab['<PAD>'])\r\n",
        "\r\n",
        "eval_generator = trax.data.inputs.add_loss_weights(\r\n",
        "    data_generator(batch_size, x_test, y_test,vocab['<PAD>'] ,True),\r\n",
        "    id_to_mask=vocab['<PAD>'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndyi-r9C-cY3"
      },
      "source": [
        "def train_model(model, train_generator, eval_generator, train_steps=1, output_dir='model'):\r\n",
        "    train_task = training.TrainTask(\r\n",
        "      train_generator,  \r\n",
        "      loss_layer = tl.CrossEntropyLoss(), \r\n",
        "      optimizer = trax.optimizers.Adam(0.01), \r\n",
        "      n_steps_per_checkpoint=10\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask(\r\n",
        "      labeled_data = eval_generator, \r\n",
        "      metrics = [tl.CrossEntropyLoss(), tl.Accuracy()], \r\n",
        "      n_eval_batches = 10 \r\n",
        "    )\r\n",
        "\r\n",
        "    training_loop = training.Loop(\r\n",
        "        model, \r\n",
        "        train_task, \r\n",
        "        eval_tasks = eval_task, \r\n",
        "        output_dir = output_dir) \r\n",
        "\r\n",
        "    training_loop.run(n_steps = train_steps)\r\n",
        "    return training_loop"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQxGgVFE-cWP",
        "outputId": "aa93ca68-f0a5-4116-a8c9-436e0fe2169a"
      },
      "source": [
        "train_steps = 100\r\n",
        "training_loop = train_model(model, train_generator, eval_generator, train_steps)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 64264085\n",
            "Step      1: Ran 1 train steps in 85.09 secs\n",
            "Step      1: train CrossEntropyLoss |  3.15675759\n",
            "Step      1: eval  CrossEntropyLoss |  3.98971963\n",
            "Step      1: eval          Accuracy |  0.85465195\n",
            "\n",
            "Step     10: Ran 9 train steps in 401.32 secs\n",
            "Step     10: train CrossEntropyLoss |  4.18205214\n",
            "Step     10: eval  CrossEntropyLoss |  4.08520691\n",
            "Step     10: eval          Accuracy |  0.85012255\n",
            "\n",
            "Step     20: Ran 10 train steps in 344.50 secs\n",
            "Step     20: train CrossEntropyLoss |  5.38061810\n",
            "Step     20: eval  CrossEntropyLoss |  5.23265591\n",
            "Step     20: eval          Accuracy |  0.85214944\n",
            "\n",
            "Step     30: Ran 10 train steps in 85.92 secs\n",
            "Step     30: train CrossEntropyLoss |  4.72383165\n",
            "Step     30: eval  CrossEntropyLoss |  3.02188914\n",
            "Step     30: eval          Accuracy |  0.84257923\n",
            "\n",
            "Step     40: Ran 10 train steps in 242.10 secs\n",
            "Step     40: train CrossEntropyLoss |  2.26916385\n",
            "Step     40: eval  CrossEntropyLoss |  1.50706003\n",
            "Step     40: eval          Accuracy |  0.84054840\n",
            "\n",
            "Step     50: Ran 10 train steps in 32.06 secs\n",
            "Step     50: train CrossEntropyLoss |  1.17165399\n",
            "Step     50: eval  CrossEntropyLoss |  1.03978483\n",
            "Step     50: eval          Accuracy |  0.84494420\n",
            "\n",
            "Step     60: Ran 10 train steps in 83.61 secs\n",
            "Step     60: train CrossEntropyLoss |  0.94218439\n",
            "Step     60: eval  CrossEntropyLoss |  0.97405193\n",
            "Step     60: eval          Accuracy |  0.84588828\n",
            "\n",
            "Step     70: Ran 10 train steps in 83.80 secs\n",
            "Step     70: train CrossEntropyLoss |  0.94652903\n",
            "Step     70: eval  CrossEntropyLoss |  0.90257180\n",
            "Step     70: eval          Accuracy |  0.84637592\n",
            "\n",
            "Step     80: Ran 10 train steps in 86.69 secs\n",
            "Step     80: train CrossEntropyLoss |  0.85901785\n",
            "Step     80: eval  CrossEntropyLoss |  0.82021437\n",
            "Step     80: eval          Accuracy |  0.84403076\n",
            "\n",
            "Step     90: Ran 10 train steps in 139.57 secs\n",
            "Step     90: train CrossEntropyLoss |  0.80658495\n",
            "Step     90: eval  CrossEntropyLoss |  0.80226091\n",
            "Step     90: eval          Accuracy |  0.84803609\n",
            "\n",
            "Step    100: Ran 10 train steps in 140.91 secs\n",
            "Step    100: train CrossEntropyLoss |  0.76753217\n",
            "Step    100: eval  CrossEntropyLoss |  0.77913768\n",
            "Step    100: eval          Accuracy |  0.84681881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WphpBrB-cTO",
        "outputId": "bc2a3128-ca7b-493d-d435-3617cf3a3a05"
      },
      "source": [
        "train_steps"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piK9WG3U-cQF"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}