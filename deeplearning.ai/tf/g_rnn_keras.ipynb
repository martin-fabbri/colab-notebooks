{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "g_rnn_keras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOTcuTt/t06433CbZPwzmzQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/tf/g_rnn_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeHA6WtbvNpK"
      },
      "source": [
        "# Recurrent Neural Networks (RNN) with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-MusyY2voyO"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Recurrent neural networks (RNN) are a class of neural networks that is powerful for\n",
        "modeling sequence data such as time series or natural language.\n",
        "\n",
        "Schematically, a RNN layer uses a `for` loop to iterate over the timesteps of a\n",
        "sequence, while maintaining an internal state that encodes information about the\n",
        "timesteps it has seen so far.\n",
        "\n",
        "The Keras RNN API is designed with a focus on:\n",
        "\n",
        "- **Ease of use**: the built-in `keras.layers.RNN`, `keras.layers.LSTM`,\n",
        "`keras.layers.GRU` layers enable you to quickly build recurrent models without\n",
        "having to make difficult configuration choices.\n",
        "\n",
        "- **Ease of customization**: You can also define your own RNN cell layer (the inner\n",
        "part of the `for` loop) with custom behavior, and use it with the generic\n",
        "`keras.layers.RNN` layer (the `for` loop itself). This allows you to quickly\n",
        "prototype different research ideas in a flexible way with minimal code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBm62UVdvM4E"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRukNPcd9Ecz"
      },
      "source": [
        "## Built-in RNN layers: a simple \n",
        "\n",
        "There are three built-in RNN layers in Keras:\n",
        "\n",
        "1. `keras.layers.SimpleRNN`, a fully-connected RNN where the output from previous\n",
        "timestep is to be fed to next timestep.\n",
        "\n",
        "2. `keras.layers.GRU`, first proposed in\n",
        "[Cho et al., 2014](https://arxiv.org/abs/1406.1078).\n",
        "\n",
        "3. `keras.layers.LSTM`, first proposed in\n",
        "[Hochreiter & Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf).\n",
        "\n",
        "In early 2015, Keras had the first reusable open-source Python implementations of LSTM\n",
        "and GRU.\n",
        "\n",
        "Here is a simple example of a `Sequential` model that processes sequences of integers,\n",
        "embeds each integer into a 64-dimensional vector, then processes the sequence of\n",
        "vectors using a `LSTM` layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmYG4oVg9AH8",
        "outputId": "f6736529-75c8-4e65-d1e7-f6359c2b0df7"
      },
      "source": [
        "model = keras.Sequential([\n",
        "  layers.Embedding(input_dim=1000, output_dim=64),\n",
        "  layers.LSTM(128),\n",
        "  layers.Dense(10)                        \n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          64000     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 164,106\n",
            "Trainable params: 164,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCUcpBga9oOT"
      },
      "source": [
        "Built-in RNNs support a number of useful features:\n",
        "\n",
        "- Recurrent dropout, via the `dropout` and `recurrent_dropout` arguments\n",
        "- Ability to process an input sequence in reverse, via the `go_backwards` argument\n",
        "- Loop unrolling (which can lead to a large speedup when processing short sequences on\n",
        "CPU), via the `unroll` argument\n",
        "- ...and more.\n",
        "\n",
        "For more information, see the\n",
        "[RNN API documentation](https://keras.io/api/layers/recurrent_layers/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MviEC7kW9143"
      },
      "source": [
        "## Outputs and states\n",
        "\n",
        "By default, the output of a RNN layer contains a single vector per sample. This vector\n",
        "is the RNN cell output corresponding to the last timestep, containing information\n",
        "about the entire input sequence. The shape of this output is `(batch_size, units)`\n",
        "where `units` corresponds to the `units` argument passed to the layer's constructor.\n",
        "\n",
        "A RNN layer can also return the entire sequence of outputs for each sample (one vector\n",
        "per timestep per sample), if you set `return_sequences=True`. The shape of this output\n",
        "is `(batch_size, timesteps, units)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYbu05s89hLj",
        "outputId": "17b0fdc6-c14b-4dfb-c9f0-cebba9662f24"
      },
      "source": [
        "model = keras.Sequential([\n",
        "  layers.Embedding(input_dim=1000, output_dim=64),\n",
        "  layers.GRU(256, return_sequences=True),\n",
        "  layers.SimpleRNN(128),\n",
        "  layers.Dense(10)                        \n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 64)          64000     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, None, 256)         247296    \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 128)               49280     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 361,866\n",
            "Trainable params: 361,866\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL_QIpLD-vn9"
      },
      "source": [
        "In addition, a RNN layer can return its final internal state(s). The returned states\n",
        "can be used to resume the RNN execution later, or\n",
        "[to initialize another RNN](https://arxiv.org/abs/1409.3215).\n",
        "This setting is commonly used in the\n",
        "encoder-decoder sequence-to-sequence model, where the encoder final state is used as\n",
        "the initial state of the decoder.\n",
        "\n",
        "To configure a RNN layer to return its internal state, set the `return_state` parameter\n",
        "to `True` when creating the layer. Note that `LSTM` has 2 state  tensors, but `GRU`\n",
        "only has one.\n",
        "\n",
        "To configure the initial state of the layer, just call the layer with additional\n",
        "keyword argument `initial_state`.\n",
        "Note that the shape of the state needs to match the unit size of the layer, like in the\n",
        "example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLnTLIC4-wcd",
        "outputId": "f8b3345c-2fcf-4629-e715-f9b63841a7a9"
      },
      "source": [
        "encoder_vocab = 1000\n",
        "decoder_vocab = 2000\n",
        "\n",
        "encoder_input = layers.Input(shape=(None,))\n",
        "encoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=64)(\n",
        "    encoder_input\n",
        ")\n",
        "\n",
        "# Return states in addition to output\n",
        "output, state_h, state_c = layers.LSTM(64, return_state=True, name=\"encoder\")(\n",
        "    encoder_embedded\n",
        ")\n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "decoder_input = layers.Input(shape=(None,))\n",
        "decoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=64)(\n",
        "    decoder_input\n",
        ")\n",
        "\n",
        "# Pass the 2 states to a new LSTM layer, as initial state\n",
        "decoder_output = layers.LSTM(64, name=\"decoder\")(\n",
        "    decoder_embedded, initial_state=encoder_state\n",
        ")\n",
        "output = layers.Dense(10)(decoder_output)\n",
        "\n",
        "model = keras.Model([encoder_input, decoder_input], output)\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 64)     64000       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, None, 64)     128000      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "encoder (LSTM)                  [(None, 64), (None,  33024       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "decoder (LSTM)                  (None, 64)           33024       embedding_3[0][0]                \n",
            "                                                                 encoder[0][1]                    \n",
            "                                                                 encoder[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           650         decoder[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 258,698\n",
            "Trainable params: 258,698\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}