{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "c3_w2_loading_text.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPIysnnm94FJKhLXPEBZGdo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/tf/c3_w2_loading_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPXHnP2dD91b"
      },
      "source": [
        "# Load Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl5cBqD-EGiR"
      },
      "source": [
        "We will use lower-level utilities like `tf.data.TextLineDataset` to load text files, and `tf.text` to preprocess the data for finer-grain control."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXgZgdXrFn6g"
      },
      "source": [
        "!pip install tensorflow_text -q"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PihQwFJbC4Mh",
        "outputId": "4c515720-43e2-4e82-ea9f-4304ca7bb8de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import collections\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "tf.__version__, tfds.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2.3.0', '4.0.1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJkyqOTrGT0x"
      },
      "source": [
        "## Predict the tag for a Stack Overflow question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvbL46Z0FHGj",
        "outputId": "9ee3ab0f-523a-4e5c-f059-5167510f6889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
        "dataset = utils.get_file(\n",
        "    'stack_overflow_16k.tar.gz',\n",
        "    data_url,\n",
        "    untar=True,\n",
        "    cache_dir='stack_overflow',\n",
        "    cache_subdir=''\n",
        ")\n",
        "dataset_dir = pathlib.Path(dataset).parent\n",
        "dataset_dir, pathlib.Path(dataset)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('/tmp/.keras'), PosixPath('/tmp/.keras/stack_overflow_16k.tar.gz'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrnshUv9GSEU",
        "outputId": "24f4dcd3-8fc4-4374-d5f3-60d13876a056",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(dataset_dir.iterdir())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/tmp/.keras/README.md'),\n",
              " PosixPath('/tmp/.keras/train'),\n",
              " PosixPath('/tmp/.keras/stack_overflow_16k.tar.gz.tar.gz'),\n",
              " PosixPath('/tmp/.keras/test')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP11js0sGSJ1",
        "outputId": "9f3916cc-0ddc-45c9-f427-c2d1159dc3f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dir = dataset_dir/'train'\n",
        "list(train_dir.iterdir())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/tmp/.keras/train/javascript'),\n",
              " PosixPath('/tmp/.keras/train/python'),\n",
              " PosixPath('/tmp/.keras/train/csharp'),\n",
              " PosixPath('/tmp/.keras/train/java')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRJs0cyXISj1"
      },
      "source": [
        "The `train/csharp`, `train/java`, `train/python`, and `train/javascript` directories contain many text files, each of wich is Stack Overflow question. Print a file and inspect the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYvH4eB-GSMt",
        "outputId": "fc40f105-ac73-4356-d219-2ec55870920c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sample_file = train_dir/'python/1757.txt'\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"i want to return a specific list when i have a collection of lists i want to return a specific list, when i have a collection of lists, but i am not to sure how to do this. i tried this approach but it didn't work. any ideas..this_list1 = [2,3,4,5].this_list2 = [5,6,9,8]..x = input(\"\"which list do you want\"\")..print this_list(x)\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NweuvvvIJpal"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "Next, we will load the data off disk and prepare it into a format suitable for training. To do so, you will use `text_dataset_from_directory` utility to create a labeled `tf.data.Dataset`.\n",
        "\n",
        "The `preprocessing.text_dataset_from_directory` expects a directory structure as follows.\n",
        "\n",
        "```\n",
        "train/\n",
        "...csharp/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...java/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...javascript/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...python/\n",
        "......1.txt\n",
        "......2.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE_vy8MKKnjM"
      },
      "source": [
        "When running a machine learning experiment, it is a best practice to divide your dataset into three splits: train, validation and test. Test Stack Overflow has already been divided into train and test, but it lack a validation set. Create a validation set using a 80:20 split of the training data by using the `validation_split` argument below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTCVSQKvGSHF",
        "outputId": "1e04ce3b-7bc9-451b-9567-7e8965f4be11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = preprocessing.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed    \n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 4 classes.\n",
            "Using 6400 files for training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr4rsQhONNka"
      },
      "source": [
        "As you can see above, there are 8,000 examples in the training folder, of which you will use 80% (or 6,400) for training. As you will see in a moment, you can train a model by passing a `tf.data.Dataset` directly to `model.fit`. First, iterate over the dataset and print out a few examples, to get a feel for the data.\n",
        "\n",
        "Note: To increase the difficulty of the classification problem, the dataset author replaced occurrences of the words *Python*, *CSharp*, *JavaScript*, or *Java* in the programming question with the word *blank*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfJmyx_CGSBR",
        "outputId": "cef63428-d701-4ecb-e834-73a27cccd592",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(10):\n",
        "    print('Question:', text_batch.numpy()[i][:100], '...')\n",
        "    print('Label:', label_batch.numpy()[i])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: b'\"my tester is going to the wrong constructor i am new to programming so if i ask a question that can' ...\n",
            "Label: 1\n",
            "Question: b'\"blank code slow skin detection this code changes the color space to lab and using a threshold finds' ...\n",
            "Label: 3\n",
            "Question: b'\"option and validation in blank i want to add a new option on my system where i want to add two text' ...\n",
            "Label: 1\n",
            "Question: b'\"exception: dynamic sql generation for the updatecommand is not supported against a selectcommand th' ...\n",
            "Label: 0\n",
            "Question: b'\"parameter with question mark and super in blank, i\\'ve come across a method that is formatted like t' ...\n",
            "Label: 1\n",
            "Question: b'call two objects wsdl the first time i got a very strange wsdl. ..i would like to call the object (i' ...\n",
            "Label: 0\n",
            "Question: b'how to correctly make the icon for systemtray in blank using icon sizes of any dimension for systemt' ...\n",
            "Label: 0\n",
            "Question: b'\"is there a way to check a variable that exists in a different script than the original one? i\\'m try' ...\n",
            "Label: 3\n",
            "Question: b'\"blank control flow i made a number which asks for 2 numbers with blank and responds with  the corre' ...\n",
            "Label: 0\n",
            "Question: b'\"credentials cannot be used for ntlm authentication i am getting org.apache.commons.httpclient.auth.' ...\n",
            "Label: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrcIgCNSOMN2"
      },
      "source": [
        "The labels are `0`, `1`, `2` or `3`. To see which of these correspond to which string label, you can check the `class_names` property on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0i9QZRkNj2V",
        "outputId": "789f7f01-5e12-415a-f07e-e1f13790b7e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i, label in enumerate(raw_train_ds.class_names):\n",
        "  print('Label', i, 'corresponds to', label)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label 0 corresponds to csharp\n",
            "Label 1 corresponds to java\n",
            "Label 2 corresponds to javascript\n",
            "Label 3 corresponds to python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIsCv1UHOnL8",
        "outputId": "b1d1c9fd-c01b-4fdf-ae1b-07c6448efb33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "raw_train_ds.class_names"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['csharp', 'java', 'javascript', 'python']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WtrRZ0HOnRo",
        "outputId": "ba48478f-072a-41eb-ecff-75f519d232a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "raw_val_ds = preprocessing.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 4 classes.\n",
            "Using 1600 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48XxrVKsOnXn",
        "outputId": "32e41749-6a98-41de-e807-9146889f09aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_dir = dataset_dir/'test'\n",
        "raw_test_dir = preprocessing.text_dataset_from_directory(\n",
        "    test_dir,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_7SfXDGophp"
      },
      "source": [
        "Next, you will standardize, tokenize, and vectorize the data using the `preprocessing.TextVectorization` layer.\n",
        "* Standardization refers to preprocessing the text, typically to remove punctuation or HTML elements to simplify the dataset.\n",
        "\n",
        "* Tokenization refers to splitting strings into tokens (for example, splitting a sentence into individual words by splitting on whitespace).\n",
        "\n",
        "* Vectorization refers to converting tokens into numbers so they can be fed into a neural network.\n",
        "\n",
        "All of these tasks can be accomplished with this layer. You can learn more about each of these in the [API doc](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization).\n",
        "\n",
        "* The default standardization converts text to lowercase and removes punctuation.\n",
        "\n",
        "* The default tokenizer splits on whitespace.\n",
        "\n",
        "* The default vectorization mode is `int`. This outputs integer indices (one per token). This mode can be used to build models that take word order into account. You can also use other modes, like `binary`, to build bag-of-word models.\n",
        "\n",
        "\n",
        "You will build two modes to learn more about these. First, you will use the `binary` model to build a bag-of-words model. Next, you will use the `int` mode with a 1D ConvNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erKaDD0gOndJ"
      },
      "source": [
        "VOCAB_SIZE=10000\n",
        "\n",
        "binary_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='binary'\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFgHA-G3pJs7"
      },
      "source": [
        "For `int` mode, in addition to maximum vocabulary size, you need to set an explicit maximum sequence length, which will cause the layer to pad or truncate sequences to exactly sequence_length values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKWPRHjZOnjC"
      },
      "source": [
        "MAX_SEQUENCE_LENGHT = 250\n",
        "\n",
        "int_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGHT\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXednOCRqOsG"
      },
      "source": [
        "Next, you will call adapt to fit the state of the preprocessing layer to the dataset. This will cause the model to build an index of strings to integers.\n",
        "\n",
        "Note: it's important to only use your training data when calling adapt (using the test set would leak information)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JSsQYgyOnmF"
      },
      "source": [
        "# make a text-only dataset (without labels), then call adapt\n",
        "train_text = raw_train_ds.map(lambda text, labels: text)\n",
        "binary_vectorize_layer.adapt(train_text)\n",
        "int_vectorize_layer.adapt(train_text)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWqxMToUq0ZB"
      },
      "source": [
        "See the results of using these layers to process data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc5GVpAlOngK"
      },
      "source": [
        "def binary_vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return binary_vectorize_layer(text), label"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_ZlaDUJOnai"
      },
      "source": [
        "def int_vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return int_vectorize_layer(text), label"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hynx2d9wOnUr",
        "outputId": "82cf3dec-06bc-4bd8-a38f-a8dfe4f31b2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_question, first_label = text_batch[0], label_batch[0]\n",
        "print('question:', first_question)\n",
        "print('label:', first_label)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "question: tf.Tensor(b'\"function expected error in blank for dynamically created check box when it is clicked i want to grab the attribute value.it is working in ie 8,9,10 but not working in ie 11,chrome shows function expected error..&lt;input type=checkbox checked=\\'checked\\' id=\\'symptomfailurecodeid\\' tabindex=\\'54\\' style=\\'cursor:pointer;\\' onclick=chkclickevt(this);  failurecodeid=\"\"1\"\" &gt;...function chkclickevt(obj) { .    alert(obj.attributes(\"\"failurecodeid\"\"));.}\"\\n', shape=(), dtype=string)\n",
            "label: tf.Tensor(2, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7AEPgtGOnO3",
        "outputId": "4b807500-5135-4114-87e2-12b1012bb7ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('binary vectorized question:', binary_vectorize_text(first_question, first_label)[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "binary vectorized question: tf.Tensor([[1. 1. 1. ... 0. 0. 0.]], shape=(1, 10000), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asuqfy5iOnIr",
        "outputId": "4cb1a73f-c562-4a2f-8b36-1df3a9c784aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('int vectorized question:', int_vectorize_text(first_question, first_label)[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "int vectorized question: tf.Tensor(\n",
            "[[  38  450   65    7   16   12  892  265  186  451   44   11    6  685\n",
            "     3   46    4 2062    2  485    1    6  158    7  479    1   26   20\n",
            "   158    7  479    1  502   38  450    1 1767 1763    1    1    1    1\n",
            "     1    1    1    1    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]], shape=(1, 250), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}