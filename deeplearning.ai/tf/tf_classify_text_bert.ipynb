{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tf_classify_text_bert.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNWUcmSbP1UZWdnPJJ60Y1/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"85DXqRy3UfJN"},"source":["# Classify text with BERT\r\n","\r\n","BERT and other Transformer encoder architectures have been wildly successful on a variety of NLP tasks. They compute vector space representations of natural language that are suitable for use in Deep Learning models. The BERT family of model uses the transformer encoder architecture to process each token of input text in the full context of tokens before and after."]},{"cell_type":"markdown","metadata":{"id":"KQlybARVl7j_"},"source":["## BERT models results\n","\n","Model | Architecture | Loss | Accuracy | Training Time (5 epochs)\n","--- | --- | --- | --- | ---\n","Sm BERT | L-12_H-768_A-12/3 | 0.6232010126113892 | 0.8850799798965454 | ?\n","Albert | - | 0.6523738503456116 | 0.884880006313324 | 24 mins (1 GPU)\n"]},{"cell_type":"code","metadata":{"id":"y0q3NLh4VbzY","cellView":"form"},"source":["#@title ##Install dependencies\r\n","#@markdown - tensorflow-text: text preprocessing\r\n","#@markdown - tf-models-official: pre-trained models hosted on tensorflow-hub\r\n","%%capture --no-stderr\r\n","!pip install -Uqq tensorflow-text\r\n","!pip install -Uqq tf-models-official"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m8f8oBy2lAHz","cellView":"form"},"source":["#@title ##Import packages\r\n","import os\r\n","import shutil\r\n","import uuid\r\n","\r\n","import matplotlib.pyplot as plt\r\n","import pandas as pd\r\n","import tensorflow as tf\r\n","import tensorflow_hub as hub\r\n","import tensorflow_text as text\r\n","from google.colab import auth, data_table\r\n","from official.nlp import optimization\r\n","from tensorflow.config.experimental import list_physical_devices\r\n","from tensorflow.keras import Model\r\n","from tensorflow.keras.layers import Dense, Dropout, Input \r\n","from tensorflow.keras.preprocessing import text_dataset_from_directory\r\n","\r\n","tf.get_logger().setLevel(\"ERROR\")\r\n","\r\n","print(\"tensorflow:\", tf.__version__)\r\n","print(\"tensorflow_hub:\", hub.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDa_SgmS849v","cellView":"form"},"source":["#@title ## Google Cloud Setup\r\n","#@markdown - Authenticate user\r\n","#@markdown - Project config\r\n","#@markdown - Create dataset bucket\r\n","#project_id = \"gcp project id\" #@param {type:\"string\"}\r\n","\r\n","#auth.authenticate_user()\r\n","\r\n","# !gcloud config set project {project_id}\r\n","\r\n","# bucket_name = f\"bert-text-cls-{uuid.uuid1()}\"\r\n","# !gsutil mb gs://{bucket_name}\r\n","\r\n","# with open(\"to_upload.txt\", \"w\") as f:\r\n","#     f.write(\"my sample file\")\r\n","\r\n","# !gsutil cp to_upload.txt gs://{bucket_name}\r\n","# !gsutil cat gs://{bucket_name}/to_upload.txt\r\n","# !gsutil rm -f -r gs://{bucket_name}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nlaoaC8-qnC4"},"source":["#@title Choose Accelerator Strategy\r\n","accelerator_strategy = \"GPU\" #@param [\"TPU\", \"GPU\", \"None\"]\r\n","\r\n","if accelerator_strategy == \"TPU\":\r\n","    try:\r\n","        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\r\n","        print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\r\n","    except ValueError:\r\n","        raise BaseException('ERROR: Not connected to a TPU runtime.')\r\n","\r\n","    tf.config.experimental_connect_to_cluster(tpu)\r\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\r\n","    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n","    print(\"TPU devices: \", tf.config.list_logical_devices('TPU'))\r\n","else:\r\n","    gpus = list_physical_devices('GPU')\r\n","    if gpus:\r\n","        # Restrict TensorFlow to only use the first GPU\r\n","        try:\r\n","            tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\r\n","            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n","            print('Running on GPU')\r\n","            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\r\n","        except RuntimeError as e:\r\n","            # Visible devices must be set before GPUs have been initialized\r\n","            print(e)\r\n","            print(\"Num GPUs available:\", len(list_physical_devices(GPU)))\r\n","    else:\r\n","        print(\"Running on CPU.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qKfKG46UNaND","cellView":"form"},"source":["#@title Download Dataset - Large Movie Review Dataset\r\n","#@markdown [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) \r\n","#@markdown that contains the text of 50,000 movie reviews from the \r\n","#@markdown [Internet Movie Database](https://www.imdb.com/).\r\n","\r\n","url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\r\n","dataset = tf.keras.utils.get_file(\r\n","    \"aclImdb_v1.tar.gz\", url, untar=True, cache_dir=\".\", cache_subdir=\"\"\r\n",")\r\n","dataset_dir = os.path.join(os.path.dirname(dataset), \"aclImdb\")\r\n","train_dir = os.path.join(dataset_dir, \"train\")\r\n","print(\"Train dataset:\", train_dir)\r\n","\r\n","# remove unused folders from train folder\r\n","remove_dir = os.path.join(train_dir, \"unsup\")\r\n","shutil.rmtree(remove_dir)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rfJEwm8MdRws"},"source":["The IMDB dataset has already been divided into train and test, but it lacks a validation set. Let's create a validation set using an 80:20 split of the training data by using `validation_split`. \r\n","\r\n",">When using `validation_split` and `subset` or to pass `suffle=False` so that the validation and training splits have no overlap."]},{"cell_type":"code","metadata":{"id":"oaz420CCbLs5"},"source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n","batch_size = 32\r\n","seed = 42\r\n","\r\n","def optimize_dataset(dataset):\r\n","    return dataset.cache().prefetch(buffer_size=AUTOTUNE)\r\n","\r\n","raw_train_ds = text_dataset_from_directory(\r\n","    \"aclImdb/train\",\r\n","    batch_size=batch_size,\r\n","    validation_split=0.2,\r\n","    subset='training',\r\n","    seed=seed\r\n",")\r\n","\r\n","class_names = raw_train_ds.class_names\r\n","train_ds = optimize_dataset(raw_train_ds)\r\n","\r\n","val_ds = text_dataset_from_directory(\r\n","    \"aclImdb/train\",\r\n","    batch_size=batch_size,\r\n","    validation_split=0.2,\r\n","    subset='validation',\r\n","    seed=seed\r\n",")\r\n","val_ds = optimize_dataset(val_ds)\r\n","\r\n","test_ds = tf.keras.preprocessing.text_dataset_from_directory(\r\n","    \"aclImdb/test\",\r\n","    batch_size=batch_size\r\n",")\r\n","test_ds = optimize_dataset(test_ds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-mG7-8XA6CKQ"},"source":["text_batch, label_batch = next(train_ds.as_numpy_iterator())\r\n","df = pd.DataFrame({\"text\": text_batch, \"label\": label_batch})\r\n","data_table.DataTable(df, include_index=False, num_rows_per_page=3, )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CI9_5ocFRyQG"},"source":["## BERT models overview\r\n","\r\n","Currently these is the family of BERT models available on Tensorflow Hub:\r\n","\r\n","- `BERT-Base`, `Uncased` the original BERT models.\r\n","- `Small BERTs` maintain the original architecture but with fewer and/or smaller Transformer blocks.\r\n","- `ALBERT` reduces the model size by sharing parameters between layers. Doesn't improve processing times.\r\n","- `BERT Experts` offer a choice of domain specific pre-trained models.\r\n","- `Electra` gets trained as a discriminator from GANs. (A must try!)\r\n","- `BERT with talking-heads Attention` has improved the core of the Transformers architecture.\r\n","\r\n","### Game plan\r\n","\r\n","1. Start with a smaller model (`sm BERT uncased`) since they are faster to train.\r\n","2. Upgrade to ALBERT looking for higher accuracy.\r\n","3. Models like Electra, Talking Heads or BERT expert are the next options in terms of improving accuracy.\r\n","\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"gBdNMHx7IuUr","cellView":"form"},"source":["#@title Choose a BERT model to fine-tune\r\n","\r\n","bert_model_name = \"sm electra\" #@param [\"sm BERT uncased\", \"albert\", \"sm electra\", \"talking heads\"]\r\n","\r\n","map_name_to_handle = {\r\n","    'sm BERT uncased':\r\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\r\n","    'albert':\r\n","        'https://tfhub.dev/tensorflow/albert_en_base/2',\r\n","    'sm electra':\r\n","        'https://tfhub.dev/google/electra_small/2',\r\n","    'talking heads':\r\n","        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\r\n","}\r\n","\r\n","map_model_to_preprocess = {\r\n","    'sm BERT uncased':\r\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\r\n","    'albert':\r\n","        'https://tfhub.dev/tensorflow/albert_en_preprocess/2',\r\n","    'sm electra':\r\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\r\n","    'talking heads':\r\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\r\n","}\r\n","\r\n","tfhub_handle_encoder = map_name_to_handle[bert_model_name]\r\n","tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\r\n","\r\n","print(f'BERT model selected           : {tfhub_handle_encoder}')\r\n","print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GUcic603nLok"},"source":["## Explore the pre-processing layer\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"9LdFlF4piKmR"},"source":["bert_preprocessing_model = hub.KerasLayer(tfhub_handle_preprocess)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XJb9tvISnotq"},"source":["test_sentence = ['a masterpiece, maybe the best movie of the year!']\r\n","preprocessed_sentence = bert_preprocessing_model(test_sentence)\r\n","\r\n","print(f\"Keys       : {list(preprocessed_sentence.keys())}\")\r\n","#@markdown `input sequence` length is truncated to 128 tokens.<br><br>\r\n","print(f\"Shape      : {preprocessed_sentence['input_word_ids'].shape}\")\r\n","print(f\"Word Ids   : {preprocessed_sentence['input_word_ids'][0, :5]}\")\r\n","#@markdown `input_type_ids` only have zeros because this is a single sentence \r\n","#@markdown input. For a multiple sentence input, it would have one number for \r\n","#@markdown each input.\r\n","print(f\"Input Mask : {preprocessed_sentence['input_mask'][0, :5]}\")\r\n","print(f\"Type Ids   : {preprocessed_sentence['input_type_ids'][0, :5]}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ITZ3hP03uzvQ"},"source":["## Explore the encoder layer"]},{"cell_type":"code","metadata":{"id":"I8EdsUPes4P6"},"source":["bert_model = hub.KerasLayer(tfhub_handle_encoder)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"4qtKJ8ZTvPyZ"},"source":["bert_results = bert_model(preprocessed_sentence)\r\n","\r\n","print(f\"Loaded BERT          : {tfhub_handle_encoder}\")\r\n","print(f\"Model keys           : {list(bert_results.keys())}\")\r\n","#@markdown `pooled_output` represents each input sentence as a whole. It's \r\n","#@markdown equivalent to the embedding for the entire sentence.<br><br>\r\n","print(f\"Pooled outputs shape : {bert_results['pooled_output'].shape}\")\r\n","print(f\"Pooled outputs values: {bert_results['pooled_output'][0,:5]}\")\r\n","\r\n","#@markdown `sequence_output` represents each input token in the context.\r\n","#@markdown Equivalent to the contextual embedding for every token in the \r\n","#@markdown sentence.<br><br>\r\n","print(f\"Sequence outputs shape : {bert_results['sequence_output'].shape}\")\r\n","print(f\"Pooled outputs values: {bert_results['sequence_output'][0,:5]}\")\r\n","\r\n","#@markdown `encoder_outputs` are the intermediate activations of the `L` \r\n","#@markdown transformer blocks.<br><br>\r\n","\r\n","#@markdown For fine tunning we will use the `pooled_output` array.<br><br>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"chNTv9ufOGj1"},"source":["## Build the classifier model"]},{"cell_type":"code","metadata":{"id":"kDFSGiBMyTKt"},"source":["def build_classifier_model(optimizer, loss, metrics):\r\n","    text_input = Input(shape=(), dtype=tf.string, name=\"text\")\r\n","    preprocessing_layer = hub.KerasLayer(\r\n","        tfhub_handle_preprocess, name=\"preproc\"\r\n","    )\r\n","    encoder_inputs = preprocessing_layer(text_input)\r\n","    encoder = hub.KerasLayer(\r\n","        tfhub_handle_encoder, trainable=True, name=\"encoder\"\r\n","    )\r\n","    outputs = encoder(encoder_inputs)\r\n","    net = outputs[\"pooled_output\"]\r\n","    net = Dropout(0.1)(net)\r\n","    net = Dense(1, activation=None, name=\"classifier\")(net)\r\n","\r\n","    model = Model(text_input, net)\r\n","    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\r\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q91tQK7xBiBE"},"source":["### Build model using acceleration scope"]},{"cell_type":"code","metadata":{"id":"jbNtHtpCSG4a"},"source":["#@title ### Loss function\r\n","#@markdown BinaryCrossEntropy from Logits.<br>\r\n","#@markdown Accuracy metric: Binary accuracy.\r\n","loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n","metrics = tf.metrics.BinaryAccuracy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n77kfl6KWV2Q"},"source":["### Optimizer \r\n","For fine-tunning, let's use the same optimizer the BERT was originally trained with: Adaptative Moments(ADAM). This optimizer minimizes the prediction loss and does regularization by weight decay(nout using moments), which is also known as AdamW.\r\n","\r\n","For the learning rate `init_lr`, we use the same schedule as BERT during pretraining: linear decay of a nontional initial learning rate, prefixed with a linear warm-up phase of the 10% of training steps `num_warmup_steps`. In line with BERT, the initial learning rate is smaller for fine-tunning (best of 5e-5, 3e-5, 2e-5). "]},{"cell_type":"code","metadata":{"id":"L4MdJWJBTZ_O"},"source":["epochs = 5\r\n","steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\r\n","num_train_steps = steps_per_epoch * epochs\r\n","num_warmup_steps = int(0.1 * num_train_steps)\r\n","\r\n","init_lr = 3e-5\r\n","optimizer = optimization.create_optimizer(\r\n","    init_lr=init_lr,\r\n","    num_train_steps=num_train_steps,\r\n","    num_warmup_steps=num_warmup_steps,\r\n","    optimizer_type=\"adamw\"\r\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rEW1GolBhi3"},"source":["classifier_model = build_classifier_model(optimizer, loss, metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CG5fdldMQWSP"},"source":["Let's test the our preprocessing layer integration"]},{"cell_type":"code","metadata":{"id":"B-kO9BAzQPlv"},"source":["bert_raw_result = classifier_model(tf.constant(test_sentence))\r\n","print(tf.sigmoid(bert_raw_result).numpy()[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tIuptE19Q91U"},"source":["Let's take a look at the model's structure."]},{"cell_type":"code","metadata":{"id":"6m40laZiQwPj"},"source":["tf.keras.utils.plot_model(classifier_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aieR6UD0SBmO"},"source":["## Model Training"]},{"cell_type":"code","metadata":{"id":"XePZtTJHoeFu"},"source":["print(f\"Training model with {tfhub_handle_encoder}\")\r\n","history = classifier_model.fit(\r\n","    x=train_ds,\r\n","    validation_data=val_ds,\r\n","    epochs=epochs\r\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_elYgpLXvdr5"},"source":["## Evaluate model\r\n","\r\n","Let's see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy."]},{"cell_type":"code","metadata":{"id":"hIHhGv9-vdH-"},"source":["loss, accuracy = classifier_model.evaluate(test_ds)\r\n","\r\n","print(f\"Loss: {loss}\")\r\n","print(f\"Accuracy: {accuracy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"CpMIRuNfybZG"},"source":["#@title ## Plot accuracy and loss\r\n","history_dict = history.history\r\n","# print(history_dict.keys())\r\n","\r\n","acc = history_dict['binary_accuracy']\r\n","val_acc = history_dict['val_binary_accuracy']\r\n","loss = history_dict['loss']\r\n","val_loss = history_dict['val_loss']\r\n","\r\n","epochs = range(1, len(acc) + 1)\r\n","fig = plt.figure(figsize=(10, 6))\r\n","fig.tight_layout()\r\n","\r\n","plt.subplot(2, 1, 1)\r\n","# \"bo\" is for \"blue dot\"\r\n","plt.plot(epochs, loss, 'r', label='Training loss')\r\n","# b is for \"solid blue line\"\r\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\r\n","plt.title('Training and validation loss')\r\n","# plt.xlabel('Epochs')\r\n","plt.ylabel('Loss')\r\n","plt.legend()\r\n","\r\n","plt.subplot(2, 1, 2)\r\n","plt.plot(epochs, acc, 'r', label='Training acc')\r\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\r\n","plt.title('Training and validation accuracy')\r\n","plt.xlabel('Epochs')\r\n","plt.ylabel('Accuracy')\r\n","plt.legend(loc='lower right');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zLGIwszMzE_E"},"source":["## Export Model for Inference"]},{"cell_type":"code","metadata":{"id":"EYqt_WnFy_Ew"},"source":["dataset_name = \"imdb\"\r\n","dataset_name = dataset_name.replace(\"/\", \"_\")\r\n","saved_model_path = f\"./{dataset_name}_bert\"\r\n","classifier_model.save(saved_model_path, include_optimizer=False)\r\n","print(\"Saved model:\", saved_model_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"weQiDR1M08GD"},"source":["Let's reload the model, so we can try it side by side with the model that is still in memory."]},{"cell_type":"code","metadata":{"id":"vB4JgoJN0JLS"},"source":["reloaded_model = tf.saved_model.load(saved_model_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fV8Cc1rp1UUk"},"source":["Let's validate the `reloaded_model` accuracy."]},{"cell_type":"code","metadata":{"id":"guxW1RuE1Llf"},"source":["def print_samples(inputs, results):\r\n","    result_for_printing = [\r\n","        f\"input: {inputs[i]:<30}  score: {results[i][0]:.6f}\"\r\n","        for i in range(len(inputs))\r\n","    ]\r\n","    print(*result_for_printing, sep=\"\\n\")\r\n","    print()\r\n","\r\n","\r\n","examples = [\r\n","    \"this is such an amazing movie!\",\r\n","    \"The movie was great!\",\r\n","    \"The movie was meh.\",\r\n","    \"The movie was okish.\",\r\n","    \"The movie was terrible...\",\r\n","]\r\n","\r\n","reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\r\n","original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\r\n","\r\n","print(\"Results from the <saved> model:\")\r\n","print_samples(examples, reloaded_results)\r\n","\r\n","print(\"Results from the <original> model:\")\r\n","print_samples(examples, original_results)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-HrkwZx9vJ-"},"source":["## Production ready model\r\n","\r\n","In order to serve the saved model on TF Serving, we need to get the results througn the named signatures (`serving_default` in our case)."]},{"cell_type":"code","metadata":{"id":"vPltCQtp2Ify"},"source":["serving_results = reloaded_model.\\\r\n","    signatures[\"serving_default\"](tf.constant(examples))\r\n","serving_results = tf.sigmoid(serving_results[\"classifier\"])\r\n","print_samples(examples, serving_results)\r\n"],"execution_count":null,"outputs":[]}]}