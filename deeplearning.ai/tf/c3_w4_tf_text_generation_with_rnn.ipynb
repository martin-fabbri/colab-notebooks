{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "c3_w4_tf_text_generation_with_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIw+mWxHj5lljepiKqbJkq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/tf/c3_w4_tf_text_generation_with_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2ZNNKokC4uK"
      },
      "source": [
        "## Text generation with a RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvs1vYkXvDWZ"
      },
      "source": [
        "## Text generation with an RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow6cS4aJ-nlw"
      },
      "source": [
        "import os\r\n",
        "import time\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from tensorflow.keras.utils import get_file \r\n",
        "from tensorflow.keras import Sequential\r\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "EA5PP2lZv-5o",
        "outputId": "a50ae4bd-be2f-4d23-cba7-ba3bf685d415"
      },
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\r\n",
        "path_to_file = get_file('shakespeare.txt', url)\r\n",
        "path_to_file"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/root/.keras/datasets/shakespeare.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5QNRlxTwpj0",
        "outputId": "65f369f0-9c0c-42f8-cf46-c679b8439b10"
      },
      "source": [
        "with open(path_to_file, 'rb') as file:\r\n",
        "    text = file.read().decode(encoding=\"utf-8\")\r\n",
        "    print(f'Length of text: {len(text):,}')\r\n",
        "    vocab = sorted(set(text))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1,115,394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCrzPoWRxnGQ"
      },
      "source": [
        "Take a looks at the dataset text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2Q5O4vfwphh",
        "outputId": "bf227372-4d3f-4663-d498-8626dcb11969"
      },
      "source": [
        "print(text[:70])\r\n",
        "print(\"...\")\r\n",
        "print(text[-70:])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Spe\n",
            "...\n",
            "et'st thy fortune sleep--die, rather; wink'st\n",
            "Whiles thou art waking.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nP_0EcG7nY-",
        "outputId": "56967e20-e79b-46d4-a843-221b826feaf9"
      },
      "source": [
        "print(f\"{len(vocab)} unique characters\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L9dQ52VyWWV"
      },
      "source": [
        "### Process the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMviBkATwpeU"
      },
      "source": [
        "char2idx = {char: index for index, char in enumerate(vocab)}\r\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi7KOoo4wpbp"
      },
      "source": [
        "text_as_int = np.array([char2idx[char] for char in text])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyjLh83UwfAy",
        "outputId": "bc550d7a-b5a4-4bc9-fe0c-aa1f8a13fc46"
      },
      "source": [
        "print(\"{\")\r\n",
        "for char,_ in zip(char2idx, range(5)):\r\n",
        "    print(\"  {:4s}: {:3d},\".format(repr(char), char2idx[char]))\r\n",
        "print(\"  ...\\n}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHB2qmCg82O1",
        "outputId": "13efdfdd-a1a8-469e-8659-3981201426b8"
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\r\n",
        "print(f\"{text[:13]} <-- mapped to int --> {text_as_int[:13]}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen <-- mapped to int --> [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqiX2F9r83Nw",
        "outputId": "e49d77ac-75a6-447f-98f5-2d49d8e84ba0"
      },
      "source": [
        "seq_length = 100\r\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\r\n",
        "examples_per_epoch"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11043"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixG9bXOp83LH",
        "outputId": "f391c594-6ec1-4fb0-fcf0-38de5d123262"
      },
      "source": [
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\r\n",
        "\r\n",
        "for i in char_dataset.take(5):\r\n",
        "    print(idx2char[i.numpy()], end='')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxSVTEZdQLCi"
      },
      "source": [
        "The `batch` method lets us easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGNGRktD83IB",
        "outputId": "e3cdf16b-e00d-47b5-c575-4cf729819212"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\r\n",
        "\r\n",
        "for item in sequences.take(5):\r\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GQzMy3bVnvA"
      },
      "source": [
        "For each sequence, duplicate and shift in to form the input and target text by using the `map` method to apply a simple function to each batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf10TV2w83Eu"
      },
      "source": [
        "def split_input_target(chunk):\r\n",
        "    input_text = chunk[:-1]\r\n",
        "    target_text = chunk[1:]\r\n",
        "    return input_text, target_text\r\n",
        "\r\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANyZ76NF83BX",
        "outputId": "9311e7a2-0eb4-4069-ccf1-b247d28e2a47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\r\n",
        "    print(\"Input data:\", repr(''.join(idx2char[input_example.numpy()])))\r\n",
        "    print(\"Target data:\", repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data: 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHk0m5Hz82-d"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSMRhqeRhtdt"
      },
      "source": [
        "Each index of these vectors is processed as a one time step. For the input at time step 0, the model receives the index for \"F\" and tries to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the `RNN` considers the previous step context in addition to the current input character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9UN1aJEcpkY",
        "outputId": "9887c7cd-1d3d-42a9-85d6-e95b766320ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\r\n",
        "    print(\"Step {:4d}\".format(i))\r\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\r\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\r\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-ePYGa_iGBm"
      },
      "source": [
        "## Create training batches\r\n",
        "\r\n",
        "You used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRP8xjFOiFSZ",
        "outputId": "e42ebab2-5f84-4dbb-bf40-fcf79dca63cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "BATCH_SIZE = 64\r\n",
        "BUFFER_SIZE = 10000\r\n",
        "\r\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "dataset"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEG1dKVu1OKb"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuTrDYXaiCG5"
      },
      "source": [
        "vocab_size = len(vocab)\r\n",
        "embedding_dim = 256\r\n",
        "rnn_units = 1024"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCs-LMd7iB5I"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\r\n",
        "    model = Sequential([\r\n",
        "        Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\r\n",
        "        GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\r\n",
        "        Dense(vocab_size)\r\n",
        "    ])\r\n",
        "    return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy90p3YkiBFd",
        "outputId": "f37249dc-9347-47e0-e57f-75fcfa33e22b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\r\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5Ecw3be4heF"
      },
      "source": [
        "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/text_generation_training.png?raw=1\" alt=\"model\" width=500>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA5x57_Z36il",
        "outputId": "60fbbab2-ce07-4d76-f31d-7405f5d8d86b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\r\n",
        "    example_batch_predictions = model(input_example_batch)\r\n",
        "    print(example_batch_predictions.shape, \"# (batch, sequence, vocab)\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch, sequence, vocab)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6pWAqviERb-"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\r\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5fUhfElERY3",
        "outputId": "93afa5a5-6d62-4d5d-abf3-c72f81466cfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([38, 43, 37, 61, 41, 37,  2,  0, 22, 51, 34, 25, 60, 28, 48, 33,  5,\n",
              "       64, 52, 29, 13,  7, 23, 15, 38, 10, 28, 48, 32, 57, 14, 63, 16,  5,\n",
              "       34, 21,  5, 14, 23, 60, 19, 24,  4, 17, 47,  1, 61, 26, 28, 54, 32,\n",
              "        9, 12, 59, 19, 27, 40, 34, 13, 38, 13, 18,  1, 12, 43, 33, 36, 12,\n",
              "       55,  9,  3, 58, 40, 14, 16, 20, 53,  3, 19, 32, 45, 63, 15, 18, 13,\n",
              "       10, 43, 32, 55, 37, 34, 39, 38,  9, 20, 24, 44, 31, 35, 40])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5sdOASwERWM",
        "outputId": "7b658ede-e10d-49e1-94c5-2f6ecdc81e6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\r\n",
        "print()\r\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " \"urage and in judgment\\nThat they'll take no offence at our abuse.\\n\\nKING EDWARD IV:\\nSuppose they take \"\n",
            "\n",
            "Next Char Predictions: \n",
            " \"ZeYwcY!\\nJmVMvPjU'znQA-KCZ:PjTsByD'VI'BKvGL&Ei wNPpT3?uGObVAZAF ?eUX?q3$tbBDHo$GTgyCFA:eTqYVaZ3HLfSWb\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbDYoX3xERUz",
        "outputId": "c047042f-fd08-4650-df85-6eeafbcd09ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def loss(labels, logits):\r\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\r\n",
        "\r\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\r\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\r\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.171814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YnRlnArERQM"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwQ0F9-PERMy",
        "outputId": "0f652c8d-858c-4443-acc3-b6c2011bc50a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "EPOCHS = 10\r\n",
        "\r\n",
        "# Directory where the checkpoints will be saved\r\n",
        "checkpoint_dir = './training_checkpoints'\r\n",
        "# Name of the checkpoint files\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n",
        "\r\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_prefix,\r\n",
        "    save_weights_only=True)\r\n",
        "\r\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 10s 44ms/step - loss: 3.3669\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 9s 44ms/step - loss: 2.0672\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 9s 44ms/step - loss: 1.7597\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 9s 44ms/step - loss: 1.5825\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 9s 44ms/step - loss: 1.4795\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 1.4102\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 1.3584\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 9s 44ms/step - loss: 1.3192\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 1.2803\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 9s 43ms/step - loss: 1.2453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnOGHz0qG8Hj",
        "outputId": "45daa3ef-667c-417a-9ba1-b2f04b5b1de9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\r\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\r\n",
        "\r\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\r\n",
        "\r\n",
        "model.build(tf.TensorShape([1, None]))\r\n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CEsUJ0VGErR"
      },
      "source": [
        "def generate_text(model, start_string):\r\n",
        "    # Evaluation step (generating text using the learned model)\r\n",
        "\r\n",
        "    # Number of characters to generate\r\n",
        "    num_generate = 1000\r\n",
        "\r\n",
        "    # Converting our start string to numbers (vectorizing)\r\n",
        "    input_eval = [char2idx[s] for s in start_string]\r\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\r\n",
        "\r\n",
        "    # Empty string to store our results\r\n",
        "    text_generated = []\r\n",
        "\r\n",
        "    # Low temperature results in more predictable text.\r\n",
        "    # Higher temperature results in more surprising text.\r\n",
        "    # Experiment to find the best setting.\r\n",
        "    temperature = 1.0\r\n",
        "\r\n",
        "    # Here batch size == 1\r\n",
        "    model.reset_states()\r\n",
        "    for i in range(num_generate):\r\n",
        "        predictions = model(input_eval)\r\n",
        "        # remove the batch dimension\r\n",
        "        predictions = tf.squeeze(predictions, 0)\r\n",
        "\r\n",
        "        # using a categorical distribution to predict the character returned by the model\r\n",
        "        predictions = predictions / temperature\r\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\r\n",
        "\r\n",
        "        # Pass the predicted character as the next input to the model\r\n",
        "        # along with the previous hidden state\r\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\r\n",
        "\r\n",
        "        text_generated.append(idx2char[predicted_id])\r\n",
        "\r\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH6Lf5sGGb9d",
        "outputId": "1ad36a6f-3d31-4e0f-df2e-e3b73d12bbdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: I may be gone,\n",
            "And vicely as thou art greate to the malice:--hear me not to be\n",
            "then, alack! it were a\n",
            "joidne summer respecteden to the king;\n",
            "Whisher, rush ambitious mine.\n",
            "\n",
            "SICINIUS:\n",
            "Come hot were, I know, I hope\n",
            "My finger amongs our face and brought,\n",
            "Or never he scapsalious lies must believe you thanks.\n",
            "I heartes me o'er.\n",
            "\n",
            "PETRUCHIO:\n",
            "Sir, Wracerom'd and Just, Greme me, wife,\n",
            "And let me see thee foolish body:\n",
            "Reporting it, and his oatthal soil'd hour her see the\n",
            "dream, or no must be else.\n",
            "\n",
            "GLOUCESTER:\n",
            "Gramering them acceament on thyself:\n",
            "My wife: but hither with them, born up forbids;\n",
            "And therefore, ho ut fair and mine,\n",
            "So playngeath to me the dincy of those knighthomas\n",
            "Are, farito ye with being a bragevou most,\n",
            "I hold a banich loves!\n",
            "They shall not hear a women\n",
            "Are made ground upon a father, My noes,\n",
            "Possess I may mount him.\n",
            "\n",
            "FLIAR LICHARD:\n",
            "I was thing shall I no ut Pith and your feer'd for retres, but stare up.\n",
            "\n",
            "RICHMOND:\n",
            "If the people.\n",
            "\n",
            "MONTAGUE:\n",
            "And make you so war?\n",
            "Ferch, as I saw \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}