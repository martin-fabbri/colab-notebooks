{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c4_w2_assignment_transformer_summarizer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPEck1XV6vYbJehOX8SDrLI"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9XCeol40xzf1"},"source":["\r\n","# Assignment 2: Transformer Summarizer\r\n","\r\n","Welcome to the second assignment of course 4. In this assignment you will explore summarization using the transformer model. Yes, you will implement the transformer decoder from scratch, but we will slowly walk you through it. There are many hints in this notebook so feel free to use them as needed. \r\n","\r\n","<img src = \"transformerNews.png\">"]},{"cell_type":"markdown","metadata":{"id":"GUax1wvmDc7j"},"source":["## Outline\r\n","\r\n","- [Introduction](#0)\r\n","- [Part 1: Importing the dataset](#1)\r\n","    - [1.1 Encode & Decode helper functions](#1.1)\r\n","    - [1.2 Defining parameters](#1.2)\r\n","    - [1.3 Exploring the data](#1.3)\r\n","- [Part 2: Summarization with transformer](#2)\r\n","    - [2.1 Dot product attention](#2.1)\r\n","        - [Exercise 01](#ex01)\r\n","    - [2.2 Causal Attention](#2.2)\r\n","        - [Exercise 02](#ex02)\r\n","    - [2.3 Transformer decoder block](#2.3)\r\n","        - [Exercise 03](#ex03)\r\n","    - [2.4 Transformer Language model](#2.4)\r\n","        - [Exercise 04](#ex04)\r\n","- [Part 3: Training](#3)\r\n","    - [3.1 Training the model](#3.1)\r\n","        - [Exercise 05](#ex05)\r\n","- [Part 4: Evaluation](#4)\r\n","    - [4.1 Loading in a trained model](#4.1)\r\n","- [Part 5: Testing with your own input](#5) \r\n","    - [Exercise 6](#ex06)\r\n","    - [5.1 Greedy decoding](#5.1)\r\n","        - [Exercise 07](#ex07)"]},{"cell_type":"markdown","metadata":{"id":"PbZs68c3Dkt8"},"source":["<a name='0'></a>\r\n","### Introduction\r\n","\r\n","Summarization is an important task in natural language processing and could be useful for a consumer enterprise. For example, bots can be used to scrape articles, summarize them, and then you can use sentiment analysis to identify the sentiment about certain stocks. Anyways who wants to read an article or a long email today, when you can build a transformer to summarize text for you. Let's get started, by completing this assignment you will learn to:  \r\n","\r\n","- Use built-in functions to preprocess your data\r\n","- Implement DotProductAttention\r\n","- Implement Causal Attention\r\n","- Understand how attention works\r\n","- Build the transformer model\r\n","- Evaluate your model\r\n","- Summarize an article\r\n","\r\n","As you can tell, this model is slightly different than the ones you have already implemented. This is heavily based on attention and does not rely on sequences, which allows for parallel computing. "]},{"cell_type":"code","metadata":{"id":"hEiwSU8wEUqe","executionInfo":{"status":"ok","timestamp":1612918318714,"user_tz":480,"elapsed":4159,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["%%capture\r\n","!pip install trax"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-Bl4W37xJcW","executionInfo":{"status":"ok","timestamp":1612918355745,"user_tz":480,"elapsed":41179,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["import sys\r\n","import os\r\n","\r\n","import numpy as np\r\n","\r\n","import textwrap\r\n","wrapper = textwrap.TextWrapper(width=70)\r\n","\r\n","import trax\r\n","from trax import layers as tl\r\n","from trax.fastmath import numpy as jnp\r\n","\r\n","# to print the entire np array\r\n","np.set_printoptions(threshold=sys.maxsize)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5RULauOHE6lR"},"source":["<a name='1'></a>\r\n","## Part 1: Importing the dataset"]},{"cell_type":"markdown","metadata":{"id":"KdGRNRh0FL6m"},"source":["Trax makes it easy to work with Tensorflow's datasets:"]},{"cell_type":"code","metadata":{"id":"nE-kXpBJE7t4","executionInfo":{"status":"ok","timestamp":1612918355902,"user_tz":480,"elapsed":41330,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# This will download the dataset if no data_dir is specified.\r\n","# Downloading and processing can take bit of time,\r\n","# so we have the data already in 'data/' for you\r\n","\r\n","# Importing CNN/DailyMail articles dataset\r\n","train_stream_fn = trax.data.TFDS(\r\n","    \"cnn_dailymail\",\r\n","    data_dir=\"data/\",\r\n","    keys=(\"article\", \"highlights\"),\r\n","    train=True,\r\n",")\r\n","eval_stream_fn = trax.data.TFDS(\r\n","    \"cnn_dailymail\",\r\n","    data_dir=\"data/\",\r\n","    keys=(\"article\", \"highlights\"),\r\n","    train=False,\r\n",")"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xZKyLaBkITvT"},"source":["<a name='1.1'></a>\r\n","## 1.1 Tokenize & Detokenize helper functions\r\n","\r\n","Just like in the previous assignment, the cell above loads in the encoder for you. Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your [Trax](https://github.com/google/trax) models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following: \r\n","\r\n","- <span style='color:blue'> word2Ind: </span> a dictionary mapping the word to its index.\r\n","- <span style='color:blue'> ind2Word:</span> a dictionary mapping the index to its word.\r\n","- <span style='color:blue'> word2Count:</span> a dictionary mapping the word to the number of times it appears. \r\n","- <span style='color:blue'> num_words:</span> total number of words that have appeared. \r\n","\r\n","Since you have already implemented these in previous assignments of the specialization, we will provide you with helper functions that will do this for you. Run the cell below to get the following functions:\r\n","\r\n","- <span style='color:blue'> tokenize: </span> converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords.\r\n","- <span style='color:blue'> detokenize: </span> converts a token list to its corresponding sentence (i.e. string)."]},{"cell_type":"code","metadata":{"id":"70J7LoSBE7kc","executionInfo":{"status":"ok","timestamp":1612918355904,"user_tz":480,"elapsed":41323,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["def tokenize(input_str, EOS=1):\r\n","    \"\"\"Input str to features dict, ready for inference\"\"\"\r\n","    # Use the trax.data.tokenize method. It takes streams and returns streams,\r\n","    # we get around it by making a 1-element stream with `iter`.\r\n","    inputs = next(\r\n","        trax.data.tokenize(\r\n","            iter([input_str]),\r\n","            vocab_dir=\"vocab_dir/\",\r\n","            vocab_file=\"summarize32k.subword.subwords\",\r\n","        )\r\n","    )\r\n","    return list(inputs) + [EOS]\r\n","\r\n","\r\n","def detokenize(integers):\r\n","    \"\"\"List of ints to str\"\"\"\r\n","    s = trax.data.detokenize(\r\n","        integers,\r\n","        vocab_dir=\"vocab_dir/\",\r\n","        vocab_file=\"summarize32k.subword.subwords\",\r\n","    )\r\n","    return wrapper.fill(s)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-L4yprQ04PqF"},"source":["<a name='1.2'></a>\r\n","\r\n","## 1.2 Preprocessing for Language Models: Concatenate It!\r\n","\r\n","This week you will use a language model -- Transformer Decoder -- to solve\r\n","an input-output problem. As you know, language models only predict the next\r\n","word, they have no notion of inputs. To create a single input suitable for\r\n","a language model, we concatenate inputs with targets putting a separator\r\n","in between. We also need to create a mask -- with 0s at inputs and 1s at targets -- so that the model is not penalized for mis-predicting the article and only focuses on the summary. See the preprocess function below for how this is done."]},{"cell_type":"code","metadata":{"id":"Ux3XgKJKE7JO","executionInfo":{"status":"ok","timestamp":1612918355905,"user_tz":480,"elapsed":41317,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["SEP = 0 # padding of separator token\r\n","EOS = 1 # end of sentence token\r\n","\r\n","# concatenate tokenized inputs and targets using 0 as separator\r\n","def preprocess(stream):\r\n","    for (article, summary) in stream:\r\n","        article, summary = list(article), list(summary)\r\n","        joint = np.array(article + [EOS, SEP] + summary + [EOS])\r\n","        # accounting for EOS and SEP\r\n","        mask = [0] * (len(article) + 2) + [1] * (len(summary) + 1)\r\n","        yield joint, joint, np.array(mask)\r\n","\r\n","# you can combine a few data preprocessing steps into a pipeline like this\r\n","input_pipeline = trax.data.Serial(\r\n","    # tokenizes\r\n","    trax.data.Tokenize(\r\n","        vocab_dir=\"vocab_dir/\", vocab_file=\"summarize32k.subword.subwords\"\r\n","    ),\r\n","    # uses function defined above\r\n","    preprocess,\r\n","    # filters out examples longet than 2048\r\n","    trax.data.FilterByLength(2048)\r\n",")\r\n","\r\n","# apply preprocessing to the data streams\r\n","train_stream = input_pipeline(train_stream_fn())\r\n","eval_stream = input_pipeline(eval_stream_fn())"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"gbueNXcBYers","executionInfo":{"status":"ok","timestamp":1612918548861,"user_tz":480,"elapsed":288,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["train_input, train_target, train_mask = next(train_stream)\r\n","# target and input datasources should be the same\r\n","assert sum(train_input - train_target) == 0"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nU9gfQTgbZUJ","executionInfo":{"status":"ok","timestamp":1612918559540,"user_tz":480,"elapsed":319,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"b456087c-29f8-49d8-aa98-2931808342ad"},"source":["train_mask"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"aDkhRPgxcBG4"},"source":[""],"execution_count":null,"outputs":[]}]}