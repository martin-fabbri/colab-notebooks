{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "c1-w3-lecture-02-word-embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMI8y6pq7dQ4k8D09kVfnXC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/nlp/c1_w3_lecture_02_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2_4RbnFZ4Xb"
      },
      "source": [
        "# Manipulating Word Embeddings\n",
        "\n",
        "This notebook explores how to apply linear algebra to find analogies between words manually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEt_bHtnb33e",
        "outputId": "ff970a30-111b-4498-9651-2edc0fd51588",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget 'https://github.com/martin-fabbri/colab-notebooks/raw/master/data/nlp/word_embeddings_subset.p'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-03 14:08:20--  https://github.com/martin-fabbri/colab-notebooks/raw/master/data/nlp/word_embeddings_subset.p\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/martin-fabbri/colab-notebooks/master/data/nlp/word_embeddings_subset.p [following]\n",
            "--2020-11-03 14:08:20--  https://raw.githubusercontent.com/martin-fabbri/colab-notebooks/master/data/nlp/word_embeddings_subset.p\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 309156 (302K) [application/octet-stream]\n",
            "Saving to: ‘word_embeddings_subset.p’\n",
            "\n",
            "word_embeddings_sub 100%[===================>] 301.91K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-11-03 14:08:20 (5.83 MB/s) - ‘word_embeddings_subset.p’ saved [309156/309156]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFE2MxVaZ3tM",
        "outputId": "0695d6b5-ea9c-42a8-bfad-30000eb55eb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle \n",
        "\n",
        "word_embeddings_url = 'word_embeddings_subset.p'\n",
        "word_embeddings = pickle.load(open(word_embeddings_url, 'rb'))\n",
        "len(word_embeddings)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "243"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA0_ZL4gcRSd"
      },
      "source": [
        "Now that the model is loaded, we can take a look at the word representations. First, note that the `word embeddings` is a dictionary. Each word is the key to the entry, and the value is its corresponding vector presentation. Remember that square brackers allow access to an entry if the key exists. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlW0dmJlbtGj",
        "outputId": "9f950fe2-0d1e-47d8-c61a-f293ee6d5167",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "country_vector = word_embeddings['country']\n",
        "print(f'vector type: {type(country_vector)}')\n",
        "print(f'vector shape: {country_vector.shape}')\n",
        "country_vector[:3]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vector type: <class 'numpy.ndarray'>\n",
            "vector shape: (300,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.08007812,  0.13378906,  0.14355469], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLO1i_5QdyaH"
      },
      "source": [
        "It is important to note that we store each vector as a np array. It allows us to use the linear algebra operations on it.\n",
        "\n",
        "The vectors have a a size of 300, while the vocabulary size of Google News is around 3 million words!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UwyIAa6dWRD"
      },
      "source": [
        "def vec(w):\n",
        "  return word_embeddings[w]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3ezVlVGeqdC"
      },
      "source": [
        "## Operating on word embeddings\n",
        "\n",
        "Remember that understanding that data is one of the most critical steps in Data Science. Word embeddings are the result of machine learning processes and will be part of the input for further processes. These word embedding needs to be validated or at least understood because the performance of the derived model will strongly depend on its quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00qreHAPepou"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cEAfR21epuw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwjBH9GVep1c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaYMmPYyep7h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWll_-mUep-i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mO_a0hKep4e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBVQoJL-epyS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfYX8HJBepr7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlsI6yCCepl9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}