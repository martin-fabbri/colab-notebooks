{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c3_w2_jax_numpy_calculating_perplexity.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMzljiyEzd/g75rUpw4biQ0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VUQkDZsVx8Je"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\r\n","  <td>\r\n","    <a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/nlp/c3_w2_jax_numpy_calculating_perplexity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>    \r\n","  </td>\r\n","  <td>\r\n","    <a href=\"https://github.com/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/nlp/c3_w2_jax_numpy_calculating_perplexity.ipynb\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/martin-fabbri/colab-notebooks/master/assets/github.svg\" alt=\"View On Github\"/></a>  </td>\r\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"XAEJY8CcyL0I"},"source":["# Working with JAX numpy and calculating perplexity: Ungraded Lecture Notebook"]},{"cell_type":"markdown","metadata":{"id":"hLwEOalhyStx"},"source":["Normally you would import `numpy` and rename it as `np`. \r\n","\r\n","However in this week's assignment you will notice that this convention has been changed. \r\n","\r\n","Now standard `numpy` is not renamed and `trax.fastmath.numpy` is renamed as `np`. \r\n","\r\n","The rationale behind this change is that you will be using Trax's numpy (which is compatible with JAX) far more often. Trax's numpy supports most of the same functions as the regular numpy so the change won't be noticeable in most cases."]},{"cell_type":"code","metadata":{"id":"0YLlodxEyrVx","executionInfo":{"status":"ok","timestamp":1611700993650,"user_tz":480,"elapsed":3364,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["%%capture\r\n","!pip install trax==1.3.1"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fm7YLmYlyUrI","executionInfo":{"status":"ok","timestamp":1611701284575,"user_tz":480,"elapsed":330,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["%%capture\r\n","import numpy\r\n","import trax\r\n","import trax.fastmath.numpy as jnp\r\n","\r\n","from trax import layers as tl\r\n","\r\n","trax.supervised.trainer_lib.init_random_number_generators(32)\r\n","numpy.random.seed(32)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xQgeVNMNyUoX","executionInfo":{"status":"ok","timestamp":1611700997764,"user_tz":480,"elapsed":7456,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"a947868f-575a-4317-8b4d-564130d61eb4"},"source":["!pip list | grep 'trax\\|numpy\\|jax'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["jax                           0.2.7                \n","jaxlib                        0.1.57+cuda101       \n","numpy                         1.19.5               \n","trax                          1.3.1                \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ONeHIIkF2M1R"},"source":["One important change to take into consideration is that the types of the resulting objects will be different depending on the version of numpy. With regular numpy you get `numpy.ndarray` but with Trax's numpy you will get `jax.interpreters.xla.DeviceArray`. These two types map to each other. So if you find some error logs mentioning DeviceArray type, don't worry about it, treat it like you would treat an ndarray and march ahead.\r\n","\r\n","You can get a randomized numpy array by using the `numpy.random.random()` function.\r\n","\r\n","This is one of the functionalities that Trax's numpy does not currently support in the same way as the regular numpy. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FPDLo9Y_yUlV","executionInfo":{"status":"ok","timestamp":1611700997765,"user_tz":480,"elapsed":7449,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"fbf7897c-c2fd-4ffc-849d-4c0b575cfc5e"},"source":["numpy_array = numpy.random.random((5, 10))\r\n","print(f\"The regular numpy array looks like this:\\n\\n {numpy_array}\\n\")\r\n","print(f\"It is of type: {type(numpy_array)}\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["The regular numpy array looks like this:\n","\n"," [[0.85888927 0.37271115 0.55512878 0.95565655 0.7366696  0.81620514\n","  0.10108656 0.92848807 0.60910917 0.59655344]\n"," [0.09178413 0.34518624 0.66275252 0.44171349 0.55148779 0.70371249\n","  0.58940123 0.04993276 0.56179184 0.76635847]\n"," [0.91090833 0.09290995 0.90252139 0.46096041 0.45201847 0.99942549\n","  0.16242374 0.70937058 0.16062408 0.81077677]\n"," [0.03514717 0.53488673 0.16650012 0.30841038 0.04506241 0.23857613\n","  0.67483453 0.78238275 0.69520163 0.32895445]\n"," [0.49403187 0.52412136 0.29854125 0.46310814 0.98478429 0.50113492\n","  0.39807245 0.72790532 0.86333097 0.02616954]]\n","\n","It is of type: <class 'numpy.ndarray'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ICqpa_-r25__"},"source":["You can easily cast regular numpy arrays or lists into trax numpy arrays using the trax.fastmath.numpy.array() function:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NP1Znpk_xn5O","executionInfo":{"status":"ok","timestamp":1611701014070,"user_tz":480,"elapsed":385,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"e0787816-5ae9-42d9-b57f-43b8aa8a45cc"},"source":["trax_numpy_array = jnp.array(numpy_array)\r\n","print(f\"The trax numpy array looks like this:\\n\\n {trax_numpy_array}\\n\")\r\n","print(f\"It is of type: {type(trax_numpy_array)}\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["The trax numpy array looks like this:\n","\n"," [[0.8588893  0.37271115 0.55512875 0.9556565  0.7366696  0.81620514\n","  0.10108656 0.9284881  0.60910916 0.59655344]\n"," [0.09178413 0.34518623 0.6627525  0.44171348 0.5514878  0.70371246\n","  0.58940125 0.04993276 0.56179184 0.7663585 ]\n"," [0.91090834 0.09290995 0.9025214  0.46096042 0.45201847 0.9994255\n","  0.16242374 0.7093706  0.16062407 0.81077677]\n"," [0.03514718 0.5348867  0.16650012 0.30841038 0.04506241 0.23857613\n","  0.67483455 0.7823827  0.69520164 0.32895446]\n"," [0.49403188 0.52412134 0.29854125 0.46310815 0.9847843  0.50113493\n","  0.39807245 0.72790533 0.86333096 0.02616954]]\n","\n","It is of type: <class 'jax.interpreters.xla._DeviceArray'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pP2fIaLp3mgO"},"source":["Hope you now understand the differences (and similarities) between these two versions and numpy. **Great!**\r\n","\r\n","The previous section was a quick look at Trax's numpy. However this notebook also aims to teach you how you can calculate the perplexity of a trained model."]},{"cell_type":"markdown","metadata":{"id":"BDVR59Yc3uX8"},"source":["## Calculating Perplexity"]},{"cell_type":"markdown","metadata":{"id":"IDMRK1JK3wXB"},"source":["The perplexity is a metric that measures how well a probability model predicts a sample and it is commonly used to evaluate language models. It is defined as: \r\n","\r\n","$$P(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}$$\r\n","\r\n","As an implementation hack, you would usually take the log of that formula (to enable us to use the log probabilities we get as output of our `RNN`, convert exponents to products, and products into sums which makes computations less complicated and computationally more efficient). You should also take care of the padding, since you do not want to include the padding when calculating the perplexity (because we do not want to have a perplexity measure artificially good). The algebra behind this process is explained next:\r\n","\r\n","\r\n","$$log P(W) = {log\\big(\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)}$$\r\n","\r\n","$$ = {log\\big({\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)^{\\frac{1}{N}}}$$ \r\n","\r\n","$$ = {log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)^{-\\frac{1}{N}}} $$\r\n","$$ = -\\frac{1}{N}{log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)} $$\r\n","$$ = -\\frac{1}{N}{\\big({\\sum_{i=1}^{N}{logP(w_i| w_1,...,w_{n-1})}}\\big)} $$"]},{"cell_type":"markdown","metadata":{"id":"mcCMKAPR4NXx"},"source":["You will be working with a real example from this week's assignment. The example is made up of:\r\n","   - `predictions` : batch of tensors corresponding to lines of text predicted by the model.\r\n","   - `targets` : batch of actual tensors corresponding to lines of text."]},{"cell_type":"code","metadata":{"id":"mBN43MC_3cxz","executionInfo":{"status":"ok","timestamp":1611701272798,"user_tz":480,"elapsed":330,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["predictions = n\r\n","\r\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"gIRdIgjW4O0s"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GSFKiECV4Oov"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TaIwWBRa4Ofa"},"source":[""],"execution_count":null,"outputs":[]}]}