{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c4_w3_01_sentence_piece_and_bpe.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN1tr6O9o+C8Ts6AovyOJ8e"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ud217veYbzJs"},"source":["# SentencePiece and Byte Pair Encoding "]},{"cell_type":"markdown","metadata":{"id":"oNI82N5_b3eV"},"source":["## Introduction to Tokenization"]},{"cell_type":"markdown","metadata":{"id":"3M7QB2dlb4-q"},"source":["In order to process text in neural network models, it is first required to **encode** text as numbers with ids (such as the embedding vectors we've been using in the previous assignments), since the tensor operations act on numbers. Finally, if the output of the network are words, it is required to **decode** the predicted tokens ids back to text.\r\n","\r\n","To encode text, the first decision that has to be made is to what level of granularity are we going to consider the text? Because ultimately, from these **tokens**, features are going to be created about them. Many different experiments have been carried out using *words*, *morphological units*, *phonemic units*, *characters*. For example, \r\n","\r\n","- Tokens are tricky. (raw text)\r\n","- Tokens are tricky . ([words](https://arxiv.org/pdf/1301.3781))\r\n","- Token s _ are _ trick _ y . ([morphemes](https://arxiv.org/pdf/1907.02423.pdf))\r\n","- t oʊ k ə n z _ ɑː _ ˈt r ɪ k i. ([phonemes](https://www.aclweb.org/anthology/W18-5812.pdf), for STT)\r\n","- T o k e n s _ a r e _ t r i c k y . ([character](https://www.aclweb.org/anthology/C18-1139/))"]},{"cell_type":"markdown","metadata":{"id":"ogUsfu5_1WPP"},"source":["But how to identify these units, such as words, are largely determined by the language they come from. For example, in many European languages a space is used to separate words, while in some Asian languages there are no spaces between words. Compare English and Mandarin.\r\n","\r\n","- Tokens are tricky. (original sentence)\r\n","- 令牌很棘手 (Mandarin)\r\n","- Lìng pái hěn jí shǒu (pinyin)\r\n","- 令牌 很 棘手 (Mandarin with spaces)\r\n","\r\n","\r\n","So, the ability to **tokenize**, i.e. split text into meaningful fundamental units is not always straight-forward.\r\n","\r\n","Also, there are practical issues of how large our *vocabulary* of words, `vocab_size`, should be, considering memory limitations vs. coverage. A compromise between the finest-grained models employing characters which can be memory and more computationally efficient *subword* units such as [n-grams](https://arxiv.org/pdf/1712.09405) or larger units need to be made.\r\n","\r\n","In [SentencePiece](https://www.aclweb.org/anthology/D18-2012.pdf) unicode characters are grouped together using either a [unigram language model](https://www.aclweb.org/anthology/P18-1007.pdf) (used in this week's assignment) or [BPE](https://arxiv.org/pdf/1508.07909.pdf), **byte-pair encoding**. We will discuss BPE, since BERT and many of its variant uses a modified version of BPE and its pseudocode is easy to implement and understand... hopefully!"]},{"cell_type":"markdown","metadata":{"id":"LG0qsTcpETzc"},"source":["## SentencePiece Preprocessing\r\n","### NFKC Normalization"]},{"cell_type":"markdown","metadata":{"id":"h89Wu1mFEVhC"},"source":["Unsurprisingly, even using unicode to initially tokenize text can be ambiguous, e.g., "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3z-p_Ohnfsac","executionInfo":{"status":"ok","timestamp":1613104094851,"user_tz":480,"elapsed":312,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"fc689a14-6ee6-4f34-b1f6-b6259a18bf2d"},"source":["eaccent = '\\u00E9'\r\n","e_accent = '\\u0065\\u0301'\r\n","print(f'{eaccent} = {e_accent} : {eaccent == e_accent}')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["é = é : False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wgKVeI9agHh_"},"source":["SentencePiece uses the Unicode standard Normalization form, [NFKC](https://en.wikipedia.org/wiki/Unicode_equivalence), so this isn't an issue. Looking at our example from above again with normalization:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"325vKt35fsUy","executionInfo":{"status":"ok","timestamp":1613105713218,"user_tz":480,"elapsed":333,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"5fe313c5-a706-46e3-c71c-e110e71f265f"},"source":["from unicodedata import normalize\r\n","\r\n","norm_eaccent = normalize(\"NFKC\", \"\\u00E9\")\r\n","norm_e_accent = normalize(\"NFKC\", \"\\u0065\\u0301\")\r\n","print(f\"{norm_eaccent} = {norm_e_accent} : {norm_eaccent == norm_e_accent}\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["é = é : True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7aTRig62mB9M"},"source":["Normalization has actually changed the unicode code point (unicode unique id) for one of these two characters."]},{"cell_type":"code","metadata":{"id":"kBcHXDYkfsRZ","executionInfo":{"status":"ok","timestamp":1613106008144,"user_tz":480,"elapsed":356,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["def get_hex_encoding(s):\r\n","    return \" \".join(hex(ord(c)) for c in s)\r\n","\r\n","def print_string_and_encoding(s):\r\n","    print(f\"{s} : {get_hex_encoding(s)}\")"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p604qDoFnGgI","executionInfo":{"status":"ok","timestamp":1613106048678,"user_tz":480,"elapsed":287,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"612db73b-cbbb-410b-fc00-0c57e15a0773"},"source":["for s in [eaccent, e_accent, norm_eaccent, norm_e_accent]:\r\n","    print_string_and_encoding(s)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["é : 0xe9\n","é : 0x65 0x301\n","é : 0xe9\n","é : 0xe9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qym2LgluprpD"},"source":["This normalization has other side effects which may be considered useful such as converting curly quotes &ldquo; to \" their ASCII equivalent. (Although we *now* lose directionality of the quote...)"]},{"cell_type":"markdown","metadata":{"id":"w5Gr5NWxpyVt"},"source":["### Lossless Tokenization<sup>*</sup>"]},{"cell_type":"markdown","metadata":{"id":"yif2Sip3p0Jn"},"source":["SentencePiece also ensures that when you tokenize your data and detokenize your data the original position of white space is preserved. (However, tabs and newlines are converted to spaces, please try this experiment yourself later below.)"]},{"cell_type":"markdown","metadata":{"id":"icZ6m35Ep_aH"},"source":["To ensure this **lossless tokenization** it replaces white space with _ (U+2581). So that a simple join of the replace underscores with spaces can restore the white space, even if there are consecutives symbols. But remember first to normalize and then replace spaces with _ (U+2581). As the following example shows."]},{"cell_type":"code","metadata":{"id":"Q_hrJAZQpzFl","executionInfo":{"status":"ok","timestamp":1613106896458,"user_tz":480,"elapsed":314,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["s = \"Tokenization is hard.\"\r\n","s_ = s.replace(\" \", \"\\u2581\")\r\n","s_n = normalize(\"NFKC\", \"Tokenization is hard.\")"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLlAF1n4ptjr","executionInfo":{"status":"ok","timestamp":1613106897437,"user_tz":480,"elapsed":294,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"88aad6be-4787-4a06-896a-036e3dc276c1"},"source":["print(get_hex_encoding(s))\r\n","print(get_hex_encoding(s_))\r\n","print(get_hex_encoding(s_n))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n","0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x2581 0x69 0x73 0x2581 0x68 0x61 0x72 0x64 0x2e\n","0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PPSVQv83qr7d"},"source":["## BPE Algorithm\r\n","\r\n","Now that we have discussed the preprocessing that SentencePiece performs we will go get our data, preprocess, and apply the BPE algorithm. We will show how this reproduces the tokenization produced by training SentencePiece on our example dataset (from this week's assignment).\r\n","\r\n","### Preparing our Data\r\n","First, we get our Squad data and process as above."]},{"cell_type":"code","metadata":{"id":"8MeIHpRDqsyZ","executionInfo":{"status":"ok","timestamp":1613106972005,"user_tz":480,"elapsed":266,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["import ast"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"Foqm2mWoqtP_","executionInfo":{"status":"ok","timestamp":1613107249964,"user_tz":480,"elapsed":287,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["def convert_json_examples_to_text(filepath):\r\n","    example_jsons = list(map(ast.literal_eval, open(filepath)))\r\n","    texts = [example_jsons[\"text\"].decode(\"utf-8\") for example_json in example_jsons]\r\n","    text = \"\\n\\n\".join(texts)\r\n","    text = normalize(\"NFKC\", text)\r\n","    with open(\"example.txt\", \"w\") as fw:\r\n","        fw.write(text)\r\n","    return text"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"85o4B7cxr3JX","executionInfo":{"status":"error","timestamp":1613107288019,"user_tz":480,"elapsed":295,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"4d03cdb2-eb58-4116-e5dc-0e8feb64d3a1"},"source":["text = convert_json_examples_to_text(\"data.txt\")\r\n","print(text[:900])"],"execution_count":19,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-d1c319f01929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_json_examples_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-19e62903140e>\u001b[0m in \u001b[0;36mconvert_json_examples_to_text\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_json_examples_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mexample_jsons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexample_jsons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample_json\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexample_jsons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NFKC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.txt'"]}]},{"cell_type":"code","metadata":{"id":"Z9_kYmGvbS8a","executionInfo":{"status":"ok","timestamp":1613103765720,"user_tz":480,"elapsed":3671,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["%%capture\r\n","!pip install whatlies[all]"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":436},"id":"sI0foK1ib0VW","executionInfo":{"status":"error","timestamp":1613103872379,"user_tz":480,"elapsed":328,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"f4b7e0f6-38c5-49a0-e5c2-af3e604f98be"},"source":["from whatlies.language import BytePairLang"],"execution_count":5,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-6a54dc1bf35a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwhatlies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBytePairLang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/whatlies/language/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sense2vec_lang\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSense2VecLanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mSense2VecLanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sense2VecLanguage\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sense2vec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/whatlies/language/_sense2vec_lang.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msense2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSense2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSense2VecComponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwhatlies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwhatlies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddingset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbeddingSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sense2vec/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msense2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSense2Vec\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcomponent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSense2VecComponent\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregistry\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sense2vec/sense2vec.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msrsly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sense2vec/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfilter_spans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_array_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcatalogue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'get_array_module'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"fpfTd8MFbZpk"},"source":[""],"execution_count":null,"outputs":[]}]}