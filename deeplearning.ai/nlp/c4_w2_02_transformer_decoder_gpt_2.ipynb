{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c4_w2_02_transformer_decoder_gpt_2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPbG5ZdfTwcS2ar3HsD31lA"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"14iQjhAJXbdg"},"source":["# The Transformer Decoder: Ungraded Lab Notebook\r\n","\r\n","In this notebook, you'll explore the transformer decoder and how to implement it with Trax. \r\n","\r\n","## Background\r\n","\r\n","In the last lecture notebook, you saw how to translate the mathematics of attention into NumPy code. Here, you'll see how multi-head causal attention fits into a GPT-2 transformer decoder, and how to build one with Trax layers. In the assignment notebook, you'll implement causal attention from scratch, but here, you'll exploit the handy-dandy `tl.CausalAttention()` layer.\r\n","\r\n","The schematic below illustrates the components and flow of a transformer decoder. Note that while the algorithm diagram flows from the bottom to the top, the overview and subsequent Trax layer codes are top-down.\r\n","\r\n","<img src=\"transformer_decoder_lnb_figs/C4_W2_L6_transformer-decoder_S01_transformer-decoder.png\" width=\"1000\"/>"]},{"cell_type":"code","metadata":{"id":"1bKCbA1kXWOt"},"source":[""],"execution_count":null,"outputs":[]}]}