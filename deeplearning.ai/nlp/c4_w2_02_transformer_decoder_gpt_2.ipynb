{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c4_w2_02_transformer_decoder_gpt_2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPGf4rJ5WmHSl7SScVixL28"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"14iQjhAJXbdg"},"source":["# The Transformer Decoder: Ungraded Lab Notebook\r\n","\r\n","In this notebook, you'll explore the transformer decoder and how to implement it with Trax. \r\n","\r\n","## Background\r\n","\r\n","In the last lecture notebook, you saw how to translate the mathematics of attention into NumPy code. Here, you'll see how multi-head causal attention fits into a GPT-2 transformer decoder, and how to build one with Trax layers. In the assignment notebook, you'll implement causal attention from scratch, but here, you'll exploit the handy-dandy `tl.CausalAttention()` layer.\r\n","\r\n","The schematic below illustrates the components and flow of a transformer decoder. Note that while the algorithm diagram flows from the bottom to the top, the overview and subsequent Trax layer codes are top-down.\r\n","\r\n","<img src=\"https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/images/c4_w2_l6_transformer_decoder_s01_transformer_decoder.png\" width=\"700px\"/>"]},{"cell_type":"markdown","metadata":{"id":"W_EeJBZ7akZq"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"P2ssG3n8ap5Q","executionInfo":{"status":"ok","timestamp":1612851121466,"user_tz":480,"elapsed":14910,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["%%capture\r\n","!pip install trax"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"1bKCbA1kXWOt","executionInfo":{"status":"ok","timestamp":1612851171501,"user_tz":480,"elapsed":45625,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["import sys\r\n","import os\r\n","\r\n","import time\r\n","import numpy as np\r\n","import gin\r\n","\r\n","import textwrap\r\n","wrapper = textwrap.TextWrapper(width=70)\r\n","\r\n","import trax\r\n","from trax import layers as tl\r\n","from trax.fastmath import numpy as jnp\r\n","\r\n","# to print the entire np array\r\n","np.set_printoptions(threshold=sys.maxsize)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jT2krkbybIex"},"source":["## Sentence gets embedded, add positional encoding\r\n","Embed the words, then create vectors representing each word's position in each sentence $\\in \\{ 0, 1, 2, \\ldots , K\\}$ = `range(max_len)`, where `max_len` = $K+1$)"]},{"cell_type":"code","metadata":{"id":"EDCPBEIGa-dW","executionInfo":{"status":"ok","timestamp":1612855920216,"user_tz":480,"elapsed":279,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["def positional_encoder(vocab_size, d_model, dropout, max_len, mode):\r\n","    \"\"\"Returns a list of layers that: \r\n","    1. takes a block of text as input, \r\n","    2. embeds the words in that text, and \r\n","    3. adds positional encoding, \r\n","       i.e. associates a number in range(max_len) with \r\n","       each word in each sentence of embedded input text \r\n","    \r\n","    The input is a list of tokenized blocks of text\r\n","    \r\n","    Args:\r\n","        vocab_size (int): vocab size.\r\n","        d_model (int):  depth of embedding.\r\n","        dropout (float): dropout rate (how much to drop out).\r\n","        max_len (int): maximum symbol length for positional encoding.\r\n","        mode (str): 'train' or 'eval'.\r\n","    \"\"\"\r\n","    # Embedding inputs and positional encoder\r\n","    return [\r\n","        # Add embedding layer of dimension (vocab_size, d_model)\r\n","        tl.Embedding(vocab_size, d_model),\r\n","        # Use dropout with rate and mode specified\r\n","        tl.Dropout(rate=dropout, mode=mode),\r\n","        # Add positional encoding layer with maximum input length and mode specified\r\n","        tl.PositionalEncoding(max_len=max_len, mode=mode)\r\n","    ]"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"0YlCwzV8a-aj","executionInfo":{"status":"ok","timestamp":1612856415903,"user_tz":480,"elapsed":285,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["a = np.random.rand(10, 10, 10)\r\n","p = tl.PositionalEncoding(max_len=12)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bd2X3r4Ga-Xr","executionInfo":{"status":"ok","timestamp":1612856779437,"user_tz":480,"elapsed":270,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"dbb11eb2-9113-402d-a94d-5e420346e5d9"},"source":["layer = tl.PositionalEncoding(max_len=8)\r\n","x = np.array([[[2.0, 3.0, 4.0, 5.0], [1.0, 2.0, 3.0, 4.0], [6.0, 7.0, 8.0, 9.0]]])\r\n","x.shape"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 3, 4)"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_XxKROtVa-Ux","executionInfo":{"status":"ok","timestamp":1612856780438,"user_tz":480,"elapsed":419,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"9494c6b4-5302-4ea0-db03-b5f12ab2d135"},"source":["x"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[2., 3., 4., 5.],\n","        [1., 2., 3., 4.],\n","        [6., 7., 8., 9.]]])"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tHzAVsuCa-Rw","executionInfo":{"status":"ok","timestamp":1612856781430,"user_tz":480,"elapsed":283,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"3f6aeca7-c3ed-455a-c845-59ce35d4c7b7"},"source":["from trax import shapes\r\n","layer.init(shapes.signature(x))"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(DeviceArray([[[ 0.        ,  1.        ,  0.        ,  1.        ],\n","               [ 0.84147096,  0.5403023 ,  0.00999983,  0.99995   ],\n","               [ 0.9092974 , -0.41614684,  0.01999867,  0.9998    ],\n","               [ 0.14112   , -0.9899925 ,  0.0299955 ,  0.99955004],\n","               [-0.7568025 , -0.6536436 ,  0.03998933,  0.9992001 ],\n","               [-0.9589243 ,  0.2836622 ,  0.04997917,  0.99875027],\n","               [-0.2794155 ,  0.96017027,  0.059964  ,  0.99820054],\n","               [ 0.6569866 ,  0.75390226,  0.06994285,  0.997551  ]]],            dtype=float32),\n"," ())"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uM0U3xAja-Om","executionInfo":{"status":"ok","timestamp":1612856850724,"user_tz":480,"elapsed":293,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"6edab5ca-ec78-4dbb-9ece-c9b731cccb7f"},"source":["y = layer(x)\r\n","y"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DeviceArray([[[2.       , 4.       , 4.       , 6.       ],\n","              [1.841471 , 2.5403023, 3.0099998, 4.99995  ],\n","              [6.9092975, 6.5838532, 8.019999 , 9.9998   ]]],            dtype=float32)"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"7n6kJpp9w_gT"},"source":["## Multi-head causal attention\r\n","\r\n","The layers and array dimensions involved in multi-head causal attention (which looks at previous words in the input text) are summarized in the figure below: \r\n","\r\n","<img src=\"https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/images/c4_w2_l5_multi_head_attention_s05_multi_head_attention_concatenation_stripped.png\" width=\"500px\"/>\r\n","\r\n","`tl.CausalAttention()` does all of this for you! You might be wondering, though, whether you need to pass in your input text 3 times, since for causal attention, the queries Q, keys K, and values V all come from the same source. Fortunately, `tl.CausalAttention()` handles this as well by making use of the [`tl.Branch()`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators) combinator layer. In general, each branch within a `tl.Branch()` layer performs parallel operations on copies of the layer's inputs. For causal attention, each branch (representing Q, K, and V) applies a linear transformation (i.e. a dense layer without a subsequent activation) to its copy of the input, then splits that result into heads. You can see the syntax for this in the screenshot from the `trax.layers.attention.py` [source code](https://github.com/google/trax/blob/master/trax/layers/attention.py) below: \r\n","\r\n","<img src=\"https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/images/use_of_tl_branch_in_tl_causal_attention.png\" width=\"500px\"/>"]},{"cell_type":"markdown","metadata":{"id":"vjqIDwEF3mZE"},"source":["## Feed-forward layer \r\n","* Typically ends with a ReLU activation, but we'll leave open the possibility of a different activation\r\n","* Most of the parameters are here"]},{"cell_type":"code","metadata":{"id":"OR4yEpq5v1J_","executionInfo":{"status":"ok","timestamp":1612859181191,"user_tz":480,"elapsed":392,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["def feed_forward(d_model, d_ff, dropout, mode, ff_activation):\r\n","    \"\"\"Returns a list of layers that implements a feed-forward block.\r\n","\r\n","    The input is an activation tensor.\r\n","\r\n","    Args:\r\n","        d_model (int):  depth of embedding.\r\n","        d_ff (int): depth of feed-forward layer.\r\n","        dropout (float): dropout rate (how much to drop out).\r\n","        mode (str): 'train' or 'eval'.\r\n","        ff_activation (function): the non-linearity in feed-forward layer.\r\n","\r\n","    Returns:\r\n","        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n","    \"\"\"\r\n","    # Create feed-forward block (list) with two dense layers with dropout and \r\n","    # input normalized\r\n","    return [\r\n","        # Normalize layer inputs\r\n","        tl.LayerNorm(), \r\n","        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\r\n","        tl.Dense(d_ff), \r\n","        # Add activation function passed in as a parameter (you need to call it!)\r\n","        ff_activation(),  # Generally ReLU\r\n","        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\r\n","        tl.Dropout(rate=dropout, mode=mode), \r\n","        # Add second feed forward layer (don't forget to set the correct value for n_units)\r\n","        tl.Dense(d_model), \r\n","        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\r\n","        tl.Dropout(rate=dropout, mode=mode)\r\n","    ]\r\n","    "],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J3MmO0yE44i8"},"source":["## Decoder block\r\n","Here, we return a list containing two residual blocks. The first wraps around the causal attention layer, whose inputs are normalized and to which we apply dropout regulation. The second wraps around the feed-forward layer. You may notice that the second call to `tl.Residual()` doesn't call a normalization layer before calling the feed-forward layer. This is because the normalization layer is included in the feed-forward layer."]},{"cell_type":"code","metadata":{"id":"xsZULQX9v2QW","executionInfo":{"status":"ok","timestamp":1612859185521,"user_tz":480,"elapsed":525,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["def DecoderBlock(d_model, d_ff, n_heads,\r\n","                 dropout, mode, ff_activation):\r\n","    \"\"\"Returns a list of layers that implements a Transformer decoder block.\r\n","\r\n","    The input is an activation tensor.\r\n","\r\n","    Args:\r\n","        d_model (int):  depth of embedding.\r\n","        d_ff (int): depth of feed-forward layer.\r\n","        n_heads (int): number of attention heads.\r\n","        dropout (float): dropout rate (how much to drop out).\r\n","        mode (str): 'train' or 'eval'.\r\n","        ff_activation (function): the non-linearity in feed-forward layer.\r\n","\r\n","    Returns:\r\n","        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n","    \"\"\"\r\n","        \r\n","    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n","    return [\r\n","      tl.Residual(\r\n","          # Normalize layer input\r\n","          tl.LayerNorm(), \r\n","          # Add causal attention \r\n","          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode) \r\n","        ),\r\n","      tl.Residual(\r\n","          # Add feed-forward block\r\n","          # We don't need to normalize the layer inputs here. The feed-forward block takes care of that for us.\r\n","          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n","        ),\r\n","      ]"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jn0VuYhB5sET"},"source":["## The transformer decoder: putting it all together\r\n","## A.k.a. repeat N times, dense layer and softmax for output"]},{"cell_type":"code","metadata":{"id":"8B02WRND5tA6","executionInfo":{"status":"ok","timestamp":1612859385969,"user_tz":480,"elapsed":383,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["def TransformerLM(vocab_size=33300,\r\n","                  d_model=512,\r\n","                  d_ff=2048,\r\n","                  n_layers=6,\r\n","                  n_heads=8,\r\n","                  dropout=0.1,\r\n","                  max_len=4096,\r\n","                  mode='train',\r\n","                  ff_activation=tl.Relu):\r\n","    \"\"\"Returns a Transformer language model.\r\n","\r\n","    The input to the model is a tensor of tokens. (This model uses only the\r\n","    decoder part of the overall Transformer.)\r\n","\r\n","    Args:\r\n","        vocab_size (int): vocab size.\r\n","        d_model (int):  depth of embedding.\r\n","        d_ff (int): depth of feed-forward layer.\r\n","        n_layers (int): number of decoder layers.\r\n","        n_heads (int): number of attention heads.\r\n","        dropout (float): dropout rate (how much to drop out).\r\n","        max_len (int): maximum symbol length for positional encoding.\r\n","        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n","        ff_activation (function): the non-linearity in feed-forward layer.\r\n","\r\n","    Returns:\r\n","        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n","        to activations over a vocab set.\r\n","    \"\"\"\r\n","    \r\n","    # Create stack (list) of decoder blocks with n_layers with necessary parameters\r\n","    decoder_blocks = [ \r\n","        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \r\n","\r\n","    # Create the complete model as written in the figure\r\n","    return tl.Serial(\r\n","        # Use teacher forcing (feed output of previous step to current step)\r\n","        tl.ShiftRight(mode=mode), \r\n","        # Add embedding inputs and positional encoder\r\n","        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\r\n","        # Add decoder blocks\r\n","        decoder_blocks, \r\n","        # Normalize layer\r\n","        tl.LayerNorm(), \r\n","\r\n","        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n","        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n","        tl.Dense(vocab_size), \r\n","        # Get probabilities with Logsoftmax\r\n","        tl.LogSoftmax() \r\n","    )"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YIgirR2E6U17"},"source":["## Concluding remarks\r\n","\r\n","In this week's assignment, you'll see how to train a transformer decoder on the [cnn_dailymail](https://www.tensorflow.org/datasets/catalog/cnn_dailymail) dataset, available from TensorFlow Datasets (part of TensorFlow Data Services). Because training such a model from scratch is time-intensive, you'll use a pre-trained model to summarize documents later in the assignment. Due to time and storage concerns, we will also not train the decoder on a different summarization dataset in this lab. If you have the time and space, we encourage you to explore the other [summarization](https://www.tensorflow.org/datasets/catalog/overview#summarization) datasets at TensorFlow Datasets. Which of them might suit your purposes better than the `cnn_dailymail` dataset? Where else can you find datasets for text summarization models?"]}]}