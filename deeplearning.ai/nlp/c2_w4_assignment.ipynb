{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c2_w4_assignment.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/nlp/c2_w4_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"HHTijOy1JFqz"},"source":["# Assignment 4: Word Embeddings \n","\n","Welcome to the fourth (and last) programming assignment of Course 2! \n","\n","In this assignment, you will practice how to compute word embeddings and use them for sentiment analysis.\n","- To implement sentiment analysis, you can go beyond counting the number of positive words and negative words. \n","- You can find a way to represent each word numerically, by a vector. \n","- The vector could then represent syntactic (i.e. parts of speech) and semantic (i.e. meaning) structures. \n","\n","In this assignment, you will explore a classic way of generating word embeddings or representations.\n","- You will implement a famous model called the continuous bag of words (CBOW) model. \n","\n","By completing this assignment you will:\n","\n","- Train word vectors from scratch.\n","- Learn how to create batches of data.\n","- Understand how backpropagation works.\n","- Plot and visualize your learned word vectors.\n","\n","Knowing how to train these models will give you a better understanding of word vectors, which are building blocks to many applications in natural language processing."]},{"cell_type":"markdown","metadata":{"id":"Wp9WAWNYJWkT"},"source":["## Outline\n","\n","- [1 The Continuous bag of words model](#1)\n","- [2 Training the Model](#2)\n","    - [2.0 Initialize the model](#2)\n","        - [Exercise 01](#ex-01)\n","    - [2.1 Softmax Function](#2.1)\n","        - [Exercise 02](#ex-02)\n","    - [2.2 Forward Propagation](#2.2)\n","        - [Exercise 03](#ex-03)\n","    - [2.3 Cost Function](#2.3)\n","    - [2.4 Backproagation](#2.4)\n","        - [Exercise 04](#ex-04)\n","    - [2.5 Gradient Descent](#2.5)\n","        - [Exercise 05](#ex-05)\n","- [3 Visualizing the word vectors](#3)"]},{"cell_type":"markdown","metadata":{"id":"pu_c3yDKJdOK"},"source":["<a name='1'></a>\n","# 1. The Continuous bag of words model\n","\n","Let's take a look at the following sentence: \n",">**'I am happy because I am learning'**. \n","\n","- In continuous bag of words (CBOW) modeling, we try to predict the center word given a few context words (the words around the center word).\n","- For example, if you were to choose a context half-size of say $C = 2$, then you would try to predict the word **happy** given the context that includes 2 words before and 2 words after the center word:\n","\n","> $C$ words before: [I, am] \n","\n","> $C$ words after: [because, I] \n","\n","- In other words:\n","\n","$$context = [I,am, because, I]$$\n","$$target = happy$$\n","\n","The structure of your model will look like this:\n","\n","<img src=\"https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/images/word2.png\" alt=\"alternate text\" width=\"600px\"/>\n","\n","Where $\\bar x$ is the average of all the one hot vectors of the context words. \n","\n","<img src=\"https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/images/mean_vec2.png\" alt=\"alternate text\" width=\"700px\"/>\n","\n","Once you have encoded all the context words, you can use $\\bar x$ as the input to your model. \n","\n","The architecture you will be implementing is as follows:\n","\n","\\begin{align}\n"," h &= W_1 \\  X + b_1  \\tag{1} \\\\\n"," a &= ReLU(h)  \\tag{2} \\\\\n"," z &= W_2 \\  a + b_2   \\tag{3} \\\\\n"," \\hat y &= softmax(z)   \\tag{4} \\\\\n","\\end{align}"]},{"cell_type":"code","metadata":{"id":"hpn-lG5zYLWi","executionInfo":{"status":"ok","timestamp":1611300143651,"user_tz":480,"elapsed":709,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["%%capture\n","!wget https://raw.githubusercontent.com/martin-fabbri/colab-notebooks/master/deeplearning.ai/nlp/datasets/shakespeare.txt"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"MNpEXreZJSKS","executionInfo":{"status":"ok","timestamp":1611300143652,"user_tz":480,"elapsed":692,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["from collections import Counter, defaultdict\n","\n","import nltk\n","import re\n","import numpy as np\n","import scipy as linalg\n","from nltk.tokenize import word_tokenize"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"mL3iwjr0V4yS","executionInfo":{"status":"ok","timestamp":1611300143899,"user_tz":480,"elapsed":933,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["#@markdown Utility functions\n","#@markdown - sigmoid(z)\n","#@markdown - get_idx(words, word2Ind)\n","#@markdown - pack_idx_with_frequency(context_words, word2Ind)\n","#@markdown - get_vectors(data, word2Ind, V, C)\n","#@markdown - get_batches(data, word2Ind, V, C, batch_size)\n","#@markdown - compute_pca(data, n_components=2)\n","#@markdown - get_dict(data)\n","\n","def sigmoid(z):\n","    # sigmoid function\n","    return 1.0/(1.0+np.exp(-z))\n","\n","\n","def get_idx(words, word2Ind):\n","    idx = []\n","    for word in words:\n","        idx = idx + [word2Ind[word]]\n","    return idx\n","\n","\n","def pack_idx_with_frequency(context_words, word2Ind):\n","    freq_dict = defaultdict(int)\n","    for word in context_words:\n","        freq_dict[word] += 1\n","    idxs = get_idx(context_words, word2Ind)\n","    packed = []\n","    for i in range(len(idxs)):\n","        idx = idxs[i]\n","        freq = freq_dict[context_words[i]]\n","        packed.append((idx, freq))\n","    return packed\n","\n","\n","def get_vectors(data, word2Ind, V, C):\n","    i = C\n","    while True:\n","        y = np.zeros(V)\n","        x = np.zeros(V)\n","        center_word = data[i]\n","        y[word2Ind[center_word]] = 1\n","        context_words = data[(i - C):i] + data[(i+1):(i+C+1)]\n","        num_ctx_words = len(context_words)\n","        for idx, freq in pack_idx_with_frequency(context_words, word2Ind):\n","            x[idx] = freq/num_ctx_words\n","        yield x, y\n","        i += 1\n","        if i >= len(data):\n","            print('i is being set to 0')\n","            i = 0\n","\n","\n","def get_batches(data, word2Ind, V, C, batch_size):\n","    batch_x = []\n","    batch_y = []\n","    for x, y in get_vectors(data, word2Ind, V, C):\n","        while len(batch_x) < batch_size:\n","            batch_x.append(x)\n","            batch_y.append(y)\n","        else:\n","            yield np.array(batch_x).T, np.array(batch_y).T\n","            batch = []\n","\n","\n","def compute_pca(data, n_components=2):\n","    \"\"\"\n","    Input: \n","        data: of dimension (m,n) where each row corresponds to a word vector\n","        n_components: Number of components you want to keep.\n","    Output: \n","        X_reduced: data transformed in 2 dims/columns + regenerated original data\n","    pass in: data as 2D NumPy array\n","    \"\"\"\n","\n","    m, n = data.shape\n","\n","    ### START CODE HERE ###\n","    # mean center the data\n","    data -= data.mean(axis=0)\n","    # calculate the covariance matrix\n","    R = np.cov(data, rowvar=False)\n","    # calculate eigenvectors & eigenvalues of the covariance matrix\n","    # use 'eigh' rather than 'eig' since R is symmetric,\n","    # the performance gain is substantial\n","    evals, evecs = linalg.eigh(R)\n","    # sort eigenvalue in decreasing order\n","    # this returns the corresponding indices of evals and evecs\n","    idx = np.argsort(evals)[::-1]\n","\n","    evecs = evecs[:, idx]\n","    # sort eigenvectors according to same index\n","    evals = evals[idx]\n","    # select the first n eigenvectors (n is desired dimension\n","    # of rescaled data array, or dims_rescaled_data)\n","    evecs = evecs[:, :n_components]\n","    ### END CODE HERE ###\n","    return np.dot(evecs.T, data.T).T\n","\n","\n","def get_dict(data):\n","    \"\"\"\n","    Input:\n","        K: the number of negative samples\n","        data: the data you want to pull from\n","        indices: a list of word indices\n","    Output:\n","        word_dict: a dictionary with the weighted probabilities of each word\n","        word2Ind: returns dictionary mapping the word to its index\n","        Ind2Word: returns dictionary mapping the index to its word\n","    \"\"\"\n","    #\n","#     words = nltk.word_tokenize(data)\n","    words = sorted(list(set(data)))\n","    n = len(words)\n","    idx = 0\n","    # return these correctly\n","    word2Ind = {}\n","    Ind2word = {}\n","    for k in words:\n","        word2Ind[k] = idx\n","        Ind2word[idx] = k\n","        idx += 1\n","    return word2Ind, Ind2word"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"gnRCYGOwJTVv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611300143900,"user_tz":480,"elapsed":927,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"088fa650-982d-4f3b-900a-51bd6cd10426"},"source":["# Download sentence tokenizer\n","nltk.data.path.append('.')\n","nltk.download('punkt')"],"execution_count":27,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"tkpjUc__JTTV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611300144636,"user_tz":480,"elapsed":1655,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"bf9be8ff-623a-428d-8d62-59728a88694d"},"source":["with open(\"shakespeare.txt\") as f:\n","    data = f.read()                     #  Read in the data\n","data = re.sub(r\"[,!?;-]\", \".\", data)    #  Punktuations are replaced by .\n","data = nltk.word_tokenize(data)         #  Tokenize string to words\n","data = [\n","    ch.lower() for ch in data if ch.isalpha() or ch == \".\"\n","] #  Lower case and drop non-alphabetical tokens\n","print(\"Number of tokens:\", len(data), \"\\n\", data[:15])"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Number of tokens: 60933 \n"," ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rcsv7RTDJTQd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611300144637,"user_tz":480,"elapsed":1647,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"75d3df4b-939a-48c8-d998-ffe004c31627"},"source":["# Compute the frequency distribution of the words in the dataset (vocabulary)\n","fdist = nltk.FreqDist(word for word in data)\n","print(\"Size of vocabulary: \", len(fdist))\n","print(\n","    \"Most frequent tokens: \", fdist.most_common(20)\n",")  # print the 20 most frequent words and their freq."],"execution_count":29,"outputs":[{"output_type":"stream","text":["Size of vocabulary:  5772\n","Most frequent tokens:  [('.', 9630), ('the', 1521), ('and', 1394), ('i', 1252), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 770), ('you', 748), ('a', 742), ('is', 630), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-LadjwPubCB5"},"source":["#### Mapping words to indices and indices to words\n","We provide a helper function to create a dictionary that maps words to indices and indices to words."]},{"cell_type":"code","metadata":{"id":"pHoRsbbfJTNs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611300144638,"user_tz":480,"elapsed":1640,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"9cc98c9a-9374-4705-9a18-023a807e2254"},"source":["# get_dict creates two dictionaries, converting words to indices and viceversa.\n","word2Ind, Ind2word = get_dict(data)\n","V = len(word2Ind)\n","print(\"Size of vocabulary: \", V)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Size of vocabulary:  5772\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ljv9WmTbbDce","executionInfo":{"status":"ok","timestamp":1611300144639,"user_tz":480,"elapsed":1631,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"2d8e3a68-ec75-46a8-e513-6fc775e72c15"},"source":["# example of word to index mapping\n","print(\"Index of the word 'king' :  \",word2Ind['king'] )\n","print(\"Word which has index 2743:  \",Ind2word[2743] )"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Index of the word 'king' :   2743\n","Word which has index 2743:   king\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Nq4p9PHRbUpJ"},"source":["<a name='2'></a>\n","# 2 Training the Model\n","\n","###  Initializing the model\n","\n","You will now initialize two matrices and two vectors. \n","- The first matrix ($W_1$) is of dimension $N \\times V$, where $V$ is the number of words in your vocabulary and $N$ is the dimension of your word vector.\n","- The second matrix ($W_2$) is of dimension $V \\times N$. \n","- Vector $b_1$ has dimensions $N\\times 1$\n","- Vector $b_2$ has dimensions  $V\\times 1$. \n","- $b_1$ and $b_2$ are the bias vectors of the linear layers from matrices $W_1$ and $W_2$.\n","\n","The overall structure of the model will look as in Figure 1, but at this stage we are just initializing the parameters. \n","\n","<a name='ex-01'></a>\n","### Exercise 01\n","Please use [numpy.random.rand](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html) to generate matrices that are initialized with random values from a uniform distribution, ranging between 0 and 1.\n","\n","**Note:** In the next cell you will encounter a random seed. Please **DO NOT** modify this seed so your solution can be tested correctly."]},{"cell_type":"code","metadata":{"id":"57RhzESqbDZd","executionInfo":{"status":"ok","timestamp":1611300144640,"user_tz":480,"elapsed":1624,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED FUNCTION: initialize_model\n","def initialize_model(N,V, random_seed=1):\n","    '''\n","    Inputs: \n","        N:  dimension of hidden vector \n","        V:  dimension of vocabulary\n","        random_seed: random seed for consistent results in the unit tests\n","     Outputs: \n","        W1, W2, b1, b2: initialized weights and biases\n","    '''\n","    \n","    np.random.seed(random_seed)\n","    \n","    ### START CODE HERE (Replace instances of 'None' with your code) ###\n","    # W1 has shape (N,V)\n","    W1 = np.random.rand(N, V)\n","    # W2 has shape (V,N)\n","    W2 = np.random.rand(V, N)\n","    # b1 has shape (N,1)\n","    b1 = np.random.rand(N, 1)\n","    # b2 has shape (V,1)\n","    b2 = np.random.rand(V, 1)\n","    ### END CODE HERE ###\n","\n","    return W1, W2, b1, b2"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TuFjoronbDWU","executionInfo":{"status":"ok","timestamp":1611300144640,"user_tz":480,"elapsed":1617,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"ec71643d-5736-4232-d4a0-641c7349a585"},"source":["# Test your function example.\n","tmp_N = 4\n","tmp_V = 10\n","tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n","assert tmp_W1.shape == ((tmp_N,tmp_V))\n","assert tmp_W2.shape == ((tmp_V,tmp_N))\n","print(f\"tmp_W1.shape: {tmp_W1.shape}\")\n","print(f\"tmp_W2.shape: {tmp_W2.shape}\")\n","print(f\"tmp_b1.shape: {tmp_b1.shape}\")\n","print(f\"tmp_b2.shape: {tmp_b2.shape}\")"],"execution_count":33,"outputs":[{"output_type":"stream","text":["tmp_W1.shape: (4, 10)\n","tmp_W2.shape: (10, 4)\n","tmp_b1.shape: (4, 1)\n","tmp_b2.shape: (10, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cVbBM6IdigVp"},"source":["##### Expected Output \n","\n","```CPP\n","tmp_W1.shape: (4, 10)\n","tmp_W2.shape: (10, 4)\n","tmp_b1.shape: (4, 1)\n","tmp_b2.shape: (10, 1)\n","```"]},{"cell_type":"markdown","metadata":{"id":"4EuBQH4uinjQ"},"source":["<a name='2.1'></a>\n","### 2.1 Softmax\n","Before we can start training the model, we need to implement the softmax function as defined in equation 5:  \n","\n","<br>\n","$$ \\text{softmax}(z_i) = \\frac{e^{z_i} }{\\sum_{i=0}^{V-1} e^{z_i} }  \\tag{5} $$\n","\n","- Array indexing in code starts at 0.\n","- $V$ is the number of words in the vocabulary (which is also the number of rows of $z$).\n","- $i$ goes from 0 to |V| - 1.\n","\n","\n","<a name='ex-02'></a>\n","### Exercise 02\n","**Instructions**: Implement the softmax function below. \n","\n","- Assume that the input $z$ to `softmax` is a 2D array\n","- Each training example is represented by a column of shape (V, 1) in this 2D array.\n","- There may be more than one column, in the 2D array, because you can put in a batch of examples to increase efficiency.  Let's call the batch size lowercase $m$, so the $z$ array has shape (V, m)\n","- When taking the sum from $i=1 \\cdots V-1$, take the sum for each column (each example) separately.\n","\n","Please use\n","- numpy.exp\n","- numpy.sum (set the axis so that you take the sum of each column in z)"]},{"cell_type":"code","metadata":{"id":"VblNpQJRbDTZ","executionInfo":{"status":"ok","timestamp":1611300144641,"user_tz":480,"elapsed":1611,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED FUNCTION: softmax\n","def softmax(z):\n","    '''\n","    Inputs: \n","        z: output scores from the hidden layer [shape (V, m)] where m = #batch \n","    Outputs: \n","        yhat: prediction (estimate of y)\n","    '''\n","    \n","    ### START CODE HERE (Replace instances of 'None' with your own code) ###\n","    \n","    # Calculate yhat (softmax)\n","    z_exp = np.exp(z)\n","    z_exp_sum = np.sum(z_exp, axis=0)\n","    yhat = z_exp / z_exp_sum\n","    \n","    ### END CODE HERE ###\n","    \n","    return yhat"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"KYftRR-DJTKp","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1611300144642,"user_tz":480,"elapsed":1605,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"a4bcfe32-93ed-4d1b-b2e5-28fda0c46a69"},"source":["# Test the function\n","tmp = np.array([[1,2,3],\n","                [1,1,1]\n","               ])\n","tmp_sm = softmax(tmp)\n","display(tmp_sm)"],"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":["array([[0.5       , 0.73105858, 0.88079708],\n","       [0.5       , 0.26894142, 0.11920292]])"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"kvhdk01xydeC"},"source":["##### Expected Ouput\n","\n","```CPP\n","array([[0.5       , 0.73105858, 0.88079708],\n","       [0.5       , 0.26894142, 0.11920292]])\n","```"]},{"cell_type":"markdown","metadata":{"id":"RjZbeQan6k1P"},"source":["<a name='2.2'></a>\n","### 2.2 Forward propagation\n","\n","<a name='ex-03'></a>\n","### Exercise 03\n","Implement the forward propagation $z$ according to equations (1) to (3). <br>\n","\n","\\begin{align}\n"," h &= W_1 \\  X + b_1  \\tag{1} \\\\\n"," a &= ReLU(h)  \\tag{2} \\\\\n"," z &= W_2 \\  a + b_2   \\tag{3} \\\\\n","\\end{align}\n","\n","For that, you will use as activation the Rectified Linear Unit (ReLU) given by:\n","\n","$$f(h)=\\max (0,h) \\tag{6}$$"]},{"cell_type":"markdown","metadata":{"id":"htIgOP61asC7"},"source":["<details>    \n","<summary>\n","    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n","</summary>\n","<p>\n","<ul>\n","    <li>You can use numpy.maximum(x1,x2) to get the maximum of two values</li>\n","    <li>Use numpy.dot(A,B) to matrix multiply A and B</li>\n","</ul>\n","</p>\n"]},{"cell_type":"code","metadata":{"id":"J-7hZQdAJTG3","executionInfo":{"status":"ok","timestamp":1611300144643,"user_tz":480,"elapsed":1598,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED FUNCTION: forward_prop\n","def forward_prop(x, W1, W2, b1, b2):\n","    '''\n","    Inputs: \n","        x:  average one hot vector for the context \n","        W1, W2, b1, b2:  matrices and biases to be learned\n","     Outputs: \n","        z:  output score vector\n","    '''\n","    \n","    ### START CODE HERE (Replace instances of 'None' with your own code) ###\n","    \n","    # Calculate h\n","    h = np.dot(W1, x) + b1\n","    \n","    # Apply the relu on h (store result in h)\n","    h = np.maximum(0, h)\n","    \n","    # Calculate z\n","    z = np.dot(W2, h) + b2\n","    \n","    ### END CODE HERE ###\n","\n","    return z, h"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"039auXetJS9j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611300144889,"user_tz":480,"elapsed":1833,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"1a01364e-b59a-40f0-8e9c-ede32ad3d5da"},"source":["# Test the function\n","\n","# Create some inputs\n","tmp_N = 2\n","tmp_V = 3\n","tmp_x = np.array([[0,1,0]]).T\n","tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(N=tmp_N,V=tmp_V, random_seed=1)\n","\n","print(f\"x has shape {tmp_x.shape}\")\n","print(f\"N is {tmp_N} and vocabulary size V is {tmp_V}\")\n","\n","# call function\n","tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n","print(\"call forward_prop\")\n","print()\n","# Look at output\n","print(f\"z has shape {tmp_z.shape}\")\n","print(\"z has values:\")\n","print(tmp_z)\n","\n","print()\n","\n","print(f\"h has shape {tmp_h.shape}\")\n","print(\"h has values:\")\n","print(tmp_h)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["x has shape (3, 1)\n","N is 2 and vocabulary size V is 3\n","call forward_prop\n","\n","z has shape (3, 1)\n","z has values:\n","[[0.55379268]\n"," [1.58960774]\n"," [1.50722933]]\n","\n","h has shape (2, 1)\n","h has values:\n","[[0.92477674]\n"," [1.02487333]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kvgHzO10eGua"},"source":["##### Expected output\n","```CPP\n","x has shape (3, 1)\n","N is 2 and vocabulary size V is 3\n","call forward_prop\n","\n","z has shape (3, 1)\n","z has values:\n","[[0.55379268]\n"," [1.58960774]\n"," [1.50722933]]\n","\n","h has shape (2, 1)\n","h has values:\n","[[0.92477674]\n"," [1.02487333]]\n","```"]},{"cell_type":"markdown","metadata":{"id":"5zBtqCuTezxB"},"source":["<a name='2.3'></a>\n","## 2.3 Cost function\n","\n","- We have implemented the *cross-entropy* cost function for you."]},{"cell_type":"code","metadata":{"id":"sg28Dou2HNwP","executionInfo":{"status":"ok","timestamp":1611300144890,"user_tz":480,"elapsed":1822,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# compute_cost: cross-entropy cost functioN\n","def compute_cost(y, yhat, batch_size):\n","    # cost function \n","    logprobs = np.multiply(np.log(yhat),y) + np.multiply(np.log(1 - yhat), 1 - y)\n","    cost = - 1/batch_size * np.sum(logprobs)\n","    cost = np.squeeze(cost)\n","    return cost"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kjhmkhw5hfL8","executionInfo":{"status":"ok","timestamp":1611300144891,"user_tz":480,"elapsed":1812,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"809ad1d3-2dc8-4967-8bde-93ef50677b1d"},"source":["# Test the function\n","tmp_C = 2\n","tmp_N = 50\n","tmp_batch_size = 4\n","tmp_word2Ind, tmp_Ind2word = get_dict(data)\n","tmp_V = len(word2Ind)\n","\n","tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n","        \n","print(f\"tmp_x.shape {tmp_x.shape}\")\n","print(f\"tmp_y.shape {tmp_y.shape}\")\n","\n","tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n","\n","print(f\"tmp_W1.shape {tmp_W1.shape}\")\n","print(f\"tmp_W2.shape {tmp_W2.shape}\")\n","print(f\"tmp_b1.shape {tmp_b1.shape}\")\n","print(f\"tmp_b2.shape {tmp_b2.shape}\")\n","\n","tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n","print(f\"tmp_z.shape: {tmp_z.shape}\")\n","print(f\"tmp_h.shape: {tmp_h.shape}\")\n","\n","tmp_yhat = softmax(tmp_z)\n","print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n","\n","tmp_cost = compute_cost(tmp_y, tmp_yhat, tmp_batch_size)\n","print(\"call compute_cost\")\n","print(f\"tmp_cost {tmp_cost:.4f}\")"],"execution_count":39,"outputs":[{"output_type":"stream","text":["tmp_x.shape (5772, 4)\n","tmp_y.shape (5772, 4)\n","tmp_W1.shape (50, 5772)\n","tmp_W2.shape (5772, 50)\n","tmp_b1.shape (50, 1)\n","tmp_b2.shape (5772, 1)\n","tmp_z.shape: (5772, 4)\n","tmp_h.shape: (50, 4)\n","tmp_yhat.shape: (5772, 4)\n","call compute_cost\n","tmp_cost 12.9825\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qNTM6Zb7hzsD"},"source":["##### Expected output\n","\n","```CPP\n","tmp_x.shape (5778, 4)\n","tmp_y.shape (5778, 4)\n","tmp_W1.shape (50, 5778)\n","tmp_W2.shape (5778, 50)\n","tmp_b1.shape (50, 1)\n","tmp_b2.shape (5778, 1)\n","tmp_z.shape: (5778, 4)\n","tmp_h.shape: (50, 4)\n","tmp_yhat.shape: (5778, 4)\n","call compute_cost\n","tmp_cost 9.9560\n","```"]},{"cell_type":"markdown","metadata":{"id":"qTVQ5uuDlBcf"},"source":["<a name='2.4'></a>\n","## 2.4 Training the Model - Backpropagation\n","\n","<a name='ex-04'></a>\n","### Exercise 04\n","Now that you have understood how the CBOW model works, you will train it. <br>\n","You created a function for the forward propagation. Now you will implement a function that computes the gradients to backpropagate the errors."]},{"cell_type":"code","metadata":{"id":"lmdlTYL1lCIw","executionInfo":{"status":"ok","timestamp":1611300144891,"user_tz":480,"elapsed":1802,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED FUNCTION: back_prop\n","def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n","    '''\n","    Inputs: \n","        x:  average one hot vector for the context \n","        yhat: prediction (estimate of y)\n","        y:  target vector\n","        h:  hidden vector (see eq. 1)\n","        W1, W2, b1, b2:  matrices and biases  \n","        batch_size: batch size \n","     Outputs: \n","        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n","    '''\n","    ### START CODE HERE (Replace instanes of 'None' with your code) ###\n","    \n","    # Compute l1 as W2^T (Yhat - Y)\n","    # Re-use it whenever you see W2^T (Yhat - Y) used to compute a gradient\n","    l1 = np.dot(W2.T, yhat - y)\n","    # Apply relu to l1\n","    l1 = np.maximum(0, l1)\n","    # Compute the gradient of W1\n","    grad_W1 = 1/batch_size * np.dot(l1, x.T) \n","    # Compute the gradient of W2\n","    grad_W2 = 1/batch_size * np.dot(yhat - y, h.T)\n","    # Compute the gradient of b1\n","    grad_b1 = 1/batch_size * np.sum(l1, axis=1, keepdims=True)\n","    # Compute the gradient of b2\n","    grad_b2 = 1/batch_size * np.sum(yhat - y, axis=1, keepdims=True)\n","    ### END CODE HERE ###\n","    \n","    return grad_W1, grad_W2, grad_b1, grad_b2"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bmovsuq2lCbo","executionInfo":{"status":"ok","timestamp":1611300144892,"user_tz":480,"elapsed":1792,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"e33c8fdb-835e-4b03-ceef-0fead4cae3b4"},"source":["# Test the function\n","tmp_C = 2\n","tmp_N = 50\n","tmp_batch_size = 4\n","tmp_word2Ind, tmp_Ind2word = get_dict(data)\n","tmp_V = len(word2Ind)\n","\n","# get a batch of data\n","tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n","\n","print(\"get a batch of data\")\n","print(f\"tmp_x.shape {tmp_x.shape}\")\n","print(f\"tmp_y.shape {tmp_y.shape}\")\n","\n","print()\n","print(\"Initialize weights and biases\")\n","tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n","\n","print(f\"tmp_W1.shape {tmp_W1.shape}\")\n","print(f\"tmp_W2.shape {tmp_W2.shape}\")\n","print(f\"tmp_b1.shape {tmp_b1.shape}\")\n","print(f\"tmp_b2.shape {tmp_b2.shape}\")\n","\n","print()\n","print(\"Forwad prop to get z and h\")\n","tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n","print(f\"tmp_z.shape: {tmp_z.shape}\")\n","print(f\"tmp_h.shape: {tmp_h.shape}\")\n","\n","print()\n","print(\"Get yhat by calling softmax\")\n","tmp_yhat = softmax(tmp_z)\n","print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n","\n","tmp_m = (2*tmp_C)\n","tmp_grad_W1, tmp_grad_W2, tmp_grad_b1, tmp_grad_b2 = back_prop(tmp_x, tmp_yhat, tmp_y, tmp_h, tmp_W1, tmp_W2, tmp_b1, tmp_b2, tmp_batch_size)\n","\n","print()\n","print(\"call back_prop\")\n","print(f\"tmp_grad_W1.shape {tmp_grad_W1.shape}\")\n","print(f\"tmp_grad_W2.shape {tmp_grad_W2.shape}\")\n","print(f\"tmp_grad_b1.shape {tmp_grad_b1.shape}\")\n","print(f\"tmp_grad_b2.shape {tmp_grad_b2.shape}\")"],"execution_count":41,"outputs":[{"output_type":"stream","text":["get a batch of data\n","tmp_x.shape (5772, 4)\n","tmp_y.shape (5772, 4)\n","\n","Initialize weights and biases\n","tmp_W1.shape (50, 5772)\n","tmp_W2.shape (5772, 50)\n","tmp_b1.shape (50, 1)\n","tmp_b2.shape (5772, 1)\n","\n","Forwad prop to get z and h\n","tmp_z.shape: (5772, 4)\n","tmp_h.shape: (50, 4)\n","\n","Get yhat by calling softmax\n","tmp_yhat.shape: (5772, 4)\n","\n","call back_prop\n","tmp_grad_W1.shape (50, 5772)\n","tmp_grad_W2.shape (5772, 50)\n","tmp_grad_b1.shape (50, 1)\n","tmp_grad_b2.shape (5772, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hGEcJlPVtSH6"},"source":["<a name='2.5'></a>\n","## Gradient Descent\n","\n","<a name='ex-05'></a>\n","### Exercise 05\n","Now that you have implemented a function to compute the gradients, you will implement batch gradient descent over your training set. \n","\n","**Hint:** For that, you will use `initialize_model` and the `back_prop` functions which you just created (and the `compute_cost` function). You can also use the provided `get_batches` helper function:\n","\n","```for x, y in get_batches(data, word2Ind, V, C, batch_size):```\n","\n","```...```\n","\n","Also: print the cost after each batch is processed (use batch size = 128)"]},{"cell_type":"code","metadata":{"id":"Xc5sryW6lCYY","executionInfo":{"status":"ok","timestamp":1611300145117,"user_tz":480,"elapsed":2008,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED FUNCTION: gradient_descent\n","def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03):\n","    \n","    '''\n","    This is the gradient_descent function\n","    \n","      Inputs: \n","        data:      text\n","        word2Ind:  words to Indices\n","        N:         dimension of hidden vector  \n","        V:         dimension of vocabulary \n","        num_iters: number of iterations  \n","     Outputs: \n","        W1, W2, b1, b2:  updated matrices and biases   \n","\n","    '''\n","    W1, W2, b1, b2 = initialize_model(N,V, random_seed=282)\n","    batch_size = 128\n","    iters = 0\n","    C = 2\n","    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n","        ### START CODE HERE (Replace instances of 'None' with your own code) ###\n","        # Get z and h\n","        z, h = forward_prop(x, W1, W2, b1, b2)\n","        # Get yhat\n","        yhat = softmax(z)\n","        # Get cost\n","        cost = compute_cost(y, yhat, batch_size)\n","        if ( (iters+1) % 10 == 0):\n","            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n","        # Get gradients\n","        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n","        \n","        # Update weights and biases\n","        W1 = W1 - alpha * grad_W1 \n","        W2 = W2 - alpha * grad_W2\n","        b1 = b1 - alpha * grad_b1\n","        b2 = b2 - alpha * grad_b2\n","        \n","        ### END CODE HERE ###\n","        \n","        iters += 1 \n","        if iters == num_iters: \n","            break\n","        if iters % 100 == 0:\n","            alpha *= 0.66\n","            \n","    return W1, W2, b1, b2"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLilnqzslCVc","executionInfo":{"status":"ok","timestamp":1611300164433,"user_tz":480,"elapsed":21315,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"7036d83b-3318-468e-d88a-13dde596f737"},"source":["# test your function\n","C = 2\n","N = 50\n","word2Ind, Ind2word = get_dict(data)\n","V = len(word2Ind)\n","num_iters = 150\n","print(\"Call gradient_descent\")\n","W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Call gradient_descent\n","iters: 10 cost: 0.543259\n","iters: 20 cost: 0.105205\n","iters: 30 cost: 0.057815\n","iters: 40 cost: 0.039841\n","iters: 50 cost: 0.030391\n","iters: 60 cost: 0.024565\n","iters: 70 cost: 0.020614\n","iters: 80 cost: 0.017758\n","iters: 90 cost: 0.015598\n","iters: 100 cost: 0.013906\n","iters: 110 cost: 0.012934\n","iters: 120 cost: 0.012129\n","iters: 130 cost: 0.011417\n","iters: 140 cost: 0.010785\n","iters: 150 cost: 0.010219\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PmOXDK766-2U"},"source":["##### Expected Output\n","\n","\n","```CPP\n","iters: 10 cost: 0.789141\n","iters: 20 cost: 0.105543\n","iters: 30 cost: 0.056008\n","iters: 40 cost: 0.038101\n","iters: 50 cost: 0.028868\n","iters: 60 cost: 0.023237\n","iters: 70 cost: 0.019444\n","iters: 80 cost: 0.016716\n","iters: 90 cost: 0.014660\n","iters: 100 cost: 0.013054\n","iters: 110 cost: 0.012133\n","iters: 120 cost: 0.011370\n","iters: 130 cost: 0.010698\n","iters: 140 cost: 0.010100\n","iters: 150 cost: 0.009566\n","```\n","\n","Your numbers may differ a bit depending on which version of Python you're using."]}]}