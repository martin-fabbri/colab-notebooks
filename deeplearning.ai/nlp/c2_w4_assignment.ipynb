{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c2_w4_assignment.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMT4m8Xjw6xli6tmIoXlUhN"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HHTijOy1JFqz"},"source":["# Assignment 4: Word Embeddings \n","\n","Welcome to the fourth (and last) programming assignment of Course 2! \n","\n","In this assignment, you will practice how to compute word embeddings and use them for sentiment analysis.\n","- To implement sentiment analysis, you can go beyond counting the number of positive words and negative words. \n","- You can find a way to represent each word numerically, by a vector. \n","- The vector could then represent syntactic (i.e. parts of speech) and semantic (i.e. meaning) structures. \n","\n","In this assignment, you will explore a classic way of generating word embeddings or representations.\n","- You will implement a famous model called the continuous bag of words (CBOW) model. \n","\n","By completing this assignment you will:\n","\n","- Train word vectors from scratch.\n","- Learn how to create batches of data.\n","- Understand how backpropagation works.\n","- Plot and visualize your learned word vectors.\n","\n","Knowing how to train these models will give you a better understanding of word vectors, which are building blocks to many applications in natural language processing."]},{"cell_type":"markdown","metadata":{"id":"Wp9WAWNYJWkT"},"source":["## Outline\n","\n","- [1 The Continuous bag of words model](#1)\n","- [2 Training the Model](#2)\n","    - [2.0 Initialize the model](#2)\n","        - [Exercise 01](#ex-01)\n","    - [2.1 Softmax Function](#2.1)\n","        - [Exercise 02](#ex-02)\n","    - [2.2 Forward Propagation](#2.2)\n","        - [Exercise 03](#ex-03)\n","    - [2.3 Cost Function](#2.3)\n","    - [2.4 Backproagation](#2.4)\n","        - [Exercise 04](#ex-04)\n","    - [2.5 Gradient Descent](#2.5)\n","        - [Exercise 05](#ex-05)\n","- [3 Visualizing the word vectors](#3)"]},{"cell_type":"markdown","metadata":{"id":"pu_c3yDKJdOK"},"source":["<a name='1'></a>\n","# 1. The Continuous bag of words model\n","\n","Let's take a look at the following sentence: \n",">**'I am happy because I am learning'**. \n","\n","- In continuous bag of words (CBOW) modeling, we try to predict the center word given a few context words (the words around the center word).\n","- For example, if you were to choose a context half-size of say $C = 2$, then you would try to predict the word **happy** given the context that includes 2 words before and 2 words after the center word:\n","\n","> $C$ words before: [I, am] \n","\n","> $C$ words after: [because, I] \n","\n","- In other words:\n","\n","$$context = [I,am, because, I]$$\n","$$target = happy$$\n","\n","The structure of your model will look like this:\n","\n","<img src='.png' alt=\"alternate text\" width=\"width\"/>\n","\n","Where $\\bar x$ is the average of all the one hot vectors of the context words. \n","\n","<img src='mean_vec2.png' alt=\"alternate text\" width=\"width\"/>\n","\n","Once you have encoded all the context words, you can use $\\bar x$ as the input to your model. \n","\n","The architecture you will be implementing is as follows:\n","\n","\\begin{align}\n"," h &= W_1 \\  X + b_1  \\tag{1} \\\\\n"," a &= ReLU(h)  \\tag{2} \\\\\n"," z &= W_2 \\  a + b_2   \\tag{3} \\\\\n"," \\hat y &= softmax(z)   \\tag{4} \\\\\n","\\end{align}"]},{"cell_type":"code","metadata":{"id":"MNpEXreZJSKS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gnRCYGOwJTVv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tkpjUc__JTTV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rcsv7RTDJTQd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pHoRsbbfJTNs"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KYftRR-DJTKp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J-7hZQdAJTG3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"039auXetJS9j"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sg28Dou2HNwP"},"source":[""],"execution_count":null,"outputs":[]}]}