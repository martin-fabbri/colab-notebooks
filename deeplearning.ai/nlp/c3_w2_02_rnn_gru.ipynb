{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c3_w2_02_rnn_gru.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMojD24u/VNjdMjnPZvyiT4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ao8Q3xCGHAnk"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\r\n","  <td>\r\n","    <a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/nlp/c3_w2_02_rnn_gru.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>    \r\n","  </td>\r\n","  <td>\r\n","    <a href=\"https://github.com/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/nlp/c3_w2_02_rnn_gru.ipynb\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/martin-fabbri/colab-notebooks/master/assets/github.svg\" alt=\"View On Github\"/></a>  </td>\r\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"pDuptD2CGrNR"},"source":["# Vanilla RNNs, GRUs and the `scan` function"]},{"cell_type":"markdown","metadata":{"id":"OgOJTHPzGtZe"},"source":["In this notebook, you will learn how to define the forward method for vanilla RNNs and GRUs. Additionally, you will see how to define and use the function `scan` to compute forward propagation for RNNs.\r\n","\r\n","By completing this notebook, you will:\r\n","\r\n","- Be able to define the forward method for vanilla RNNs and GRUs\r\n","- Be able to define the `scan` function to perform forward propagation for RNNs\r\n","- Understand how forward propagation is implemented for RNNs."]},{"cell_type":"code","metadata":{"id":"gU95xdJr_FAy","executionInfo":{"status":"ok","timestamp":1611705450260,"user_tz":480,"elapsed":337,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["import numpy as np\r\n","from numpy import random\r\n","from time import perf_counter"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6T6IZyFcIZGH"},"source":["An implementation of the `sigmoid` function is provided below so you can use it in this notebook."]},{"cell_type":"code","metadata":{"id":"HFgm6VDwKZUI","executionInfo":{"status":"ok","timestamp":1611706084395,"user_tz":480,"elapsed":384,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["def sigmoid(x):\r\n","    return 1.0 / (1.0 + np.exp(-x))"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oyp1v2grKzwW"},"source":["# Part 1: Forward method for vanilla RNNs and GRUs"]},{"cell_type":"markdown","metadata":{"id":"ij1KBuboK3MW"},"source":["In this part of the notebook, you'll see the implementation of the forward method for a vanilla RNN and you'll implement that same method for a GRU. For this excersice you'll use a set of random weights and variables with the following dimensions:\r\n","\r\n","- Embedding size (`emb`) : 128\r\n","- Hidden state size (`h_dim`) : (16,1)\r\n","\r\n","The weights `w_` and biases `b_` are initialized with dimensions (`h_dim`, `emb + h_dim`) and (`h_dim`, 1). We expect the hidden state `h_t` to be a column vector with size (`h_dim`,1) and the initial hidden state `h_0` is a vector of zeros."]},{"cell_type":"code","metadata":{"id":"Dy34J1v8KzVI","executionInfo":{"status":"ok","timestamp":1611710238512,"user_tz":480,"elapsed":584,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["random.seed(10)\r\n","emb = 128\r\n","T = 256\r\n","h_dim = 16\r\n","h_0 = np.zeros((h_dim, 1))\r\n","w1 = random.standard_normal((h_dim, emb + h_dim))\r\n","w2 = random.standard_normal((h_dim, emb + h_dim))\r\n","w3 = random.standard_normal((h_dim, emb + h_dim))\r\n","\r\n","b1 = random.standard_normal((h_dim, 1))\r\n","b2 = random.standard_normal((h_dim, 1))\r\n","b3 = random.standard_normal((h_dim, 1))\r\n","\r\n","X = random.standard_normal((T, emb, 1))\r\n","weights = [w1, w2, w3, b1, b2, b3]"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rs6MTv5War_q"},"source":["## 1.1 Forward method for vanilla RNNs"]},{"cell_type":"markdown","metadata":{"id":"4UnWLtlOc7Ml"},"source":["The vanilla RNN cell is quite straight forward. Its most general structure is presented in the next figure: \r\n","\r\n","<img src=\"\" width=\"400\"/>\r\n","\r\n","As you saw in the lecture videos, the computations made in a vanilla RNN cell are equivalent to the following equations:\r\n","\r\n","$$h^{<t>}=g(W_{h}[h^{<t-1>},x^{<t>}] + b_h)$$\r\n","\r\n","$$\\hat{y}^{<t>}=g(W_{yh}h^{<t>} + b_y)$$\r\n","\r\n","where $[h^{<t-1>},x^{<t>}]$ means that $h^{<t-1>}$ and $x^{<t>}$ are concatenated together. In the next cell we provide the implementation of the forward method for a vanilla RNN. "]},{"cell_type":"code","metadata":{"id":"N8NGc3oNam4l"},"source":[""],"execution_count":null,"outputs":[]}]}