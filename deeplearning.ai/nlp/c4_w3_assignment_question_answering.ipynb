{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c4_w3_assignment_question_answering.ipynb","provenance":[],"collapsed_sections":["3niTYw2wGbB5","U4hwy9DTGdbR","dcOcxUqzGgAr"],"authorship_tag":"ABX9TyOLiHLQMlPRbzlFD9U9w6uY"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3niTYw2wGbB5"},"source":["# Assignment 3: Question Answering\r\n","\r\n","Welcome to this week's assignment of course 4. In this you will explore question answering. You will implement the \"Text to Text Transfer from Transformers\" (better known as T5). Since you implemented transformers from scratch last week you will now be able to use them. \r\n","\r\n","<img src=\"https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/images/w4_t5_qa.png\" width=\"300px\"> "]},{"cell_type":"markdown","metadata":{"id":"U4hwy9DTGdbR"},"source":["## Outline\r\n","\r\n","- [Overview](#0)\r\n","- [Part 0: Importing the Packages](#0)\r\n","- [Part 1: C4 Dataset](#1)\r\n","    - [1.1 Pre-Training Objective](#1.1)\r\n","    - [1.2 Process C4](#1.2)\r\n","        - [1.2.1 Decode to natural language](#1.2.1)\r\n","    - [1.3 Tokenizing and Masking](#1.3)\r\n","        - [Exercise 01](#ex01)\r\n","    - [1.4 Creating the Pairs](#1.4)\r\n","- [Part 2: Transfomer](#2)\r\n","    - [2.1 Transformer Encoder](#2.1)\r\n","        - [2.1.1 The Feedforward Block](#2.1.1)\r\n","            - [Exercise 02](#ex02)\r\n","        - [2.1.2 The Encoder Block](#2.1.2)\r\n","            - [Exercise 03](#ex03)\r\n","        - [2.1.3 The Transformer Encoder](#2.1.3)            \r\n","            - [Exercise 04](#ex04)"]},{"cell_type":"markdown","metadata":{"id":"dcOcxUqzGgAr"},"source":["<a name='0'></a>\r\n","### Overview\r\n","\r\n","This assignment will be different from the two previous ones. Due to memory and time constraints of this environment you will not be able to train a model and use it for inference. Instead you will create the necessary building blocks for the transformer encoder model and will use a pretrained version of the same model in two ungraded labs after this assignment.\r\n","\r\n","After completing these 3 (1 graded and 2 ungraded) labs you will:\r\n","* Implement the code neccesary for Bidirectional Encoder Representation from Transformer (BERT).\r\n","* Understand how the C4 dataset is structured.\r\n","* Use a pretrained model for inference.\r\n","* Understand how the \"Text to Text Transfer from Transformers\" or T5 model works. "]},{"cell_type":"markdown","metadata":{"id":"9dGvSjhwGiVF"},"source":["<a name='0'></a>\r\n","# Part 0: Importing the Packages"]},{"cell_type":"code","metadata":{"id":"nryw5Mk0HPbO","executionInfo":{"status":"ok","timestamp":1613263021109,"user_tz":480,"elapsed":15998,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["%%capture\r\n","!pip install trax\r\n","!wget https://raw.githubusercontent.com/martin-fabbri/colab-notebooks/master/deeplearning.ai/nlp/assets/bpe_data.txt\r\n","!wget https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/pre-trained-models/w4_t5_sentencepiece.model"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLBTL2JrF4F5","executionInfo":{"status":"ok","timestamp":1613263062942,"user_tz":480,"elapsed":57826,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["import ast\r\n","import string\r\n","import textwrap\r\n","import itertools\r\n","import numpy as np\r\n","\r\n","import trax\r\n","from trax import layers as tl\r\n","from trax.supervised import decoding\r\n","\r\n","# Will come handy later.\r\n","wrapper = textwrap.TextWrapper(width=70)\r\n","\r\n","# Set random seed\r\n","np.random.seed(42)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nb81Im_oI6r-","executionInfo":{"status":"ok","timestamp":1613263063644,"user_tz":480,"elapsed":58511,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"c0e03ad7-9cc2-4366-93df-6f58d2767e44"},"source":["!pip list | grep trax"],"execution_count":4,"outputs":[{"output_type":"stream","text":["trax                          1.3.7                \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ps4x47h5Qnay"},"source":["<a name='1'></a>\r\n","## Part 1: C4 Dataset\r\n","\r\n","The [C4](https://www.tensorflow.org/datasets/catalog/c4) is a huge data set. For the purpose of this assignment you will use a few examples out of it which are present in `data.txt`. C4 is based on the [common crawl](https://commoncrawl.org/) project. Feel free to read more on their website. \r\n","\r\n","Run the cell below to see how the examples look like. "]},{"cell_type":"code","metadata":{"id":"QzhC3DJfGna2","executionInfo":{"status":"ok","timestamp":1613263063645,"user_tz":480,"elapsed":58509,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# load example jsons\r\n","example_jsons = list(map(ast.literal_eval, open(\"bpe_data.txt\")))"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ccK72VkcXSR7","executionInfo":{"status":"ok","timestamp":1613263063646,"user_tz":480,"elapsed":58503,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"fbf36a78-5dbe-48bf-b00b-ccfe8fc37981"},"source":["for i in range(5):\r\n","    print(f\"example number {i+5}: \\n\\n{example_jsons[i]} \\n\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["example number 5: \n","\n","{'content-length': b'1970', 'content-type': b'text/plain', 'text': b'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.', 'timestamp': b'2019-04-25T12:57:54Z', 'url': b'https://klyq.com/beginners-bbq-class-taking-place-in-missoula/'} \n","\n","example number 6: \n","\n","{'content-length': b'12064', 'content-type': b'text/plain', 'text': b'Discussion in \\'Mac OS X Lion (10.7)\\' started by axboi87, Jan 20, 2012.\\nI\\'ve got a 500gb internal drive and a 240gb SSD.\\nWhen trying to restore using disk utility i\\'m given the error \"Not enough space on disk ____ to restore\"\\nBut I shouldn\\'t have to do that!!!\\nAny ideas or workarounds before resorting to the above?\\nUse Carbon Copy Cloner to copy one drive to the other. I\\'ve done this several times going from larger HDD to smaller SSD and I wound up with a bootable SSD drive. One step you have to remember not to skip is to use Disk Utility to partition the SSD as GUID partition scheme HFS+ before doing the clone. If it came Apple Partition Scheme, even if you let CCC do the clone, the resulting drive won\\'t be bootable. CCC usually works in \"file mode\" and it can easily copy a larger drive (that\\'s mostly empty) onto a smaller drive. If you tell CCC to clone a drive you did NOT boot from, it can work in block copy mode where the destination drive must be the same size or larger than the drive you are cloning from (if I recall).\\nI\\'ve actually done this somehow on Disk Utility several times (booting from a different drive (or even the dvd) so not running disk utility from the drive your cloning) and had it work just fine from larger to smaller bootable clone. Definitely format the drive cloning to first, as bootable Apple etc..\\nThanks for pointing this out. My only experience using DU to go larger to smaller was when I was trying to make a Lion install stick and I was unable to restore InstallESD.dmg to a 4 GB USB stick but of course the reason that wouldn\\'t fit is there was slightly more than 4 GB of data.', 'timestamp': b'2019-04-21T10:07:13Z', 'url': b'https://forums.macrumors.com/threads/restore-from-larger-disk-to-smaller-disk.1311329/'} \n","\n","example number 7: \n","\n","{'content-length': b'5235', 'content-type': b'text/plain', 'text': b'Foil plaid lycra and spandex shortall with metallic slinky insets. Attached metallic elastic belt with O-ring. Headband included. Great hip hop or jazz dance costume. Made in the USA.', 'timestamp': b'2019-04-25T10:40:23Z', 'url': b'https://awishcometrue.com/Catalogs/Clearance/Tweens/V1960-Find-A-Way'} \n","\n","example number 8: \n","\n","{'content-length': b'4967', 'content-type': b'text/plain', 'text': b\"How many backlinks per day for new site?\\nDiscussion in 'Black Hat SEO' started by Omoplata, Dec 3, 2010.\\n1) for a newly created site, what's the max # backlinks per day I should do to be safe?\\n2) how long do I have to let my site age before I can start making more blinks?\\nI did about 6000 forum profiles every 24 hours for 10 days for one of my sites which had a brand new domain.\\nThere is three backlinks for every of these forum profile so thats 18 000 backlinks every 24 hours and nothing happened in terms of being penalized or sandboxed. This is now maybe 3 months ago and the site is ranking on first page for a lot of my targeted keywords.\\nbuild more you can in starting but do manual submission and not spammy type means manual + relevant to the post.. then after 1 month you can make a big blast..\\nWow, dude, you built 18k backlinks a day on a brand new site? How quickly did you rank up? What kind of competition/searches did those keywords have?\", 'timestamp': b'2019-04-21T12:46:19Z', 'url': b'https://www.blackhatworld.com/seo/how-many-backlinks-per-day-for-new-site.258615/'} \n","\n","example number 9: \n","\n","{'content-length': b'4499', 'content-type': b'text/plain', 'text': b'The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what\\xe2\\x80\\x99s included in the mill levy measure.', 'timestamp': b'2019-04-20T14:33:21Z', 'url': b'http://bond.dpsk12.org/category/news/'} \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z82BOSOMDELU"},"source":["Notice the `b` before each string? This means that this data comes as bytes rather than strings. Strings are actually lists of bytes so for the rest of the assignments the name `strings` will be used to describe the data. \r\n","\r\n","To check this run the following cell:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IuugTmsM9wlO","executionInfo":{"status":"ok","timestamp":1613263063647,"user_tz":480,"elapsed":58497,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"708fcc22-35fb-4482-f6db-08ecac3e369d"},"source":["type(example_jsons[0].get('text'))"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["bytes"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uJ1d3u8oDeUc","executionInfo":{"status":"ok","timestamp":1613263063647,"user_tz":480,"elapsed":58490,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"19d4b14a-d798-411b-ae4f-34124e83fe51"},"source":["example_jsons[0].get('text')"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["b'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"QAnwjZu9EMM4"},"source":["<a name='1.1'></a>\r\n","###  1.1 Pre-Training Objective\r\n","\r\n","**Note:** The word \"mask\" will be used throughout this assignment in context of hiding/removing word(s)\r\n","\r\n","You will be implementing the BERT loss as shown in the following image. \r\n","\r\n","<img src=\"https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/images/w4_t5_loss.png\" width=\"600px\" height = \"400px\">\r\n","\r\n","Assume you have the following text: <span style = \"color:blue\"> **Thank you <span style = \"color:red\">for inviting </span> me to your party <span style = \"color:red\">last</span>  week** </span> \r\n","\r\n","\r\n","Now as input you will mask the words in red in the text: \r\n","\r\n","<span style = \"color:blue\"> **Input:**</span> Thank you  **X** me to your party **Y** week.\r\n","\r\n","<span style = \"color:blue\">**Output:**</span> The model should predict the words(s) for **X** and **Y**. \r\n","\r\n","**Z** is used to represent the end."]},{"cell_type":"markdown","metadata":{"id":"mxteCV2hHz0i"},"source":["<a name='1.2'></a>\r\n","### 1.2 Process C4\r\n","\r\n","C4 only has the plain string `text` field, so you will tokenize and have `inputs` and `targets` out of it for supervised learning. Given your inputs, the goal is to predict the targets during training. \r\n","\r\n","You will now take the `text` and convert it to `inputs` and `targets`."]},{"cell_type":"code","metadata":{"id":"uFV1SJVzDeRJ","executionInfo":{"status":"ok","timestamp":1613263063648,"user_tz":480,"elapsed":58488,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["natural_language_texts = [example_json[\"text\"] for example_json in example_jsons]"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xxB11NeMDeM4","executionInfo":{"status":"ok","timestamp":1613263063649,"user_tz":480,"elapsed":58483,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"2df160ba-225d-4ac4-9a39-4dd5ecda02e9"},"source":["natural_language_texts[4]"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["b'The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what\\xe2\\x80\\x99s included in the mill levy measure.'"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"pBqe3kLGJMul"},"source":["<a name='1.2.1'></a>\r\n","#### 1.2.1 Decode to natural language\r\n","\r\n","The following functions will help you `detokenize` and`tokenize` the text data.  \r\n","\r\n","The `sentencepiece` vocabulary was used to convert from text to ids. This vocabulary file is loaded and used in this helper functions.\r\n","\r\n","`natural_language_texts` has the text from the examples we gave you. \r\n","\r\n","Run the cells below to see what is going on. "]},{"cell_type":"code","metadata":{"id":"8gDP09TLDeI-","executionInfo":{"status":"ok","timestamp":1613263063651,"user_tz":480,"elapsed":58482,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["PAD, EOS, UNK = 0, 1, 2\r\n","\r\n","\r\n","def detokenize(np_array):\r\n","    return trax.data.detokenize(\r\n","        np_array,\r\n","        vocab_type=\"sentencepiece\",\r\n","        vocab_file=\"w4_t5_sentencepiece.model\",\r\n","        vocab_dir=\".\",\r\n","    )\r\n","\r\n","\r\n","def tokenize(s):\r\n","    # The trax.data.tokenize function operates on streams,\r\n","    # that's why we have to create 1-element stream with iter\r\n","    # and later retrieve the result with next.\r\n","    return next(\r\n","        trax.data.tokenize(\r\n","            iter([s]),\r\n","            vocab_type=\"sentencepiece\",\r\n","            vocab_file=\"w4_t5_sentencepiece.model\",\r\n","            vocab_dir=\".\",\r\n","        )\r\n","    )"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Geq05jbYNHmN","executionInfo":{"status":"ok","timestamp":1613263071268,"user_tz":480,"elapsed":66093,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"92614ea5-f09e-47ba-f0f5-c49c7b7be9f2"},"source":["# printing the encoding of each word to see how subwords are tokenized\r\n","tokenized_text = [(tokenize(word).tolist(), word) for word in natural_language_texts[0].split()]\r\n","print(tokenized_text, '\\n')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["[([12847, 277], b'Beginners'), ([15068], b'BBQ'), ([4501], b'Class'), ([3, 12297], b'Taking'), ([3399], b'Place'), ([16], b'in'), ([5964, 7115, 9, 55], b'Missoula!'), ([531], b'Do'), ([25], b'you'), ([241], b'want'), ([12], b'to'), ([129], b'get'), ([394], b'better'), ([44], b'at'), ([492], b'making'), ([3326], b'delicious'), ([15068, 58], b'BBQ?'), ([148], b'You'), ([56], b'will'), ([43], b'have'), ([8], b'the'), ([1004, 6], b'opportunity,'), ([474], b'put'), ([48], b'this'), ([30], b'on'), ([39], b'your'), ([4793], b'calendar'), ([230, 5], b'now.'), ([2721, 6], b'Thursday,'), ([1600], b'September'), ([1630, 727], b'22nd'), ([1715], b'join'), ([1150], b'World'), ([4501], b'Class'), ([15068], b'BBQ'), ([16127, 6], b'Champion,'), ([9137], b'Tony'), ([2659, 5595], b'Balay'), ([45], b'from'), ([301, 782, 3624], b'Lonestar'), ([14627, 15], b'Smoke'), ([12612, 277, 5], b'Rangers.'), ([216], b'He'), ([56], b'will'), ([36], b'be'), ([2119], b'teaching'), ([3, 9], b'a'), ([19529], b'beginner'), ([593], b'level'), ([853], b'class'), ([21], b'for'), ([921], b'everyone'), ([113], b'who'), ([2746], b'wants'), ([12], b'to'), ([129], b'get'), ([394], b'better'), ([28], b'with'), ([70], b'their'), ([17712], b'culinary'), ([1098, 5], b'skills.'), ([216], b'He'), ([56], b'will'), ([3884], b'teach'), ([25], b'you'), ([762], b'everything'), ([25], b'you'), ([174], b'need'), ([12], b'to'), ([214], b'know'), ([12], b'to'), ([5978], b'compete'), ([16], b'in'), ([3, 9], b'a'), ([3, 23405, 4547], b'KCBS'), ([15068], b'BBQ'), ([2259, 6], b'competition,'), ([379], b'including'), ([2097, 6], b'techniques,'), ([5459, 6], b'recipes,'), ([13618, 7, 6], b'timelines,'), ([3604], b'meat'), ([1801], b'selection'), ([11], b'and'), ([27856, 6], b'trimming,'), ([303], b'plus'), ([24190], b'smoker'), ([11], b'and'), ([1472], b'fire'), ([251, 5], b'information.'), ([37], b'The'), ([583], b'cost'), ([12], b'to'), ([36], b'be'), ([16], b'in'), ([8], b'the'), ([853], b'class'), ([19], b'is'), ([25264], b'$35'), ([399], b'per'), ([568, 6], b'person,'), ([11], b'and'), ([21], b'for'), ([21380, 7], b'spectators'), ([34], b'it'), ([19], b'is'), ([339, 5], b'free.'), ([15746, 26], b'Included'), ([16], b'in'), ([8], b'the'), ([583], b'cost'), ([56], b'will'), ([36], b'be'), ([893], b'either'), ([3, 9], b'a'), ([3, 17, 18, 9486], b't-shirt'), ([42], b'or'), ([3, 9, 1409, 29], b'apron'), ([11], b'and'), ([25], b'you'), ([56], b'will'), ([36], b'be'), ([12246], b'tasting'), ([5977], b'samples'), ([13], b'of'), ([284], b'each'), ([3604], b'meat'), ([24], b'that'), ([19], b'is'), ([2657, 5], b'prepared.')] \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WkiWRHPaNHc2","executionInfo":{"status":"ok","timestamp":1613263071489,"user_tz":480,"elapsed":66308,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"2f6ff994-f7e0-4f30-b5f4-50fd67ca4a66"},"source":["print(f\"tokenized: {tokenize('Beginners')}\\ndetokenized: {detokenize(tokenize('Beginners'))}\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["tokenized: [12847   277]\n","detokenized: Beginners\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"effvy5I8zljH"},"source":["As you can see above, you were able to take a piece of string and tokenize it. \r\n","\r\n","Now you will create `input` and `target` pairs that will allow you to train your model. T5 uses the ids at the end of the vocab file as sentinels. For example, it will replace: \r\n","   - `vocab_size - 1` by `<Z>`\r\n","   - `vocab_size - 2` by `<Y>`\r\n","   - and so forth. \r\n","   \r\n","It assigns every word a `chr`.\r\n","\r\n","The `pretty_decode` function below, which you will use in a bit, helps in handling the type when decoding. Take a look and try to understand what the function is doing.\r\n","\r\n","\r\n","Notice that:\r\n","```python\r\n","string.ascii_letters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\r\n","```\r\n","\r\n","**NOTE:** Targets may have more than the 52 sentinels we replace, but this is just to give you an idea of things."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fDndB4YUzg8u","executionInfo":{"status":"ok","timestamp":1613263071490,"user_tz":480,"elapsed":66303,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"9a542f33-4197-4c08-95f6-c291e4f2ce7d"},"source":["vocab_size = trax.data.vocab_size(\r\n","    vocab_type=\"sentencepiece\",\r\n","    vocab_file=\"w4_t5_sentencepiece.model\",\r\n","    vocab_dir=\".\"\r\n",")\r\n","\r\n","vocab_size"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["32000"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"BQzkHVKCzg50","executionInfo":{"status":"ok","timestamp":1613263071491,"user_tz":480,"elapsed":66299,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["def get_sentinels(vocab_size=vocab_size, display=False):\r\n","    sentinels = {}\r\n","    for i, char in enumerate(reversed(string.ascii_letters), 1):\r\n","        decoded_text = detokenize([vocab_size - i])\r\n","\r\n","        # sentinels, ex <Z> - <a>\r\n","        sentinels[decoded_text] = f\"<{char}>\"\r\n","\r\n","        if display:\r\n","            print(f\"The sentinel is <{char}> and the decoded token is\", decoded_text)\r\n","    return sentinels   "],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QImBnseZzg28","executionInfo":{"status":"ok","timestamp":1613263075077,"user_tz":480,"elapsed":69879,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"92fd4555-dccf-4718-8db5-60d962010931"},"source":["sentinels = get_sentinels(vocab_size, display=True)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["The sentinel is <Z> and the decoded token is Internațional\n","The sentinel is <Y> and the decoded token is erwachsene\n","The sentinel is <X> and the decoded token is Cushion\n","The sentinel is <W> and the decoded token is imunitar\n","The sentinel is <V> and the decoded token is Intellectual\n","The sentinel is <U> and the decoded token is traditi\n","The sentinel is <T> and the decoded token is disguise\n","The sentinel is <S> and the decoded token is exerce\n","The sentinel is <R> and the decoded token is nourishe\n","The sentinel is <Q> and the decoded token is predominant\n","The sentinel is <P> and the decoded token is amitié\n","The sentinel is <O> and the decoded token is erkennt\n","The sentinel is <N> and the decoded token is dimension\n","The sentinel is <M> and the decoded token is inférieur\n","The sentinel is <L> and the decoded token is refugi\n","The sentinel is <K> and the decoded token is cheddar\n","The sentinel is <J> and the decoded token is unterlieg\n","The sentinel is <I> and the decoded token is garanteaz\n","The sentinel is <H> and the decoded token is făcute\n","The sentinel is <G> and the decoded token is réglage\n","The sentinel is <F> and the decoded token is pedepse\n","The sentinel is <E> and the decoded token is Germain\n","The sentinel is <D> and the decoded token is distinctly\n","The sentinel is <C> and the decoded token is Schraub\n","The sentinel is <B> and the decoded token is emanat\n","The sentinel is <A> and the decoded token is trimestre\n","The sentinel is <z> and the decoded token is disrespect\n","The sentinel is <y> and the decoded token is Erasmus\n","The sentinel is <x> and the decoded token is Australia\n","The sentinel is <w> and the decoded token is permeabil\n","The sentinel is <v> and the decoded token is deseori\n","The sentinel is <u> and the decoded token is manipulated\n","The sentinel is <t> and the decoded token is suggér\n","The sentinel is <s> and the decoded token is corespund\n","The sentinel is <r> and the decoded token is nitro\n","The sentinel is <q> and the decoded token is oyons\n","The sentinel is <p> and the decoded token is Account\n","The sentinel is <o> and the decoded token is échéan\n","The sentinel is <n> and the decoded token is laundering\n","The sentinel is <m> and the decoded token is genealogy\n","The sentinel is <l> and the decoded token is QuickBooks\n","The sentinel is <k> and the decoded token is constituted\n","The sentinel is <j> and the decoded token is Fertigung\n","The sentinel is <i> and the decoded token is goutte\n","The sentinel is <h> and the decoded token is regulă\n","The sentinel is <g> and the decoded token is overwhelmingly\n","The sentinel is <f> and the decoded token is émerg\n","The sentinel is <e> and the decoded token is broyeur\n","The sentinel is <d> and the decoded token is povești\n","The sentinel is <c> and the decoded token is emulator\n","The sentinel is <b> and the decoded token is halloween\n","The sentinel is <a> and the decoded token is combustibil\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZXtH4j_-zgz6","executionInfo":{"status":"ok","timestamp":1613263075079,"user_tz":480,"elapsed":69866,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["def pretty_decode(encoded_str_list, sentinels=sentinels):\r\n","    if isinstance(encoded_str_list, (str, bytes)):\r\n","        for token, char in sentinels.items():\r\n","            encoded_str_list = encoded_str_list.replace(token, char)\r\n","        return encoded_str_list\r\n","\r\n","    return pretty_decode(detokenize(encoded_str_list))"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"aBJmB1ZPzgww","executionInfo":{"status":"ok","timestamp":1613263075080,"user_tz":480,"elapsed":69864,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["pretty_decode(\"I want to dress up as an Intellectual this halloween.\")"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FJ4aYNsN-EXS"},"source":["The functions above make your `inputs` and `targets` more readable. For example, you might see something like this once you implement the masking function below. \r\n","\r\n","- <span style=\"color:red\"> Input sentence: </span> Younes and Lukasz were working together in the lab yesterday after lunch. \r\n","- <span style=\"color:red\">Input: </span> Younes and Lukasz  **Z** together in the **Y** yesterday after lunch.\r\n","- <span style=\"color:red\">Target: </span> **Z** were working **Y** lab."]},{"cell_type":"markdown","metadata":{"id":"Y5u4qB_n-Hfk"},"source":["<a name='1.3'></a>\r\n","### 1.3 Tokenizing and Masking\r\n","\r\n","You will now implement the `tokenize_and_mask` function. This function  will allow you to tokenize and mask input words with a noise probability. We usually mask 15% of the words."]},{"cell_type":"markdown","metadata":{"id":"l8EQn40e-Jka"},"source":["<a name='ex01'></a>\r\n","### Exercise 01"]},{"cell_type":"code","metadata":{"id":"5IOl9Pm6zguB","executionInfo":{"status":"ok","timestamp":1613264610127,"user_tz":480,"elapsed":369,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# UNQ_C1\r\n","# GRADED FUNCTION: tokenize_and_mask\r\n","def tokenize_and_mask(text, vocab_size=vocab_size, noise=0.15, \r\n","                      randomizer=np.random.uniform, tokenize=tokenize):\r\n","    \"\"\"Tokenizes and masks a given input.\r\n","\r\n","    Args:\r\n","        text (str or bytes): Text input.\r\n","        vocab_size (int, optional): Size of the vocabulary. Defaults to vocab_size.\r\n","        noise (float, optional): Probability of masking a token. Defaults to 0.15.\r\n","        randomizer (function, optional): Function that generates random values. Defaults to np.random.uniform.\r\n","        tokenize (function, optional): Tokenizer function. Defaults to tokenize.\r\n","\r\n","    Returns:\r\n","        tuple: Tuple of lists of integers associated to inputs and targets.\r\n","    \"\"\"\r\n","    \r\n","    # current sentinel number (starts at 0)\r\n","    cur_sentinel_num = 0\r\n","    # inputs\r\n","    inps = []\r\n","    # targets\r\n","    targs = []\r\n","    \r\n","    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\r\n","    \r\n","    # prev_no_mask is True if the previous token was NOT masked, False otherwise\r\n","    # set prev_no_mask to True\r\n","    prev_no_mask = True\r\n","    \r\n","    # loop through tokenized `text`\r\n","    for token in tokenize(text):\r\n","        # check if the `noise` is greater than a random value (weighted coin flip)\r\n","        if randomizer() < noise:\r\n","            # check to see if the previous token was not masked\r\n","            if prev_no_mask==True: # add new masked token at end_id\r\n","                # number of masked tokens increases by 1\r\n","                cur_sentinel_num += 1\r\n","                # compute `end_id` by subtracting current sentinel value out of the total vocabulary size\r\n","                end_id = vocab_size - cur_sentinel_num\r\n","                # append `end_id` at the end of the targets\r\n","                targs.append(end_id)\r\n","                # append `end_id` at the end of the inputs\r\n","                inps.append(end_id)\r\n","            # append `token` at the end of the targets\r\n","            targs.append(token)\r\n","            # set prev_no_mask accordingly\r\n","            prev_no_mask = False\r\n","        \r\n","        else: # don't have two masked tokens in a row\r\n","            # append `token ` at the end of the inputs\r\n","            inps.append(token)\r\n","            # set prev_no_mask accordingly\r\n","            prev_no_mask = True\r\n","            \r\n","    ### END CODE HERE ###\r\n","    \r\n","    return inps, targs"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"yrhpbzsQzgq3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613264611738,"user_tz":480,"elapsed":373,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"74f4aabd-37b7-4003-8761-b1e544f72e60"},"source":["# Some logic to mock a np.random value generator\r\n","# Needs to be in the same cell for it to always generate same output\r\n","def testing_rnd():\r\n","    def dummy_generator():\r\n","        vals = np.linspace(0, 1, 10)\r\n","        cyclic_vals = itertools.cycle(vals)\r\n","        for _ in range(100):\r\n","            yield next(cyclic_vals)\r\n","\r\n","    dumr = itertools.cycle(dummy_generator())\r\n","\r\n","    def dummy_randomizer():\r\n","        return next(dumr)\r\n","    \r\n","    return dummy_randomizer\r\n","\r\n","input_str = natural_language_texts[0]\r\n","print(f\"input string:\\n\\n{input_str}\\n\")\r\n","inps, targs = tokenize_and_mask(input_str, randomizer=testing_rnd())\r\n","print(f\"tokenized inputs:\\n\\n{inps}\\n\")\r\n","print(f\"targets:\\n\\n{targs}\")"],"execution_count":19,"outputs":[{"output_type":"stream","text":["input string:\n","\n","b'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.'\n","\n","tokenized inputs:\n","\n","[31999, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 31998, 531, 25, 241, 12, 129, 394, 44, 492, 31997, 58, 148, 56, 43, 8, 1004, 6, 474, 31996, 39, 4793, 230, 5, 2721, 6, 1600, 1630, 31995, 1150, 4501, 15068, 16127, 6, 9137, 2659, 5595, 31994, 782, 3624, 14627, 15, 12612, 277, 5, 216, 31993, 2119, 3, 9, 19529, 593, 853, 21, 921, 31992, 12, 129, 394, 28, 70, 17712, 1098, 5, 31991, 3884, 25, 762, 25, 174, 12, 214, 12, 31990, 3, 9, 3, 23405, 4547, 15068, 2259, 6, 31989, 6, 5459, 6, 13618, 7, 6, 3604, 1801, 31988, 6, 303, 24190, 11, 1472, 251, 5, 37, 31987, 36, 16, 8, 853, 19, 25264, 399, 568, 31986, 21, 21380, 7, 34, 19, 339, 5, 15746, 31985, 8, 583, 56, 36, 893, 3, 9, 3, 31984, 9486, 42, 3, 9, 1409, 29, 11, 25, 31983, 12246, 5977, 13, 284, 3604, 24, 19, 2657, 31982]\n","\n","targets:\n","\n","[31999, 12847, 277, 31998, 9, 55, 31997, 3326, 15068, 31996, 48, 30, 31995, 727, 1715, 31994, 45, 301, 31993, 56, 36, 31992, 113, 2746, 31991, 216, 56, 31990, 5978, 16, 31989, 379, 2097, 31988, 11, 27856, 31987, 583, 12, 31986, 6, 11, 31985, 26, 16, 31984, 17, 18, 31983, 56, 36, 31982, 5]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mpBlzE6HELFs"},"source":["#### **Expected Output:**\r\n","```CPP\r\n","b'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.'\r\n","\r\n","tokenized inputs:\r\n","\r\n","[31999, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 31998, 531, 25, 241, 12, 129, 394, 44, 492, 31997, 58, 148, 56, 43, 8, 1004, 6, 474, 31996, 39, 4793, 230, 5, 2721, 6, 1600, 1630, 31995, 1150, 4501, 15068, 16127, 6, 9137, 2659, 5595, 31994, 782, 3624, 14627, 15, 12612, 277, 5, 216, 31993, 2119, 3, 9, 19529, 593, 853, 21, 921, 31992, 12, 129, 394, 28, 70, 17712, 1098, 5, 31991, 3884, 25, 762, 25, 174, 12, 214, 12, 31990, 3, 9, 3, 23405, 4547, 15068, 2259, 6, 31989, 6, 5459, 6, 13618, 7, 6, 3604, 1801, 31988, 6, 303, 24190, 11, 1472, 251, 5, 37, 31987, 36, 16, 8, 853, 19, 25264, 399, 568, 31986, 21, 21380, 7, 34, 19, 339, 5, 15746, 31985, 8, 583, 56, 36, 893, 3, 9, 3, 31984, 9486, 42, 3, 9, 1409, 29, 11, 25, 31983, 12246, 5977, 13, 284, 3604, 24, 19, 2657, 31982]\r\n","\r\n","targets:\r\n","\r\n","[31999, 12847, 277, 31998, 9, 55, 31997, 3326, 15068, 31996, 48, 30, 31995, 727, 1715, 31994, 45, 301, 31993, 56, 36, 31992, 113, 2746, 31991, 216, 56, 31990, 5978, 16, 31989, 379, 2097, 31988, 11, 27856, 31987, 583, 12, 31986, 6, 11, 31985, 26, 16, 31984, 17, 18, 31983, 56, 36, 31982, 5]\r\n","```"]},{"cell_type":"markdown","metadata":{"id":"UkI1geD2F2kh"},"source":["You will now use the inputs and the targets from the `tokenize_and_mask` function you implemented above. Take a look at the masked sentence using your `inps` and `targs` from the sentence above. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tREwnCUCF2BI","executionInfo":{"status":"ok","timestamp":1613265077599,"user_tz":480,"elapsed":420,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"c699de33-c35e-4edf-8069-1544229f21a7"},"source":["print('Inputs: \\n\\n', pretty_decode(inps))\r\n","print('\\nTargets: \\n\\n', pretty_decode(targs))"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Inputs: \n","\n"," <Z> BBQ Class Taking Place in Missoul <Y> Do you want to get better at making <X>? You will have the opportunity, put <W> your calendar now. Thursday, September 22 <V> World Class BBQ Champion, Tony Balay <U>onestar Smoke Rangers. He <T> teaching a beginner level class for everyone<S> to get better with their culinary skills.<R> teach you everything you need to know to <Q> a KCBS BBQ competition,<P>, recipes, timelines, meat selection <O>, plus smoker and fire information. The<N> be in the class is $35 per person <M> for spectators it is free. Include <L> the cost will be either a  <K>shirt or apron and you <J> tasting samples of each meat that is prepared <I>\n","\n","Targets: \n","\n"," <Z> Beginners <Y>a! <X> delicious BBQ <W> this on <V>nd join <U> from L <T> will be<S> who wants<R> He will <Q> compete in<P> including techniques <O> and trimming<N> cost to <M>, and <L>d in <K>t- <J> will be <I>.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G0LMILLlGeWW"},"source":["<a name='1.4'></a>\r\n","### 1.4 Creating the Pairs\r\n","\r\n","You will now create pairs using your dataset. You will iterate over your data and create (inp, targ) pairs using the functions that we have given you. "]},{"cell_type":"code","metadata":{"id":"kSrB0jPuFlkO","executionInfo":{"status":"ok","timestamp":1613265397011,"user_tz":480,"elapsed":623,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["inputs_targets_pairs = [tokenize_and_mask(text) for text in natural_language_texts]"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"RF8HhiHFGy1z","executionInfo":{"status":"ok","timestamp":1613265397916,"user_tz":480,"elapsed":279,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["def display_input_target_pairs(inputs_targets_pairs):\r\n","    for i, inp_tgt_pair in enumerate(inputs_targets_pairs, 1):\r\n","        inps, tgts = inp_tgt_pair\r\n","        inps, tgts = pretty_decode(inps), pretty_decode(tgts)\r\n","        print(f'[{i}]\\n\\n'\r\n","              f'inputs:\\n{wrapper.fill(text=inps)}\\n\\n'\r\n","              f'targets:\\n{wrapper.fill(text=tgts)}\\n\\n\\n\\n')"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d79ZpGUBHCAN","executionInfo":{"status":"ok","timestamp":1613265399779,"user_tz":480,"elapsed":878,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"fa0ee9a0-72e6-4ae4-eb88-7af756d4fcd4"},"source":["display_input_target_pairs(inputs_targets_pairs)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["[1]\n","\n","inputs:\n","Beginners BBQ Class Taking Place in Missoula! <Z> you want <Y> get\n","better at making delicious BBQ <X> You will have the opportunity <W>\n","put this <V> your calendar now. Thursday, September 22nd join World\n","Class BBQ Champion, Tony Balay from Lonestar Smok <U> Rang <T>. He\n","will be teaching <S> beginner level class for everyone who wants to<R>\n","better with their culinary skills. <Q> will teach<P> everything you\n","<O> to know to<N> in a  <M>BS <L>, including techniques, recipes,\n","timelines, meat selection and trimming <K> plus smoker and fire\n","information. The cost to be in the class is $35 per <J>, and for\n","spectators it is free. Included in <I> will be either <H>t-shirt\n","or<G>apron and you <F> tasting samples of each meat that is<E>.\n","\n","targets:\n","<Z> Do <Y> to <X>? <W>, <V> on <U>e <T>ers<S>a<R> get <Q> He<P> you\n","<O> need<N> compete <M>KC <L> BBQ competition <K>, <J> person <I> the\n","cost<H>a <G>  <F> will be<E> prepared\n","\n","\n","\n","\n","[2]\n","\n","inputs:\n","<Z> in  <Y>Mac OS X <X> (10. <W>' <V>  <U>xboi87 <T> Jan 20,<S> I<R>ve\n","got a 500gb <Q> drive and a 240g<P>. When trying to restore using disk\n","utility i'm given the error \"Not enough <O> disk ____ to restore\"<N> I\n","shouldn't have to do that!!! Any ideas or workarounds <M> resorting to\n","the above? Use Carbon Copy Cloner to copy one drive to the other. I'\n","<L> done this several <K> going from larger HDD to smaller SSD and I\n","<J> up with a bootable SSD drive <I> One<H> you have<G> not to skip is\n","<F> Disk<E> to partition the SSD as GUID<D> HFS+ before doing<C>\n","clone. If it came Apple Partition Scheme <B> even if you let CCC <A>\n","the  <z>ne, the resulting drive won't be bootable. C <y> usually<x> in\n","\"file mode\" and it can easily<w> a larger drive (that's<v> empty) onto\n","a smaller drive. If you<u> C <t> to clon <s> a<r> you did NOT boot\n","from, it can work in block<q> mode where the destination drive<p> be\n","the same size or larger <o> the drive you are cloning from (if I\n","recall). I've actually <n> this somehow on Disk Utility several\n","<m>booting from a different drive ( <l> even the dvd) so not<k>\n","utility from the <j> cloning)<i> had<h> work just fine from larger to\n","smaller bootable clone.<g>Definitely format the drive clo <f>ing to\n","first, as bootable Apple etc.. Thanks for pointing<e> out <d> My only\n","experience <c> DU to <b> larger to smaller was when I was trying to\n","make a Lion <a> stick and I was unable to Théâtre InstallESD.dmgKeep a\n","4 GB USB stick butdürftig courseutti wouldn't fit is there was Carolyn\n","more than 4 GBcosting data.\n","\n","targets:\n","<Z> Discussion <Y>' <X> Lion <W>7) <V> started by <U>a <T>,<S>\n","2012.<R>' <Q> internal<P>b SSD <O> space on<N> But <M> before <L>ve\n","<K> times <J> wound <I>.<H> step<G> to remember <F> to use<E>\n","Utility<D> partition scheme<C> the <B>, <A> do <z>clo <y>CC<x>\n","works<w> copy<v> mostly<u> tell <t>CC <s>e<r> drive<q> copy<p> must\n","<o> than <n> done <m> times ( <l>or<k> running disk <j> drive your<i>\n","and<h> it<g>  <f>n<e> this <d>. <c> using <b> go <a> install Théâtre\n","restoreKeep todürftig ofutti the reason that Carolyn slightlycosting\n","of\n","\n","\n","\n","\n","[3]\n","\n","inputs:\n","<Z>il <Y> lycra <X>d <W> short <V> with metallic slinky inset <U>.\n","Attached metallic elastic belt with <T>-ring<S> Headband<R>. Great <Q>\n","hop or jazz dance costume<P> Made in the <O>.\n","\n","targets:\n","<Z> Fo <Y> plaid <X> and span <W>ex <V>all <U>s <T> O<S>.<R> included\n","<Q> hip<P>. <O> USA\n","\n","\n","\n","\n","[4]\n","\n","inputs:\n","How many backlinks per day for new site? Discussion in 'Black <Z> SEO'\n","started by Omopla <Y>a, Dec 3, 2010. <X>  <W> site, what's the max #\n","back <V>s per day <U> do to be safe? 2) <T> long do I have<S> let my\n","site age before I can start making<R> blinks? I did about 6000 forum\n","<Q> every 24 hours for 10 days for one of my sites which had a brand\n","new domain.<P> is <O> backlinks for every of these forum profile so\n","thats 18 000 backlinks<N> 24 hours and <M> happened in terms of being\n","penalized or s <L>box <K>d. This is now <J> 3 months ago and the site\n","is ranking on first page <I> a lot<H> my targeted keywords<G> build\n","more you can in starting but do manual <F> not spammy type means\n","manual<E> relevant to<D> post.. then after 1 month you can<C> a big\n","blast.. Wow, <B>, you built 18k backlinks a day on  <A> new site? How\n","quickly did you <z> up? What <y> competition/searches did those\n","keywords have<x>\n","\n","targets:\n","<Z> Hat <Y>t <X> 1) for <W>a newly created <V>link <U> I should <T>\n","how<S> to<R> more <Q> profiles<P> There <O> three<N> every <M> nothing\n","<L>and <K>e <J> maybe <I> for<H> of<G>. <F> submission and<E> +<D>\n","the<C> make <B> dude <A>a brand <z> rank <y> kind of<x>?\n","\n","\n","\n","\n","[5]\n","\n","inputs:\n","The <Z> of <Y> opened the 2017-18 <X> year with an update <W> projects\n","that include new construction, upgrades, heat mitigation and quality\n","<V> environments. We are excited that <U> students will <T> the\n","beneficiaries<S> a four year, $572 million General Ob<R>ation Bond.\n","<Q> the passage of the bond, our construction team has worked to\n","schedule the projects<P> four-year term <O> the bond. Denver voters on\n","Tuesday approved bond and mill funding measures for students in Denver\n","Public Schools, agreeing to<N> $572 million in bond funding to <M> and\n","improve schools and $56.6 million in operating dollars to support <L>\n","initiatives, such as <K> literacy. Denver voters say yes to bond and\n","mill levy funding support for D <J> students <I> schools. Click to\n","learn more about the details of<H> voter-approved<G> measure. Denver\n","voters on Nov. 8 approved bond <F> mill funding measures for DPS\n","students and schools. Learn<E> about what’s included in<D> mill<C>levy\n","measure.\n","\n","targets:\n","<Z> Denver Board <Y> Education <X> school <W> on <V> learning <U>\n","Denver <T> be<S> of<R>lig <Q> Since<P> over the <O> of<N> invest <M>\n","build <L> proven <K> early <J>PS <I> and<H> the<G> bond <F> and<E>\n","more<D> the<C>\n","\n","\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_t3EeaM8HM4u"},"source":["<a name='2'></a>\r\n","# Part 2: Transfomer\r\n","\r\n","We now load a Transformer model checkpoint that has been pre-trained using the above C4 dataset and decode from it. This will save you a lot of time rather than have to train your model yourself. Later in this notebook, we will show you how to fine-tune your model.\r\n","\r\n","<img src=\"https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/images/w4_t5_fulltransformer.png\" width=\"300px\" height=\"600px\">\r\n","\r\n","Start by loading in the model. We copy the checkpoint to local dir for speed, otherwise initialization takes a very long time. Last week you implemented the decoder part for the transformer. Now you will implement the encoder part. Concretely you will implement the following. \r\n","\r\n","\r\n","<img src=\"https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/images/w4_t5_encoder.png\" width=\"300px\" height=\"600px\">"]},{"cell_type":"markdown","metadata":{"id":"EZ5bCj0UIkMv"},"source":["<a name='2.1'></a>\r\n","### 2.1 Transformer Encoder\r\n","\r\n","You will now implement the transformer encoder. Concretely you will implement two functions. The first function is `FeedForwardBlock`.\r\n","\r\n","<a name='2.1.1'></a>\r\n","#### 2.1.1 The Feedforward Block\r\n","\r\n","The `FeedForwardBlock` function is an important one so you will start by implementing it. To do so, you need to return a list of the following: \r\n","\r\n","- [`tl.LayerNorm()`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm) = layer normalization.\r\n","- [`tl.Dense(d_ff)`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) = fully connected layer.\r\n","- [`activation`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu) = activation relu, tanh, sigmoid etc. \r\n","- `dropout_middle` = we gave you this function (don't worry about its implementation).\r\n","- [`tl.Dense(d_model)`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) = fully connected layer with same dimension as the model.\r\n","- `dropout_final` = we gave you this function (don't worry about its implementation).\r\n","\r\n","You can always take a look at [trax documentation](https://trax-ml.readthedocs.io/en/latest/) if needed.\r\n","\r\n","**Instructions**: Implement the feedforward part of the transformer. You will be returning a list. "]},{"cell_type":"markdown","metadata":{"id":"5FnF8UO_JGFF"},"source":["<a name='ex02'></a>\r\n","### Exercise 02"]},{"cell_type":"code","metadata":{"id":"eEuwrnhfHB9a"},"source":["# UNQ_C2\r\n","# GRADED FUNCTION: FeedForwardBlock\r\n","def FeedForwardBlock(d_model, d_ff, dropout, dropout_shared_axes, mode, activation):\r\n","    \"\"\"Returns a list of layers implementing a feed-forward block.\r\n","    Args:\r\n","        d_model: int:  depth of embedding\r\n","        d_ff: int: depth of feed-forward layer\r\n","        dropout: float: dropout rate (how much to drop out)\r\n","        dropout_shared_axes: list of integers, axes to share dropout mask\r\n","        mode: str: 'train' or 'eval'\r\n","        activation: the non-linearity in feed-forward layer\r\n","    Returns:\r\n","        A list of layers which maps vectors to vectors.\r\n","    \"\"\"\r\n","    \r\n","    dropout_middle = tl.Dropout(rate=dropout,\r\n","                                shared_axes=dropout_shared_axes, \r\n","                                mode=mode)\r\n","  \r\n","    dropout_final = tl.Dropout(rate=dropout, \r\n","                               shared_axes=dropout_shared_axes, \r\n","                               mode=mode)\r\n","\r\n","    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\r\n","    \r\n","    ff_block = [ \r\n","        # trax Layer normalization \r\n","        tl.LayerNorm(),\r\n","        # trax Dense layer using `d_ff`\r\n","        tl.Dense(d_ff),\r\n","        # activation() layer - you need to call (use parentheses) this func!\r\n","        ,\r\n","        # dropout middle layer\r\n","        None,\r\n","        # trax Dense layer using `d_model`\r\n","        None,\r\n","        # dropout final layer\r\n","        None,\r\n","    ]\r\n","    \r\n","    ### END CODE HERE ###\r\n","    \r\n","    return ff_block"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_lkcqzaEHB6M"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZyaRvUTNHB3K"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fR6pJyHfHB0K"},"source":[""],"execution_count":null,"outputs":[]}]}