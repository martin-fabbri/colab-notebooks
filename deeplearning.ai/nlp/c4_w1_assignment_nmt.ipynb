{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c4_w1_assignment_nmt.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO2SjmwIXP95v7qbQ/VX6Eo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"82c7a503d809422ebcfc1907c1953ac1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_60456dd2f5ca45f4a2fb09c50b6bf9b9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_86c253301bba4c5389121ace8b5c9fe7","IPY_MODEL_a44ec96fe1504afebcdb6619fda92f25"]}},"60456dd2f5ca45f4a2fb09c50b6bf9b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"86c253301bba4c5389121ace8b5c9fe7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7db9e0a654524939a1dd63d52ccf3c46","_dom_classes":[],"description":"Dl Completed...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_53f37851a6ef4ba09193e322a7528f2e"}},"a44ec96fe1504afebcdb6619fda92f25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9807c23963a046b4b2747415f2623d4f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:02&lt;00:00,  2.56s/ url]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c18480a1de9f496f974fad4353deeff7"}},"7db9e0a654524939a1dd63d52ccf3c46":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"53f37851a6ef4ba09193e322a7528f2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9807c23963a046b4b2747415f2623d4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c18480a1de9f496f974fad4353deeff7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9a455c00c9854737b462f70f989775ab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_32d398a2223e42cd9b9167cb6bbb0af5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1c28887ac2d945d48efe124a9423bc5a","IPY_MODEL_27b2ea69f6534fddb44eaf78b4320f61"]}},"32d398a2223e42cd9b9167cb6bbb0af5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1c28887ac2d945d48efe124a9423bc5a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d4e91fb675cc473c915d769f321d69f8","_dom_classes":[],"description":"Dl Size...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8fe87d49c5fb47af978b2cdbaf603028"}},"27b2ea69f6534fddb44eaf78b4320f61":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ad4000347a4f4e33a9b0beecfe6f665d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 34/34 [00:02&lt;00:00, 13.39 MiB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2a1c08d658f44e8d9ef728789e240109"}},"d4e91fb675cc473c915d769f321d69f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8fe87d49c5fb47af978b2cdbaf603028":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ad4000347a4f4e33a9b0beecfe6f665d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2a1c08d658f44e8d9ef728789e240109":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a2ed60fe11604a9dac17221dbb660185":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b587f3f422834f3f82d286bad1f29a19","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_43c0e64607b74ee1a4da980babeec688","IPY_MODEL_3139f0234c644673b53f1dc3a91e4f33"]}},"b587f3f422834f3f82d286bad1f29a19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"43c0e64607b74ee1a4da980babeec688":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e5ffe058c3dd4dae93eb91171b18df8b","_dom_classes":[],"description":"Extraction completed...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_59172fc2359b4cce93cf3791bd857b1c"}},"3139f0234c644673b53f1dc3a91e4f33":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_19a4a56d260a48999028b756dcb5198d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:02&lt;00:00,  2.50s/ file]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ae2922163bb34584a3c2f5b4408f289b"}},"e5ffe058c3dd4dae93eb91171b18df8b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"59172fc2359b4cce93cf3791bd857b1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"19a4a56d260a48999028b756dcb5198d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ae2922163bb34584a3c2f5b4408f289b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"96e24671879c4130beb2193fa859f255":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_34bb84a4d4294741939bdc0be15a85bf","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_daca0550b4e745a1b3de808f5116f4b1","IPY_MODEL_c8a3ebc3f8a249f69e5cf12d5681d881"]}},"34bb84a4d4294741939bdc0be15a85bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"daca0550b4e745a1b3de808f5116f4b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2b85a8de46c94d4c8290202429343683","_dom_classes":[],"description":"Generating splits...: 100%","_model_name":"FloatProgressModel","bar_style":"","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_79ec0ce415924a1596e5243980dbb488"}},"c8a3ebc3f8a249f69e5cf12d5681d881":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bc9dff872fca4ab7b4545801c22e3e89","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [02:51&lt;00:00, 171.59s/ splits]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a68c97acf8044aa4be8fc4151255c8ab"}},"2b85a8de46c94d4c8290202429343683":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"79ec0ce415924a1596e5243980dbb488":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bc9dff872fca4ab7b4545801c22e3e89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a68c97acf8044aa4be8fc4151255c8ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bdc31b2830b94e6fb28a88f961fe3a99":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b064c6a07cac4f3693bce4002aa3bfe9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b38069554b3f4304aad6fbdced5bcb73","IPY_MODEL_b29d8f2394aa4c44b821359c5a7c4681"]}},"b064c6a07cac4f3693bce4002aa3bfe9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b38069554b3f4304aad6fbdced5bcb73":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a1dff6fb838e44ad9d9e7fd7a6354e52","_dom_classes":[],"description":"Generating train examples...: 100%","_model_name":"FloatProgressModel","bar_style":"","max":1108752,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1108752,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_09c88fb962324801800834f4794da903"}},"b29d8f2394aa4c44b821359c5a7c4681":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4fd0bd5892c14e5b917fdeaa0e94222f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1108752/1108752 [02:47&lt;00:00, 6297.46 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0f64e1a39e614a6eb6bc7effd325ea2c"}},"a1dff6fb838e44ad9d9e7fd7a6354e52":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"09c88fb962324801800834f4794da903":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4fd0bd5892c14e5b917fdeaa0e94222f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0f64e1a39e614a6eb6bc7effd325ea2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e6607e0c01cb424889121cea3b7c5212":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_515ce6ab58574a3798aae87bdc095c16","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ce5d9c98a7d841a09ab6832fea08aa7e","IPY_MODEL_0ddee5071c69480c8128cc5aa270698b"]}},"515ce6ab58574a3798aae87bdc095c16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ce5d9c98a7d841a09ab6832fea08aa7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e4961c8af413420cbc868817f288cdb2","_dom_classes":[],"description":"Shuffling opus-train.tfrecord...: 100%","_model_name":"FloatProgressModel","bar_style":"","max":1108752,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1108752,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_407cb708b9c4467e8aaba6ecff2360ee"}},"0ddee5071c69480c8128cc5aa270698b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_537861f09f414c719b8522fbc4c7ecfe","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1108752/1108752 [00:04&lt;00:00, 997.87 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_632b96479ee44eb886fdb3ca7fda91d5"}},"e4961c8af413420cbc868817f288cdb2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"407cb708b9c4467e8aaba6ecff2360ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"537861f09f414c719b8522fbc4c7ecfe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"632b96479ee44eb886fdb3ca7fda91d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"99bed60ea0144678971449b853256861":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7ec411ff0ab64ada9e7cb51202c0df34","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c54ee65267fd4181b7b3473995eba954","IPY_MODEL_a9c5f6416bac474294634354d63e9d41"]}},"7ec411ff0ab64ada9e7cb51202c0df34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c54ee65267fd4181b7b3473995eba954":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_703a2bb8ea1b4e2ea943a92c870bdc1e","_dom_classes":[],"description":"Dl Completed...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d2d22a87b6314b6b883f6b4363b1e34e"}},"a9c5f6416bac474294634354d63e9d41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c7c2332e046a4dc6af65122aca71aa88","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:02&lt;00:00,  2.41s/ url]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_672f48c41a1c43deb11f3ef5ed7bf34f"}},"703a2bb8ea1b4e2ea943a92c870bdc1e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d2d22a87b6314b6b883f6b4363b1e34e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c7c2332e046a4dc6af65122aca71aa88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"672f48c41a1c43deb11f3ef5ed7bf34f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"716215d1bd5d4e4f94b7189c0e154473":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4e639bf41e464a0b8c9f3e84b68db897","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b3e2c81e1c094d6da3cd9dbadddc9353","IPY_MODEL_8065edae416d4c34a16afea7b74eb505"]}},"4e639bf41e464a0b8c9f3e84b68db897":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b3e2c81e1c094d6da3cd9dbadddc9353":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_dfbf74da255e4cedb129b721a93e5c95","_dom_classes":[],"description":"Dl Size...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3f69353efca7467581bb7f4d8ec70a93"}},"8065edae416d4c34a16afea7b74eb505":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_edce53f4b2a545faac487177270714ca","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 34/34 [00:02&lt;00:00, 14.30 MiB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_af354699b96547709337c8a9c487018f"}},"dfbf74da255e4cedb129b721a93e5c95":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3f69353efca7467581bb7f4d8ec70a93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"edce53f4b2a545faac487177270714ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"af354699b96547709337c8a9c487018f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"833ed45503fc4a42b226b8e9c169f445":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_89703074beb04b03931763c76dcc72af","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4eddd974068843eab536372db97e4e89","IPY_MODEL_202721a875364eb58734eee047a34d9b"]}},"89703074beb04b03931763c76dcc72af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4eddd974068843eab536372db97e4e89":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1e906b4dd39a4246bc38ab552dbfd79b","_dom_classes":[],"description":"Extraction completed...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ded0e5a3d2fb4d9bb66f777144ec9aae"}},"202721a875364eb58734eee047a34d9b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4bcf3653f1b948b987f470b197f94d97","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:02&lt;00:00,  2.35s/ file]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ade92721b52343fd827c98006d9ea2d9"}},"1e906b4dd39a4246bc38ab552dbfd79b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ded0e5a3d2fb4d9bb66f777144ec9aae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4bcf3653f1b948b987f470b197f94d97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ade92721b52343fd827c98006d9ea2d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"687c26177c284e90b35b131cd116c15c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bab9ed8b31b54d6d8ada5634e53f5110","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_015a8196111d49fbae59380345700cc5","IPY_MODEL_e2212ca9c5ca4a85ab08af236ed0486f"]}},"bab9ed8b31b54d6d8ada5634e53f5110":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"015a8196111d49fbae59380345700cc5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_eebeb4c88de341f382a0af5851afbf9b","_dom_classes":[],"description":"Generating splits...: 100%","_model_name":"FloatProgressModel","bar_style":"","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c1de432589d24c25ace9c6807d7fe70b"}},"e2212ca9c5ca4a85ab08af236ed0486f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cc98679ee1a14261ab7c24f5edb4d248","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [02:49&lt;00:00, 169.48s/ splits]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3a5d8b1da4f84c4cbedefbfe39f3f2fb"}},"eebeb4c88de341f382a0af5851afbf9b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c1de432589d24c25ace9c6807d7fe70b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cc98679ee1a14261ab7c24f5edb4d248":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3a5d8b1da4f84c4cbedefbfe39f3f2fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ea13a319a3cc469fa8fae04ae681cfcc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_85a27fea935c44bfa927d45a2f35f221","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3aa19e51034b48618af55aa16f4c1c0c","IPY_MODEL_edce5d78319c4f9e992c4f48581cf296"]}},"85a27fea935c44bfa927d45a2f35f221":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3aa19e51034b48618af55aa16f4c1c0c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_01cf68e786d8427b977ed9ad23d99b9e","_dom_classes":[],"description":"Generating train examples...: 100%","_model_name":"FloatProgressModel","bar_style":"","max":1108752,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1108752,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_902b1e920adf46e7bbee3b42716ae964"}},"edce5d78319c4f9e992c4f48581cf296":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8fcb5f1742e14f9ead557e6d2a94f88f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1108752/1108752 [02:45&lt;00:00, 6794.58 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_813d77b57d1045ca908c47c68bf38428"}},"01cf68e786d8427b977ed9ad23d99b9e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"902b1e920adf46e7bbee3b42716ae964":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8fcb5f1742e14f9ead557e6d2a94f88f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"813d77b57d1045ca908c47c68bf38428":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a4779902375d48e4b4d7434d0c2f152e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7387ed7d3435412bbe1660691f8acfd2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_70fda663f8954f7486a21cfc53b3162a","IPY_MODEL_433748a27b824e1882ceb7d5cc15a402"]}},"7387ed7d3435412bbe1660691f8acfd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"70fda663f8954f7486a21cfc53b3162a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5bc952235d2f4cceb8d2d7dbb01c1388","_dom_classes":[],"description":"Shuffling opus-train.tfrecord...: 100%","_model_name":"FloatProgressModel","bar_style":"","max":1108752,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1108752,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4ff60a45423c47328e452b37282c23fd"}},"433748a27b824e1882ceb7d5cc15a402":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8169adb36569434cab0c4bd880d53fbc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1108752/1108752 [00:03&lt;00:00, 1033.99 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f2bb66222ca44697827eba51eee69864"}},"5bc952235d2f4cceb8d2d7dbb01c1388":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4ff60a45423c47328e452b37282c23fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8169adb36569434cab0c4bd880d53fbc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f2bb66222ca44697827eba51eee69864":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"nTs1IQszuqLx"},"source":["# Assignment 1:  Neural Machine Translation\r\n","\r\n","Welcome to the first assignment of Course 4. Here, you will build an English-to-German neural machine translation (NMT) model using Long Short-Term Memory (LSTM) networks with attention.  Machine translation is an important task in natural language processing and could be useful not only for translating one language to another but also for word sense disambiguation (e.g. determining whether the word \"bank\" refers to the financial bank, or the land alongside a river). Implementing this using just a Recurrent Neural Network (RNN) with LSTMs can work for short to medium length sentences but can result in vanishing gradients for very long sequences. To solve this, you will be adding an attention mechanism to allow the decoder to access all relevant parts of the input sentence regardless of its length. By completing this assignment, you will:  \r\n","\r\n","- learn how to preprocess your training and evaluation data\r\n","- implement an encoder-decoder system with attention\r\n","- understand how attention works\r\n","- build the NMT model from scratch using Trax\r\n","- generate translations using greedy and Minimum Bayes Risk (MBR) decoding \r\n","## Outline\r\n","- [Part 1:  Data Preparation](#1)\r\n","    - [1.1  Importing the Data](#1.1)\r\n","    - [1.2  Tokenization and Formatting](#1.2)\r\n","    - [1.3  tokenize & detokenize helper functions](#1.3)\r\n","    - [1.4  Bucketing](#1.4)\r\n","    - [1.5  Exploring the data](#1.5)\r\n","- [Part 2:  Neural Machine Translation with Attention](#2)\r\n","    - [2.1  Attention Overview](#2.1)\r\n","    - [2.2  Helper functions](#2.2)\r\n","        - [Exercise 01](#ex01)\r\n","        - [Exercise 02](#ex02)\r\n","        - [Exercise 03](#ex03)\r\n","    - [2.3  Implementation Overview](#2.3)\r\n","        - [Exercise 04](#ex04)\r\n","- [Part 3:  Training](#3)\r\n","    - [3.1  TrainTask](#3.1)\r\n","        - [Exercise 05](#ex05)\r\n","    - [3.2  EvalTask](#3.2)\r\n","    - [3.3  Loop](#3.3)\r\n","- [Part 4:  Testing](#4)\r\n","    - [4.1  Decoding](#4.1)\r\n","        - [Exercise 06](#ex06)\r\n","        - [Exercise 07](#ex07)\r\n","    - [4.2  Minimum Bayes-Risk Decoding](#4.2)\r\n","        - [Exercise 08](#ex08)\r\n","        - [Exercise 09](#ex09)\r\n","        - [Exercise 10](#ex10)"]},{"cell_type":"markdown","metadata":{"id":"umt6wi7lutWa"},"source":["<a name=\"1\"></a>\r\n","# Part 1:  Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"E2BXf1FluvDR"},"source":["<a name=\"1.1\"></a>\r\n","## 1.1  Importing the Data\r\n","\r\n","We will first start by importing the packages we will use in this assignment. As in the previous course of this specialization, we will use the [Trax](https://github.com/google/trax) library created and maintained by the [Google Brain team](https://research.google/teams/brain/) to do most of the heavy lifting. It provides submodules to fetch and process the datasets, as well as build and train the model."]},{"cell_type":"code","metadata":{"id":"6T4TWigdvlfC","executionInfo":{"status":"ok","timestamp":1612658272769,"user_tz":480,"elapsed":12219,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["%%capture\r\n","!pip install trax"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3aNcZkhLug5q","executionInfo":{"status":"ok","timestamp":1612665568887,"user_tz":480,"elapsed":976,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"544bd68f-00e9-49fb-d7b7-1181c0db9f7c"},"source":["import random\r\n","\r\n","import numpy as np\r\n","import trax\r\n","from termcolor import colored\r\n","from trax import layers as tl\r\n","from trax.fastmath import numpy as fastnp\r\n","from trax.supervised import training\r\n","\r\n","!pip list | grep \"termcolor\\|trax\" "],"execution_count":31,"outputs":[{"output_type":"stream","text":["termcolor                     1.1.0                \n","trax                          1.3.7                \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1mPiLm2ZxROc"},"source":["Next, we will import the dataset we will use to train the model. To meet the storage constraints in this lab environment, we will just use a small dataset from [Opus](http://opus.nlpl.eu/), a growing collection of translated texts from the web. Particularly, we will get an English to German translation subset specified as `opus/medical` which has medical related texts. If storage is not an issue, you can opt to get a larger corpus such as the English to German translation dataset from [ParaCrawl](https://paracrawl.eu/), a large multi-lingual translation dataset created by the European Union. Both of these datasets are available via [Tensorflow Datasets (TFDS)](https://www.tensorflow.org/datasets)\r\n","and you can browse through the other available datasets [here](https://www.tensorflow.org/datasets/catalog/overview). We have downloaded the data for you in the `data/` directory of your workspace. As you'll see below, you can easily access this dataset from TFDS with `trax.data.TFDS`. The result is a python generator function yielding tuples. Use the `keys` argument to select what appears at which position in the tuple. For example, `keys=('en', 'de')` below will return pairs as (English sentence, German sentence). "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":476,"referenced_widgets":["82c7a503d809422ebcfc1907c1953ac1","60456dd2f5ca45f4a2fb09c50b6bf9b9","86c253301bba4c5389121ace8b5c9fe7","a44ec96fe1504afebcdb6619fda92f25","7db9e0a654524939a1dd63d52ccf3c46","53f37851a6ef4ba09193e322a7528f2e","9807c23963a046b4b2747415f2623d4f","c18480a1de9f496f974fad4353deeff7","9a455c00c9854737b462f70f989775ab","32d398a2223e42cd9b9167cb6bbb0af5","1c28887ac2d945d48efe124a9423bc5a","27b2ea69f6534fddb44eaf78b4320f61","d4e91fb675cc473c915d769f321d69f8","8fe87d49c5fb47af978b2cdbaf603028","ad4000347a4f4e33a9b0beecfe6f665d","2a1c08d658f44e8d9ef728789e240109","a2ed60fe11604a9dac17221dbb660185","b587f3f422834f3f82d286bad1f29a19","43c0e64607b74ee1a4da980babeec688","3139f0234c644673b53f1dc3a91e4f33","e5ffe058c3dd4dae93eb91171b18df8b","59172fc2359b4cce93cf3791bd857b1c","19a4a56d260a48999028b756dcb5198d","ae2922163bb34584a3c2f5b4408f289b","96e24671879c4130beb2193fa859f255","34bb84a4d4294741939bdc0be15a85bf","daca0550b4e745a1b3de808f5116f4b1","c8a3ebc3f8a249f69e5cf12d5681d881","2b85a8de46c94d4c8290202429343683","79ec0ce415924a1596e5243980dbb488","bc9dff872fca4ab7b4545801c22e3e89","a68c97acf8044aa4be8fc4151255c8ab","bdc31b2830b94e6fb28a88f961fe3a99","b064c6a07cac4f3693bce4002aa3bfe9","b38069554b3f4304aad6fbdced5bcb73","b29d8f2394aa4c44b821359c5a7c4681","a1dff6fb838e44ad9d9e7fd7a6354e52","09c88fb962324801800834f4794da903","4fd0bd5892c14e5b917fdeaa0e94222f","0f64e1a39e614a6eb6bc7effd325ea2c","e6607e0c01cb424889121cea3b7c5212","515ce6ab58574a3798aae87bdc095c16","ce5d9c98a7d841a09ab6832fea08aa7e","0ddee5071c69480c8128cc5aa270698b","e4961c8af413420cbc868817f288cdb2","407cb708b9c4467e8aaba6ecff2360ee","537861f09f414c719b8522fbc4c7ecfe","632b96479ee44eb886fdb3ca7fda91d5","99bed60ea0144678971449b853256861","7ec411ff0ab64ada9e7cb51202c0df34","c54ee65267fd4181b7b3473995eba954","a9c5f6416bac474294634354d63e9d41","703a2bb8ea1b4e2ea943a92c870bdc1e","d2d22a87b6314b6b883f6b4363b1e34e","c7c2332e046a4dc6af65122aca71aa88","672f48c41a1c43deb11f3ef5ed7bf34f","716215d1bd5d4e4f94b7189c0e154473","4e639bf41e464a0b8c9f3e84b68db897","b3e2c81e1c094d6da3cd9dbadddc9353","8065edae416d4c34a16afea7b74eb505","dfbf74da255e4cedb129b721a93e5c95","3f69353efca7467581bb7f4d8ec70a93","edce53f4b2a545faac487177270714ca","af354699b96547709337c8a9c487018f","833ed45503fc4a42b226b8e9c169f445","89703074beb04b03931763c76dcc72af","4eddd974068843eab536372db97e4e89","202721a875364eb58734eee047a34d9b","1e906b4dd39a4246bc38ab552dbfd79b","ded0e5a3d2fb4d9bb66f777144ec9aae","4bcf3653f1b948b987f470b197f94d97","ade92721b52343fd827c98006d9ea2d9","687c26177c284e90b35b131cd116c15c","bab9ed8b31b54d6d8ada5634e53f5110","015a8196111d49fbae59380345700cc5","e2212ca9c5ca4a85ab08af236ed0486f","eebeb4c88de341f382a0af5851afbf9b","c1de432589d24c25ace9c6807d7fe70b","cc98679ee1a14261ab7c24f5edb4d248","3a5d8b1da4f84c4cbedefbfe39f3f2fb","ea13a319a3cc469fa8fae04ae681cfcc","85a27fea935c44bfa927d45a2f35f221","3aa19e51034b48618af55aa16f4c1c0c","edce5d78319c4f9e992c4f48581cf296","01cf68e786d8427b977ed9ad23d99b9e","902b1e920adf46e7bbee3b42716ae964","8fcb5f1742e14f9ead557e6d2a94f88f","813d77b57d1045ca908c47c68bf38428","a4779902375d48e4b4d7434d0c2f152e","7387ed7d3435412bbe1660691f8acfd2","70fda663f8954f7486a21cfc53b3162a","433748a27b824e1882ceb7d5cc15a402","5bc952235d2f4cceb8d2d7dbb01c1388","4ff60a45423c47328e452b37282c23fd","8169adb36569434cab0c4bd880d53fbc","f2bb66222ca44697827eba51eee69864"]},"id":"hFAUq8_tu5fW","executionInfo":{"status":"ok","timestamp":1612658650889,"user_tz":480,"elapsed":390319,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"032e5d17-38f9-4866-d225-3873d7ed8de9"},"source":["train_stream_fn = trax.data.TFDS(\r\n","    \"opus/medical\", keys=(\"en\", \"de\"), eval_holdout_size=0.01, train=True\r\n",")\r\n","\r\n","eval_stream_fn = trax.data.TFDS(\r\n","    \"opus/medical\",\r\n","    data_dir=\"./data/\",\r\n","    keys=(\"en\", \"de\"),\r\n","    eval_holdout_size=0.01,\r\n","    train=False,\r\n",")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["\u001b[1mDownloading and preparing dataset 34.29 MiB (download: 34.29 MiB, generated: 188.85 MiB, total: 223.13 MiB) to /root/tensorflow_datasets/opus/medical/0.1.0...\u001b[0m\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82c7a503d809422ebcfc1907c1953ac1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a455c00c9854737b462f70f989775ab","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2ed60fe11604a9dac17221dbb660185","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n","\n","\n","\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"96e24671879c4130beb2193fa859f255","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Generating splits...', max=1.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bdc31b2830b94e6fb28a88f961fe3a99","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Generating train examples...', max=1108752.0, style=Progr…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6607e0c01cb424889121cea3b7c5212","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Shuffling opus-train.tfrecord...', max=1108752.0, style=P…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\u001b[1mDataset opus downloaded and prepared to /root/tensorflow_datasets/opus/medical/0.1.0. Subsequent calls will reuse this data.\u001b[0m\n","\u001b[1mDownloading and preparing dataset 34.29 MiB (download: 34.29 MiB, generated: 188.85 MiB, total: 223.13 MiB) to ./data/opus/medical/0.1.0...\u001b[0m\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"99bed60ea0144678971449b853256861","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"716215d1bd5d4e4f94b7189c0e154473","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"833ed45503fc4a42b226b8e9c169f445","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n","\n","\n","\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"687c26177c284e90b35b131cd116c15c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Generating splits...', max=1.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea13a319a3cc469fa8fae04ae681cfcc","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Generating train examples...', max=1108752.0, style=Progr…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4779902375d48e4b4d7434d0c2f152e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Shuffling opus-train.tfrecord...', max=1108752.0, style=P…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\u001b[1mDataset opus downloaded and prepared to ./data/opus/medical/0.1.0. Subsequent calls will reuse this data.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hxO6uGamHNzK"},"source":["Notice that TFDS returns a generator *function*, not a generator. This is because in Python, you cannot reset generators so you cannot go back to a previously yielded value. During deep learning training, you use Stochastic Gradient Descent and don't actually need to go back -- but it is sometimes good to be able to do that, and that's where the functions come in. It is actually very common to use generator functions in Python -- e.g., `zip` is a generator function. You can read more about [Python generators](https://book.pythontips.com/en/latest/generators.html) to understand why we use them. Let's print a a sample pair from our train and eval data. Notice that the raw ouput is represented in bytes (denoted by the `b'` prefix) and these will be converted to strings internally in the next steps."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SBRfcN4OEiZY","executionInfo":{"status":"ok","timestamp":1612658651095,"user_tz":480,"elapsed":390518,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"049ba891-4c7e-41ce-f643-5480356c1d9f"},"source":["train_stream = train_stream_fn()\r\n","print(colored(\"train data(en, de) tuple:\", \"yellow\"), next(train_stream))\r\n","\r\n","eval_stream = eval_stream_fn()\r\n","print(colored(\"eval data (en, de) tuple:\", \"green\"), next(eval_stream))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\u001b[33mtrain data(en, de) tuple:\u001b[0m (b'Tel: +421 2 57 103 777\\n', b'Tel: +421 2 57 103 777\\n')\n","\u001b[32meval data (en, de) tuple:\u001b[0m (b'Lutropin alfa Subcutaneous use.\\n', b'Pulver zur Injektion Lutropin alfa Subkutane Anwendung\\n')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vcT2Cs1FJbZh"},"source":["<a name=\"1.2\"></a>\r\n","## 1.2  Tokenization and Formatting\r\n","\r\n","Now that we have imported our corpus, we will be preprocessing the sentences into a format that our model can accept. This will be composed of several steps:\r\n","\r\n","**Tokenizing the sentences using subword representations:** As you've learned in the earlier courses of this specialization, we want to represent each sentence as an array of integers instead of strings. For our application, we will use *subword* representations to tokenize our sentences. This is a common technique to avoid out-of-vocabulary words by allowing parts of words to be represented separately. For example, instead of having separate entries in your vocabulary for --\"fear\", \"fearless\", \"fearsome\", \"some\", and \"less\"--, you can simply store --\"fear\", \"some\", and \"less\"-- then allow your tokenizer to combine these subwords when needed. This allows it to be more flexible so you won't have to save uncommon words explicitly in your vocabulary (e.g. *stylebender*, *nonce*, etc). Tokenizing is done with the `trax.data.Tokenize()` command and we have provided you the combined subword vocabulary for English and German (i.e. `ende_32k.subword`) saved in the `data` directory. Feel free to open this file to see how the subwords look like."]},{"cell_type":"code","metadata":{"id":"Qp_uRh4tIVT5","executionInfo":{"status":"ok","timestamp":1612658651097,"user_tz":480,"elapsed":390515,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# global variables that state the filename and directory of the vocabulary file\r\n","VOCAB_FILE = \"ende_32k.subword\"\r\n","VOCAB_DIR = \"gs://trax-ml/vocabs/\"\r\n","\r\n","tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\r\n","tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)\r\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"688iOjJEMGtB"},"source":["**Append an end-of-sentence token to each sentence:** We will assign a token (i.e. in this case `1`) to mark the end of a sentence. This will be useful in inference/prediction so we'll know that the model has completed the translation."]},{"cell_type":"code","metadata":{"id":"ln7Bj0PJL2GG","executionInfo":{"status":"ok","timestamp":1612658651098,"user_tz":480,"elapsed":390511,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# Append EOS at the end of each sentence.\r\n","\r\n","# Integer assigned as end-of-sentence (EOS)\r\n","EOS = 1\r\n","\r\n","# generator helper function to append EOS to each sentence\r\n","def append_eos(stream):\r\n","    for (inputs, targets) in stream:\r\n","        inputs_with_eos = list(inputs) + [EOS]\r\n","        targets_with_eos = list(targets) + [EOS]\r\n","        yield np.array(inputs_with_eos), np.array(targets_with_eos)\r\n","\r\n","tokenized_train_stream = append_eos(tokenized_train_stream)\r\n","tokenized_eval_stream = append_eos(tokenized_eval_stream)\r\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D6UZaQYkOXbT"},"source":["**Filter long sentences:** We will place a limit on the number of tokens per sentence to ensure we won't run out of memory. This is done with the `trax.data.FilterByLength()` method and you can see its syntax below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S-Z7mfbgOUyh","executionInfo":{"status":"ok","timestamp":1612658651892,"user_tz":480,"elapsed":391300,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"30feff7f-2336-417a-f3fa-a49d22af4df9"},"source":["# Filter too long sentences to not run out of memory.\r\n","# length_keys=[0, 1] means we filter both English and German sentences, so\r\n","# both much be not longer that 256 tokens for training / 512 for eval.\r\n","filtered_train_stream = trax.data.FilterByLength(\r\n","    max_length=256, length_keys=[0, 1]\r\n",")(tokenized_train_stream)\r\n","filtered_eval_stream = trax.data.FilterByLength(\r\n","    max_length=512, length_keys=[0,1]\r\n",")(tokenized_eval_stream)\r\n","\r\n","train_input, train_target = next(filtered_train_stream)\r\n","print(colored(f\"Single tokenized example input:\", \"red\"), train_input)\r\n","print(colored(f\"Single tokenized example target:\", \"red\"), train_target)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["\u001b[31mSingle tokenized example input:\u001b[0m [ 2538  2248    30 12114 23184 16889     5     2 20852  6456 20592  5812\n","  3932    96  5178  3851    30  7891  3550 30650  4729   992     1]\n","\u001b[31mSingle tokenized example target:\u001b[0m [ 1872    11  3544    39  7019 17877 30432    23  6845    10 14222    47\n","  4004    18 21674     5 27467  9513   920   188 10630    18  3550 30650\n","  4729   992     1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wxrMOC1QVSMC"},"source":["<a name=\"1.3\"></a>\r\n","## 1.3  tokenize & detokenize helper functions\r\n","\r\n","Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your trax models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following: \r\n","\r\n","- <span style='color:blue'> word2Ind: </span> a dictionary mapping the word to its index.\r\n","- <span style='color:blue'> ind2Word:</span> a dictionary mapping the index to its word.\r\n","- <span style='color:blue'> word2Count:</span> a dictionary mapping the word to the number of times it appears. \r\n","- <span style='color:blue'> num_words:</span> total number of words that have appeared. \r\n","\r\n","Since you have already implemented these in previous assignments of the specialization, we will provide you with helper functions that will do this for you. Run the cell below to get the following functions:\r\n","\r\n","- <span style='color:blue'> tokenize(): </span> converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords (parts of words).\r\n","- <span style='color:blue'> detokenize(): </span> converts a token list to its corresponding sentence (i.e. string)."]},{"cell_type":"code","metadata":{"id":"f6vrw29LQtdw","executionInfo":{"status":"ok","timestamp":1612658651894,"user_tz":480,"elapsed":391297,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# Setup helper functions for tokenizing and detokenizing sentences\r\n","\r\n","\r\n","def tokenize(input_str, vocab_file=None, vocab_dir=None):\r\n","    \"\"\"Encodes a string to an array of integers\r\n","\r\n","    Args:\r\n","        input_str (str): human-readable string to encode\r\n","        vocab_file (str): filename of the vocabulary text file\r\n","        vocab_dir (str): path to the vocabulary file\r\n","\r\n","    Returns:\r\n","        numpy.ndarray: tokenized version of the input string\r\n","    \"\"\"\r\n","\r\n","    # Set the encoding of the \"end of sentence\" as 1\r\n","    EOS = 1\r\n","\r\n","    # Use the trax.data.tokenize method. It takes streams and returns streams,\r\n","    # we get around it by making a 1-element stream with `iter`.\r\n","    inputs = next(\r\n","        trax.data.tokenize(\r\n","            iter([input_str]), vocab_file=vocab_file, vocab_dir=vocab_dir\r\n","        )\r\n","    )\r\n","\r\n","    # Mark the end of the sentence with EOS\r\n","    inputs = list(inputs) + [EOS]\r\n","\r\n","    # Adding the batch dimension to the front of the shape\r\n","    batch_inputs = np.reshape(np.array(inputs), [1, -1])\r\n","\r\n","    return batch_inputs\r\n","\r\n","\r\n","def detokenize(integers, vocab_file=None, vocab_dir=None):\r\n","    \"\"\"Decodes an array of integers to a human readable string\r\n","\r\n","    Args:\r\n","        integers (numpy.ndarray): array of integers to decode\r\n","        vocab_file (str): filename of the vocabulary text file\r\n","        vocab_dir (str): path to the vocabulary file\r\n","\r\n","    Returns:\r\n","        str: the decoded sentence.\r\n","    \"\"\"\r\n","\r\n","    # Remove the dimensions of size 1\r\n","    integers = list(np.squeeze(integers))\r\n","\r\n","    # Set the encoding of the \"end of sentence\" as 1\r\n","    EOS = 1\r\n","\r\n","    # Remove the EOS to decode only the original tokens\r\n","    if EOS in integers:\r\n","        integers = integers[: integers.index(EOS)]\r\n","    return trax.data.detokenize(\r\n","        integers, vocab_file=vocab_file, vocab_dir=vocab_dir\r\n","    )\r\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-ZnboqZi55uo"},"source":["Let's see how we might use these functions:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ewkAYUAy56gO","executionInfo":{"status":"ok","timestamp":1612658653666,"user_tz":480,"elapsed":393064,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"4a696e81-6ea5-4f4d-84a9-cf15ba663a66"},"source":["# Detokenize an input-target pair of tokenized sentences\r\n","print(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\r\n","print(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\r\n","print()\r\n","\r\n","# Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.\r\n","# See how it combines the subwords -- 'hell' and 'o'-- to form the word 'hello'.\r\n","print(colored(f\"tokenize('hello'): \", 'green'), tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\r\n","print(colored(f\"detokenize([17332, 140, 1]): \", 'green'), detokenize([17332, 140, 1], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\u001b[31mSingle detokenized example input:\u001b[0m During treatment with olanzapine, adolescents gained significantly more weight compared with adults.\n","\n","\u001b[31mSingle detokenized example target:\u001b[0m Während der Behandlung mit Olanzapin nahmen die Jugendlichen im Vergleich zu Erwachsenen signifikant mehr Gewicht zu.\n","\n","\n","\u001b[32mtokenize('hello'): \u001b[0m [[17332   140     1]]\n","\u001b[32mdetokenize([17332, 140, 1]): \u001b[0m hello\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t__Cg-lC6JiN"},"source":["<a name=\"1.4\"></a>\r\n","## 1.4  Bucketing\r\n","\r\n","Bucketing the tokenized sentences is an important technique used to speed up training in NLP.\r\n","Here is a \r\n","[nice article describing it in detail](https://medium.com/@rashmi.margani/how-to-speed-up-the-training-of-the-sequence-model-using-bucketing-techniques-9e302b0fd976)\r\n","but the gist is very simple. Our inputs have variable lengths and you want to make these the same when batching groups of sentences together. One way to do that is to pad each sentence to the length of the longest sentence in the dataset. This might lead to some wasted computation though. For example, if there are multiple short sentences with just two tokens, do we want to pad these when the longest sentence is composed of a 100 tokens? Instead of padding with 0s to the maximum length of a sentence each time, we can group our tokenized sentences by length and bucket, as on this image (from the article above):\r\n","\r\n","![alt text](https://miro.medium.com/max/700/1*hcGuja_d5Z_rFcgwe9dPow.png)\r\n","\r\n","We batch the sentences with similar length together (e.g. the blue sentences in the image above) and only add minimal padding to make them have equal length (usually up to the nearest power of two). This allows to waste less computation when processing padded sequences.\r\n","In Trax, it is implemented in the [bucket_by_length](https://github.com/google/trax/blob/5fb8aa8c5cb86dabb2338938c745996d5d87d996/trax/supervised/inputs.py#L378) function."]},{"cell_type":"code","metadata":{"id":"wWsSCaHl56zR","executionInfo":{"status":"ok","timestamp":1612658653667,"user_tz":480,"elapsed":393060,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# Bucketing to create streams of batches.\r\n","\r\n","# Buckets are defined in terms of boundaries and batch sizes.\r\n","# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\r\n","# So below, we'll take a batch of 256 sentences of length < 8, 128 if length is\r\n","# between 8 and 16, and so on -- and only 2 if length is over 512.\r\n","boundaries =  [8,   16,  32, 64, 128, 256, 512]\r\n","batch_sizes = [256, 128, 64, 32, 16,    8,   4,  2]\r\n","\r\n","# Create the generators.\r\n","train_batch_stream = trax.data.BucketByLength(\r\n","    boundaries, batch_sizes,\r\n","    length_keys=[0, 1]  # As before: count inputs and targets to length.\r\n",")(filtered_train_stream)\r\n","\r\n","eval_batch_stream = trax.data.BucketByLength(\r\n","    boundaries, batch_sizes,\r\n","    length_keys=[0, 1]  # As before: count inputs and targets to length.\r\n",")(filtered_eval_stream)\r\n","\r\n","# Add masking for the padding (0s).\r\n","train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\r\n","eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5wcm_J8R6sEJ"},"source":["<a name=\"1.5\"></a>\r\n","## 1.5  Exploring the data\r\n","\r\n","We will now be displaying some of our data. You will see that the functions defined above (i.e. `tokenize()` and `detokenize()`) do the same things you have been doing again and again throughout the specialization. We gave these so you can focus more on building the model from scratch. Let us first get the data generator and get one batch of the data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vn6G-j4n56wf","executionInfo":{"status":"ok","timestamp":1612660955133,"user_tz":480,"elapsed":771,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"508d26ec-383c-4df4-cdaf-c235568584e5"},"source":["input_batch, target_batch, mask_batch = next(train_batch_stream)\r\n","\r\n","# let's see the data type of a batch\r\n","print(\"input_batch data type: \", type(input_batch))\r\n","print(\"target_batch data type: \", type(target_batch))\r\n","\r\n","# let's see the shape of this particular batch (batch length, sentence length)\r\n","print(\"input_batch shape: \", input_batch.shape)\r\n","print(\"target_batch shape: \", target_batch.shape)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["input_batch data type:  <class 'numpy.ndarray'>\n","target_batch data type:  <class 'numpy.ndarray'>\n","input_batch shape:  (16, 128)\n","target_batch shape:  (16, 128)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5dATDsJX-F-G"},"source":["The `input_batch` and `target_batch` are Numpy arrays consisting of tokenized English sentences and German sentences respectively. These tokens will later be used to produce embedding vectors for each word in the sentence (so the embedding for a sentence will be a matrix). The number of sentences in each batch is usually a power of 2 for optimal computer memory usage. \r\n","\r\n","We can now visually inspect some of the data. You can run the cell below several times to shuffle through the sentences. Just to note, while this is a standard data set that is used widely, it does have some known wrong translations. With that, let's pick a random sentence and print its tokenized representation."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bMCjWqeh56tY","executionInfo":{"status":"ok","timestamp":1612665217741,"user_tz":480,"elapsed":1211,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"da491235-ff4e-45b0-c8d2-6dbf6cac24bb"},"source":["# pick a random index less than the batch size.\r\n","index = random.randrange(len(input_batch))\r\n","\r\n","# use the index to grab an entry from the input and target batch\r\n","print(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\r\n","print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\r\n","print(colored('THIS IS THE GERMAN TRANSLATION: \\n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\r\n","print(colored('THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \\n', 'red'), target_batch[index], '\\n')"],"execution_count":30,"outputs":[{"output_type":"stream","text":["\u001b[31mTHIS IS THE ENGLISH SENTENCE: \n","\u001b[0m Consideration should also be given to interrupting treatment with tenofovir disoproxil fumarate in patients with creatinine clearance decreased to < 50 ml/ min or decreases in serum phosphate to < 1.0 mg/ dl (0.32 mmol/ l).\n"," \n","\n","\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n"," \u001b[0m [32505 11290   596   117    90    32   621     9 19982    81  2248    30\n","  4234  5506  7250  4811  6175 28098  5175   215  5283  4434  1865     6\n","  4996    30 24387  9508   510 29988  2713 24351   103     9   909 33287\n","   913   563  7272     5  6722  5251    66 24351    33     6  3797    77\n"," 11013  9342  1865     9   909 33287   913   135     3   266 23306     5\n","  6722 20446     5    50   266     3  3348 17050  6078  6722   215 33022\n"," 30650  4729   992     1     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0] \n","\n","\u001b[31mTHIS IS THE GERMAN TRANSLATION: \n","\u001b[0m Bei Patienten, bei denen die Kreatinin-Clearance auf < 50 ml/min oder die Serumphosphat-Konzentration auf < 1,0 mg/dl (0,32 mmol/l) gesunken ist, sollte außerdem eine Unterbrechung der Therapie mit Tenofovirdisoproxilfumarat erwogen werden.\n"," \n","\n","\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \n","\u001b[0m [  752  5122     2   113   402    10  7504  8384  8232     5    15 21769\n","  2713    37   909 33287   913   563  7272     5   123  5251    97    10\n"," 14062  9300 28917    91    15 21427    37   909 33287   913   135   227\n","   266 23306     5   123 20446     5    50   266   227  3348 17050  6078\n","   123   215    80 15237    24     2   357  3316    41 30889     5    11\n"," 15761 24780    39  9744  5506  7250  5024  6175 28098  5175  7282  3018\n"," 24774     5 24058  6835    58  3550 30650  4729   992     1     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0] \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qqLkvLygFaAW"},"source":["<a name=\"2\"></a>\r\n","# Part 2:  Neural Machine Translation with Attention\r\n","\r\n","Now that you have the data generators and have handled the preprocessing, it is time for you to build the model. You will be implementing a neural machine translation model from scratch with attention."]},{"cell_type":"markdown","metadata":{"id":"I2kZLKVxFdyP"},"source":["<a name=\"2.1\"></a>\r\n","## 2.1  Attention Overview\r\n","\r\n","The model we will be building uses an encoder-decoder architecture. This Recurrent Neural Network (RNN) will take in a tokenized version of a sentence in its encoder, then passes it on to the decoder for translation. As mentioned in the lectures, just using a a regular sequence-to-sequence model with LSTMs will work effectively for short to medium sentences but will start to degrade for longer ones. You can picture it like the figure below where all of the context of the input sentence is compressed into one vector that is passed into the decoder block. You can see how this will be an issue for very long sentences (e.g. 100 tokens or more) because the context of the first parts of the input will have very little effect on the final vector passed to the decoder.\r\n","\r\n","<img src='https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/images/plain_rnn.png' width=\"500px\">\r\n","\r\n","Adding an attention layer to this model avoids this problem by giving the decoder access to all parts of the input sentence. To illustrate, let's just use a 4-word input sentence as shown below. Remember that a hidden state is produced at each timestep of the encoder (represented by the orange rectangles). These are all passed to the attention layer and each are given a score given the current activation (i.e. hidden state) of the decoder. For instance, let's consider the figure below where the first prediction \"Wie\" is already made. To produce the next prediction, the attention layer will first receive all the encoder hidden states (i.e. orange rectangles) as well as the decoder hidden state when producing the word \"Wie\" (i.e. first green rectangle). Given these information, it will score each of the encoder hidden states to know which one the decoder should focus on to produce the next word. The result of the model training might have learned that it should align to the second encoder hidden state and subsequently assigns a high probability to the word \"geht\". If we are using greedy decoding, we will output the said word as the next symbol, then restart the process to produce the next word until we reach an end-of-sentence prediction.\r\n","\r\n","<img src='https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/images/attention_overview.png' width=\"600px\">\r\n","\r\n","\r\n","There are different ways to implement attention and the one we'll use for this assignment is the Scaled Dot Product Attention which has the form:\r\n","\r\n","$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\r\n","\r\n","You will dive deeper into this equation in the next week but for now, you can think of it as computing scores using queries (Q) and keys (K), followed by a multiplication of values (V) to get a context vector at a particular timestep of the decoder. This context vector is fed to the decoder RNN to get a set of probabilities for the next predicted word. The division by square root of the keys dimensionality ($\\sqrt{d_k}$) is for improving model performance and you'll also learn more about it next week. For our machine translation application, the encoder activations (i.e. encoder hidden states) will be the keys and values, while the decoder activations (i.e. decoder hidden states) will be the queries.\r\n","\r\n","You will see in the upcoming sections that this complex architecture and mechanism can be implemented with just a few lines of code. Let's get started!"]},{"cell_type":"markdown","metadata":{"id":"EyB6PXKvSUUn"},"source":["<a name=\"2.2\"></a>\r\n","## 2.2  Helper functions\r\n","\r\n","We will first implement a few functions that we will use later on. These will be for the input encoder, pre-attention decoder, and preparation of the queries, keys, values, and mask.\r\n","\r\n","### 2.2.1 Input encoder\r\n","\r\n","The input encoder runs on the input tokens, creates its embeddings, and feeds it to an LSTM network. This outputs the activations that will be the keys and values for attention. It is a [Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial) network which uses:\r\n","\r\n","   - [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding): Converts each token to its vector representation. In this case, it is the the size of the vocabulary by the dimension of the model: `tl.Embedding(vocab_size, d_model)`. `vocab_size` is the number of entries in the given vocabulary. `d_model` is the number of elements in the word embedding.\r\n","  \r\n","   - [tl.LSTM](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM): LSTM layer of size `d_model`. We want to be able to configure how many encoder layers we have so remember to create LSTM layers equal to the number of the `n_encoder_layers` parameter.\r\n","   \r\n","<img src=\"https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/images/input_encoder.png\">\r\n","\r\n","<a name=\"ex01\"></a>\r\n","### Exercise 01\r\n","\r\n","**Instructions:** Implement the `input_encoder_fn` function."]},{"cell_type":"code","metadata":{"id":"I08c1fZz56p7","executionInfo":{"status":"ok","timestamp":1612665692651,"user_tz":480,"elapsed":831,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# UNQ_C1\r\n","# GRADED FUNCTION\r\n","def input_encoder_fn(input_vocab_size, d_model, n_encoder_layers):\r\n","    \"\"\" Input encoder runs on the input sentence and creates\r\n","    activations that will be the keys and values for attention.\r\n","    \r\n","    Args:\r\n","        input_vocab_size: int: vocab size of the input\r\n","        d_model: int:  depth of embedding (n_units in the LSTM cell)\r\n","        n_encoder_layers: int: number of LSTM layers in the encoder\r\n","    Returns:\r\n","        tl.Serial: The input encoder\r\n","    \"\"\"\r\n","    \r\n","    # create a serial network\r\n","    input_encoder = tl.Serial( \r\n","        \r\n","        ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###\r\n","        # create an embedding layer to convert tokens to vectors\r\n","        tl.Embedding(input_vocab_size, d_model),\r\n","        \r\n","        # feed the embeddings to the LSTM layers. It is a stack of n_encoder_layers LSTM layers\r\n","        [tl.LSTM(d_model) for _ in n_encoder_layers]\r\n","        ### END CODE HERE ###\r\n","    )\r\n","\r\n","    return input_encoder"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hHOeXrm2XcGQ"},"source":["*Note: To make this notebook more neat, we moved the unit tests to a separate file called `w1_unittest.py`. Feel free to open it from your workspace if needed. Just click `File` on the upper left corner of this page then `Open` to see your Jupyter workspace directory. From there, you can see `w1_unittest.py` and you can open it in another tab or download to see the unit tests. We have placed comments in that file to indicate which functions are testing which part of the assignment (e.g. `test_input_encoder_fn()` has the unit tests for UNQ_C1).*"]},{"cell_type":"code","metadata":{"id":"ZJ2pt-Gh56mh","executionInfo":{"status":"ok","timestamp":1612658653671,"user_tz":480,"elapsed":393044,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":[""],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"S872Fy2T56jw","executionInfo":{"status":"ok","timestamp":1612658653672,"user_tz":480,"elapsed":393041,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":[""],"execution_count":10,"outputs":[]}]}