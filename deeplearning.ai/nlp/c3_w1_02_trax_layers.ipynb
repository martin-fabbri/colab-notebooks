{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c3_w1_02_trax_layers.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMgFMSxE1lIjPVm1sPHb+PJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"zSC4IYh3010_"},"source":["# Trax Layers\n","\n","This notebook introduces the core concepts of the Trax library through a series of code samples and explanations. The topics covered in following sections are:\n","\n","1. [Layers](#1): the basic building blocks and how to combine them\n","1. [Inputs and Outputs](#2): how data streams flow through layers\n","1. Defining New Layer Classes (if combining existing layers isnâ€™t enough)\n","1. Testing and Debugging Layer Classes"]},{"cell_type":"code","metadata":{"id":"q9Q2zWTIzV7z"},"source":["#@title Install dependencies\n","#@markdown - Trax\n","%%capture\n","!pip install -Uqq trax"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bel6sAVg2Kuo","executionInfo":{"status":"ok","timestamp":1611386006863,"user_tz":480,"elapsed":921,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"459420b0-da79-4f51-e197-dba31bff24fd"},"source":["#@title Import packages\n","import os\n","\n","import numpy as np\n","import tensorflow as tf\n","import trax\n","from trax import fastmath\n","from trax import layers as tl\n","from trax import shapes\n","from trax.fastmath import numpy as jnp\n","from trax.shapes import ShapeDtype, signature\n","\n","np.set_printoptions(precision=3)\n","\n","print(\"numpy      \", np.__version__)\n","print(\"tensorflow \", tf.__version__)\n","!pip list | grep trax"],"execution_count":38,"outputs":[{"output_type":"stream","text":["numpy       1.19.5\n","tensorflow  2.4.0\n","trax                          1.3.7                \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ikfFmjKdV_oH"},"source":["def show_layer_properties(layer_obj, layer_name):\n","    template = (\n","        f\"{layer_name}.n_in:      {layer_obj.n_in}\\n\"\n","        f\"{layer_name}.n_out:     {layer_obj.n_out}\\n\"\n","        f\"{layer_name}.sublayers: {layer_obj.sublayers}\\n\"\n","        f\"{layer_name}.weights:   {layer_obj.weights}\\n\"\n","    )\n","    print(template)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w-KemeiA1c1p"},"source":["<a name='1'></a>\n","## 1. Layers\n","\n","The Layer class represents Trax's basic building blocks.\n","\n","> The inputs and outputs are NumPy arrays or JAX objects behaving as numpy arrays."]},{"cell_type":"markdown","metadata":{"id":"0U4rQadsdPzq"},"source":["$tl.Relu[n_{in}=1, n_{out}=1]$"]},{"cell_type":"code","metadata":{"id":"CMObhWjw2gCk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611375753212,"user_tz":480,"elapsed":382,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"b31cef95-0965-4348-9293-eca5802c8d22"},"source":["relu = tl.Relu()\n","x = np.array([[-2, -1, 0, 1, 2], [-20, -10, 0, 10, 20]])\n","y = relu(x)\n","print(\n","    f\"x:\\n{x}\\n\\n\"\n","    f\"relu(x):\\n{y}\\n\\n\"\n","    f\"Number of inputs expected by this layer: {relu.n_in}\\n\"\n","    f\"Number of outputs promised by this layer: {relu.n_out}\"\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x:\n","[[ -2  -1   0   1   2]\n"," [-20 -10   0  10  20]]\n","\n","relu(x):\n","[[ 0  0  0  1  2]\n"," [ 0  0  0 10 20]]\n","\n","Number of inputs expected by this layer: 1\n","Number of outputs promised by this layer: 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xHLEpJysfRPQ"},"source":["$tl.Concatenate[n_{in}=2, n_{out}=1]$"]},{"cell_type":"code","metadata":{"id":"xAEZqAMEz6nC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611376195459,"user_tz":480,"elapsed":378,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"30125362-0408-4af7-d139-f53c339216ae"},"source":["concat = tl.Concatenate()\n","x0 = np.array([[1, 2, 3], [7, 8, 9]])\n","x1 = np.array([[4, 5, 6], [10, 11, 12]])\n","y = concat([x0, x1])\n","print(\n","    f\"x0:\\n{x0}\\n\\n\"\n","    f\"x1:\\n{x1}\\n\\n\"\n","    f\"concat([x1, x2]):\\n{y}\\n\\n\"\n","    f\"Number of inputs expected by this layer: {concat.n_in}\\n\"\n","    f\"Number of outputs promised by this layer: {concat.n_out}\"\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x0:\n","[[1 2 3]\n"," [7 8 9]]\n","\n","x1:\n","[[ 4  5  6]\n"," [10 11 12]]\n","\n","concat([x1, x2]):\n","[[ 1  2  3  4  5  6]\n"," [ 7  8  9 10 11 12]]\n","\n","Number of inputs expected by this layer: 2\n","Number of outputs promised by this layer: 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RQoIy-Kiggg1"},"source":["### 1.1. Layers are configurable\n","\n","Many layer types have creation-time parameters for flexibility. The Concatenate layer type, for instance, has two optional parameters:\n","\n","- `axis`: index of axis along which to concatenate the tensors; default value of -1 means to use the last axis.\n","- `n_items`: number of tensors to join into one by concatenation; default value is 2.\n","\n","The following example shows `Concatenate` configured for 3 input tensors, and concatenation along the initial ($0^{th}$) axis.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tuNu7ZYuhjzt"},"source":["$tl.Concatenate[n_{items}=3, axis=0]$"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mljACJ_zfpl8","executionInfo":{"status":"ok","timestamp":1611376697527,"user_tz":480,"elapsed":438,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"153e1f64-29ce-4a56-97d1-ebb656f29ee4"},"source":["concat3 = tl.Concatenate(n_items=3, axis=0)\n","\n","x0 = np.array([[1, 2, 3], [4, 5, 6]])\n","x1 = np.array([[10, 20, 30], [40, 50, 60]])\n","x2 = np.array([[100, 200, 300], [400, 500, 600]])\n","\n","y = concat3([x0, x1, x2])\n","\n","print(\n","    f\"x0:\\n{x0}\\n\\n\"\n","    f\"x1:\\n{x1}\\n\\n\"\n","    f\"x2:\\n{x2}\\n\\n\"\n","    f\"concat3([x0, x1, x2]):\\n{y}\"\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x0:\n","[[1 2 3]\n"," [4 5 6]]\n","\n","x1:\n","[[10 20 30]\n"," [40 50 60]]\n","\n","x2:\n","[[100 200 300]\n"," [400 500 600]]\n","\n","concat3([x0, x1, x2]):\n","[[  1   2   3]\n"," [  4   5   6]\n"," [ 10  20  30]\n"," [ 40  50  60]\n"," [100 200 300]\n"," [400 500 600]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KzksgsGAlVZ3"},"source":["### 1.2. Layers are trainable.\n","Many layer types include weights that affect the computation of outputs from inputs, and they use back-progagated gradients to update those weights.\n","\n","ðŸš§ðŸš§ A very small subset of layer types, such as ``BatchNorm``, also include modifiable weights (called ``state``) that are updated based on forward-pass inputs/computation rather than back-propagated gradients.\n","\n","Initialization\n","\n","Trainable layers must be initialized before use. Trax can take care of this as part of the overall training process. In other settings (e.g., in tests or interactively in a Colab notebook), you need to initialize the outermost/topmost layer explicitly. For this, use init:"]},{"cell_type":"markdown","metadata":{"id":"-6QizVmhmRPV"},"source":["$tl.Concatenate[n_{items}=3, axis=0]$"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-mXxiKeblSsT","executionInfo":{"status":"ok","timestamp":1611378580598,"user_tz":480,"elapsed":1316,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"130e024d-143a-4b23-97e0-0448a82a5c15"},"source":["layer_norm = tl.LayerNorm()\n","\n","x = np.array([[-2, -1, 0, 1, 2], [1, 2, 3, 4, 5], [10, 20, 30, 40, 50]]).astype(\n","    np.float32\n",")\n","\n","layer_norm.init(shapes.signature(x))\n","\n","y = layer_norm(x)\n","\n","print(\n","    f\"x:\\n{x}\\n\\n\"\n","    f\"layer_norm(x):\\n{y}\\n\"\n","    f\"layer_norm.weights:\\n{layer_norm.weights}\"\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x:\n","[[-2. -1.  0.  1.  2.]\n"," [ 1.  2.  3.  4.  5.]\n"," [10. 20. 30. 40. 50.]]\n","\n","layer_norm(x):\n","[[-1.414 -0.707  0.     0.707  1.414]\n"," [-1.414 -0.707  0.     0.707  1.414]\n"," [-1.414 -0.707  0.     0.707  1.414]]\n","layer_norm.weights:\n","(DeviceArray([1., 1., 1., 1., 1.], dtype=float32), DeviceArray([0., 0., 0., 0., 0.], dtype=float32))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yFFkc_c3qBWx"},"source":["### 1.3. Layers combine into layers.\n","\n","The Trax library authors encourage users to build networks and network components as combinations of existing layers, by means of a small set of combinator layers. A combinator makes a list of layers behave as a single layer â€“ by combining the sublayer computations yet looking from the outside like any other layer. The combined layer, like other layers, can:\n","\n","- compute outputs from inputs,\n","- update parameters from gradients, and\n","- combine with yet more layers.\n"]},{"cell_type":"markdown","metadata":{"id":"5LajAf2OqlKu"},"source":["Combine with ``Serial``<br>\n","$h(.) = g(f(.))$\n","\n","```python\n","layer_f = Serial(\n","    layer_f,\n","    layer_g,\n",")\n","```"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"79iaN37nlSo3","executionInfo":{"status":"ok","timestamp":1611379597945,"user_tz":480,"elapsed":347,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"873c103c-a92d-4535-b179-4005d72de447"},"source":["layer_block = tl.Serial(\n","    tl.Relu(),\n","    tl.LayerNorm(),\n",")\n","\n","x = np.array([[-2, -1, 0, 1, 2],\n","              [-20, -10, 0, 10, 20]]).astype(np.float32)\n","\n","layer_block.init(shapes.signature(x))\n","y = layer_block(x)\n","\n","print(\n","    f'x:\\n{x}\\n\\n'\n","    f'layer_block(x):\\n{y}'\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x:\n","[[ -2.  -1.   0.   1.   2.]\n"," [-20. -10.   0.  10.  20.]]\n","\n","layer_block(x):\n","[[-0.75 -0.75 -0.75  0.5   1.75]\n"," [-0.75 -0.75 -0.75  0.5   1.75]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CeEuCnWFlSlc","executionInfo":{"status":"ok","timestamp":1611379801315,"user_tz":480,"elapsed":343,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"eac6a618-20a2-4956-e762-301b60aaee84"},"source":["print(\n","    f\"layer_block:         {layer_block}\\n\\n\"\n","    f\"layer_block.weights: {layer_block.weights}\"\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["layer_block:         Serial[\n","  Serial[\n","    Relu\n","  ]\n","  LayerNorm\n","]\n","\n","layer_block.weights: (((), (), ()), (DeviceArray([1., 1., 1., 1., 1.], dtype=float32), DeviceArray([0., 0., 0., 0., 0.], dtype=float32)))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y19lWCZDuR9M"},"source":["Combine with ``Branch``\n","\n","The Branch combinator arranges layers into parallel computational channels.\n","\n","```python\n","def Residual(*layers, shortcut=None):\n","    layers = _ensure_flat(layers)\n","    layer = layers[0] if len(layers) == 1 else Serial(layers)\n","    return Serial(\n","        Branch(shortcut, layer),\n","        Add(),\n","    )\n","```"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SdBu3Q0QlSR9","executionInfo":{"status":"ok","timestamp":1611380599455,"user_tz":480,"elapsed":365,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"2dabdcf4-45b2-4eb8-bebb-3a1125ba05a6"},"source":["relu = tl.Relu()\n","times_100 = tl.Fn(\"Times100\", lambda x: x * 100.0)\n","branch_relu_t100 = tl.Branch(relu, times_100)\n","\n","x = np.array([[-2, -1, 0, 1, 2],\n","              [-20, -10, 0, 10, 20]])\n","branch_relu_t100.init(shapes.signature(x))\n","\n","y0, y1 = branch_relu_t100(x)\n","\n","print(\n","    f\"x:\\n{x}\\n\\n\"\n","    f\"y0:\\n{y0}\\n\\n\"\n","    f\"y1:\\n{y1}\"\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x:\n","[[ -2  -1   0   1   2]\n"," [-20 -10   0  10  20]]\n","\n","y0:\n","[[ 0  0  0  1  2]\n"," [ 0  0  0 10 20]]\n","\n","y1:\n","[[ -200.  -100.     0.   100.   200.]\n"," [-2000. -1000.     0.  1000.  2000.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ip9ke4-Dxlv6"},"source":["<a name='2'></a>\n","## 2. Inputs and Outputs\n","\n","Trax allows layers to have multiple input streams and output streams. When\n","designing a network, you have the flexibility to use layers that:\n","\n","  - process a single data stream ($n_{in} = n_{out} = 1$),\n","  - process multiple parallel data streams ($n_{in} = n_{out} = 2, 3, ... $),\n","  - split or inject data streams ($n_{in} < n_{out}$), or\n","  - merge or remove data streams ($n_{in} > n_{out}$).\n","\n","We saw in section 1 the example of `Residual`, which involves both a split and a merge:\n","```\n","  ...\n","  return Serial(\n","      Branch(shortcut, layer),\n","      Add(),\n","  )\n","```\n","In other words, layer by layer:\n","\n","  - `Branch(shortcut, layers)`: makes two copies of the single incoming data stream, passes one copy via the shortcut (typically a no-op), and processes the other copy via the given layers (applied in series). [$n_{in} = 1$, $n_{out} = 2$]\n","  - `Add()`: combines the two streams back into one by adding two tensors elementwise. [$n_{in} = 2$, $n_{out} = 1$]"]},{"cell_type":"markdown","metadata":{"id":"pjsEAl_ozhTX"},"source":["**Simple Case 1 -- Each layer takes one input and has one output.**\n","\n","This is in effect a single data stream pipeline, and the successive layers\n","behave like function composition:\n","\n","```\n","#  s(.) = h(g(f(.)))\n","layer_s = Serial(\n","    layer_f,\n","    layer_g,\n","    layer_h,\n",")\n","```\n","Note how, inside `Serial`, function composition is expressed naturally as a\n","succession of operations, so that no nested parentheses are needed and the\n","order of operations matches the textual order of layers."]},{"cell_type":"markdown","metadata":{"id":"KV1jlUnHzq-z"},"source":["**Simple Case 2 -- Each layer consumes all outputs of the preceding layer.**\n","\n","This is still a single pipeline, but data streams internal to it can split and\n","merge. The `Residual` example above illustrates this kind.\n"]},{"cell_type":"markdown","metadata":{"id":"j0f3S1hj0C2h"},"source":["**General Case -- Successive layers interact via the data stack.**\n","\n","As described in the `Serial` class docstring, each layer gets its inputs from\n","the data stack after the preceding layer has put its outputs onto the stack.\n","This covers the simple cases above, but also allows for more flexible data\n","interactions between non-adjacent layers. The following example is schematic:\n","```\n","x, y_target = get_batch_of_labeled_data()\n","\n","model_plus_eval = Serial(\n","    my_fancy_deep_model(),  # Takes one arg (x) and has one output (y_hat)\n","    my_eval(),  # Takes two args (y_hat, y_target) and has one output (score)\n",")\n","\n","eval_score = model_plus_eval((x, y_target))\n","```"]},{"cell_type":"markdown","metadata":{"id":"6YloPdWy0d5T"},"source":["Here is the corresponding progression of stack states:\n","\n","0. At start: _--empty--_\n","0. After `get_batch_of_labeled_data()`: *x*, *y_target*\n","0. After `my_fancy_deep_model()`: *y_hat*, *y_target*\n","0. After `my_eval()`: *score*\n","\n","Note in particular how the application of the model (between stack states 1\n","and 2) only uses and affects the top element on the stack: `x` --> `y_hat`.\n","The rest of the data stack (`y_target`) comes in use only later, for the\n","eval function."]},{"cell_type":"markdown","metadata":{"id":"EGzp1vE50wPp"},"source":["## 3. Defining New Layer Classes\n","\n","If you need a layer type that is not easily defined as a combination of\n","existing layer types, you can define your own layer classes in a couple\n","different ways."]},{"cell_type":"markdown","metadata":{"id":"mgV2HViG00qr"},"source":["**Example 7.** Use `Fn` to define a new layer type:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5hXfKx-_xKjF","executionInfo":{"status":"ok","timestamp":1611382084346,"user_tz":480,"elapsed":603,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"d7564304-eff3-40f9-bc95-05a237543141"},"source":["def Gcd():\n","    \"\"\"returns a layer to compute the greatest commom divisor, elemementwise.\"\"\"\n","    return tl.Fn(\"Gcd\", lambda x0, x1: jnp.gcd(x0, x1))\n","\n","\n","gcd = Gcd()\n","\n","x0 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n","x1 = np.array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n","\n","y = gcd((x0, x1))\n","\n","print(f\"x0:\\n{x0}\\n\\n\" f\"x1:\\n{x1}\\n\\n\" f\"gcd((x0, x1)):\\n{y}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x0:\n","[ 1  2  3  4  5  6  7  8  9 10]\n","\n","x1:\n","[11 12 13 14 15 16 17 18 19 20]\n","\n","gcd((x0, x1)):\n","[ 1  2  1  2  5  2  1  2  1 10]\n"],"name":"stdout"}]}]}