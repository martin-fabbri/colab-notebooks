{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c3_w3_02_assignment_ner.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMAfmuYWfj0o2qMPx9MAa12"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"m6EIpHa9D_uk"},"source":["# Assignment 3 - Named Entity Recognition (NER)\r\n","\r\n","Welcome to the third programming assignment of Course 3. In this assignment, you will learn to build more complicated models with Trax. By completing this assignment, you will be able to: \r\n","\r\n","- Design the architecture of a neural network, train it, and test it. \r\n","- Process features and represents them\r\n","- Understand word padding\r\n","- Implement LSTMs\r\n","- Test with your own sentence\r\n","\r\n","## Outline\r\n","- [Introduction](#0)\r\n","- [Part 1:  Exploring the data](#1)\r\n","    - [1.1  Importing the Data](#1.1)\r\n","    - [1.2  Data generator](#1.2)\r\n","\t\t- [Exercise 01](#ex01)\r\n","- [Part 2:  Building the model](#2)\r\n","\t- [Exercise 02](#ex02)\r\n","- [Part 3:  Train the Model ](#3)\r\n","\t- [Exercise 03](#ex03)\r\n","- [Part 4:  Compute Accuracy](#4)\r\n","\t- [Exercise 04](#ex04)\r\n","- [Part 5:  Testing with your own sentence](#5)"]},{"cell_type":"markdown","metadata":{"id":"pW518F1PEEFu"},"source":["<a name=\"0\"></a>\r\n","# Introduction\r\n","\r\n","We first start by defining named entity recognition (NER). NER is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times, etc. \r\n","\r\n","For example:\r\n","\r\n","<img src=\"https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/images/ner.png\" width=\"500px\"/>\r\n","\r\n","Is labeled as follows: \r\n","\r\n","- French: geopolitical entity\r\n","- Morocco: geographic entity \r\n","- Christmas: time indicator\r\n","\r\n","Everything else that is labeled with an `O` is not considered to be a named entity. In this assignment, you will train a named entity recognition system that could be trained in a few seconds (on a GPU) and will get around 75% accuracy. Then, you will load in the exact version of your model, which was trained for a longer period of time. You could then evaluate the trained version of your model to get 96% accuracy! Finally, you will be able to test your named entity recognition system with your own sentence."]},{"cell_type":"code","metadata":{"id":"8IjjHkZjFTVn","executionInfo":{"status":"ok","timestamp":1611808367828,"user_tz":480,"elapsed":21345,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["%%capture\r\n","!pip -q install trax==1.3.1"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"b1wp0NYvRJgf","executionInfo":{"status":"ok","timestamp":1611808945558,"user_tz":480,"elapsed":1583,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["%%capture\r\n","!wget https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/datasets/ner_dataset.csv\r\n","!wget https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/datasets/ner-data.tar.gz\r\n","!tar -xvf ner-data.tar.gz"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1feWRYDQDwus","executionInfo":{"status":"ok","timestamp":1611808385245,"user_tz":480,"elapsed":844,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"d6b5fc22-d670-47ae-c664-27004daaa1e3"},"source":["import os\r\n","import random as rnd\r\n","\r\n","import numpy as np\r\n","import pandas as pd\r\n","import trax\r\n","from trax import layers as tl\r\n","\r\n","trax.supervised.trainer_lib.init_random_number_generators(33)"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DeviceArray([ 0, 33], dtype=uint32)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"jcPi_PSxF-mf","executionInfo":{"status":"ok","timestamp":1611808375005,"user_tz":480,"elapsed":28508,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["def get_vocab(vocab_path, tags_path):\r\n","    vocab = {}\r\n","    with open(vocab_path) as f:\r\n","        for i, l in enumerate(f.read().splitlines()):\r\n","            vocab[l] = i  # to avoid the 0\r\n","        # loading tags (we require this to map tags to their indices)\r\n","    vocab['<PAD>'] = len(vocab) # 35180\r\n","    tag_map = {}\r\n","    with open(tags_path) as f:\r\n","        for i, t in enumerate(f.read().splitlines()):\r\n","            tag_map[t] = i \r\n","    \r\n","    return vocab, tag_map\r\n","\r\n","def get_params(vocab, tag_map, sentences_file, labels_file):\r\n","    sentences = []\r\n","    labels = []\r\n","\r\n","    with open(sentences_file) as f:\r\n","        for sentence in f.read().splitlines():\r\n","            # replace each token by its index if it is in vocab\r\n","            # else use index of UNK_WORD\r\n","            s = [vocab[token] if token in vocab \r\n","                 else vocab['UNK']\r\n","                 for token in sentence.split(' ')]\r\n","            sentences.append(s)\r\n","\r\n","    with open(labels_file) as f:\r\n","        for sentence in f.read().splitlines():\r\n","            # replace each label by its index\r\n","            l = [tag_map[label] for label in sentence.split(' ')] # I added plus 1 here\r\n","            labels.append(l) \r\n","    return sentences, labels, len(sentences)\r\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jOLmFI3qHoyP"},"source":["<a name=\"1\"></a>\r\n","# Part 1:  Exploring the data\r\n","\r\n","We will be using a dataset from Kaggle, which we will preprocess for you. The original data consists of four columns, the sentence number, the word, the part of speech of the word, and the tags.  A few tags you might expect to see are: \r\n","\r\n","* geo: geographical entity\r\n","* org: organization\r\n","* per: person \r\n","* gpe: geopolitical entity\r\n","* tim: time indicator\r\n","* art: artifact\r\n","* eve: event\r\n","* nat: natural phenomenon\r\n","* O: filler word\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2JBLEJ4THqj9","executionInfo":{"status":"ok","timestamp":1611809126469,"user_tz":480,"elapsed":1403,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"de605c29-e3f2-4a36-b15f-3ed7f0f6b5c3"},"source":["data = pd.read_csv(\"ner_dataset.csv\", encoding=\"ISO-8859-1\")\r\n","train_sents = open(\"data/small/train/sentences.txt\", \"r\").readline()\r\n","train_labels = open(\"data/small/train/labels.txt\", \"r\").readline()\r\n","print('SENTENCE:', train_sents)\r\n","print('SENTENCE LABEL:', train_labels)\r\n","print('ORIGINAL DATA:\\n', data.head(5))\r\n","del(data, train_sents, train_labels)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["SENTENCE: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n","\n","SENTENCE LABEL: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n","\n","ORIGINAL DATA:\n","     Sentence #           Word  POS Tag\n","0  Sentence: 1      Thousands  NNS   O\n","1          NaN             of   IN   O\n","2          NaN  demonstrators  NNS   O\n","3          NaN           have  VBP   O\n","4          NaN        marched  VBN   O\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JI77truCUJHe"},"source":["<a name=\"1.1\"></a>\r\n","## 1.1  Importing the Data\r\n","\r\n","In this part, we will import the preprocessed data and explore it."]},{"cell_type":"code","metadata":{"id":"UiY-zmH8R1HG"},"source":[""],"execution_count":null,"outputs":[]}]}