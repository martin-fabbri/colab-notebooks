{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c3_w2_01_hidden_state_activation.ipynb","provenance":[],"authorship_tag":"ABX9TyMwUlexRjdKzc1Sx+SaWaf4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sO52ePtVMavj"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\r\n","  <td>\r\n","    <a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/nlp/c3_w1_assigment_deep_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>    \r\n","  </td>\r\n","  <td>\r\n","    <a href=\"https://github.com/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/nlp/c3_w1_assigment_deep_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/fastai/fastpages/master/assets/badges/github.svg\" alt=\"View On Github\"/></a>  </td>\r\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"nVmPm872MdiT"},"source":["# Hidden State Activation : Ungraded Lecture Notebook\r\n","\r\n","In this notebook you'll take another look at the hidden state activation function. It can be written in two different ways. \r\n","\r\n","I'll show you, step by step, how to implement each of them and then how to verify whether the results produced by each of them are same or not.\r\n","\r\n","## Background\r\n","\r\n","![vanilla rnn](vanilla_rnn.PNG)\r\n","\r\n","\r\n","This is the hidden state activation function for a vanilla RNN.\r\n","\r\n","$h^{<t>}=g(W_{h}[h^{<t-1>},x^{<t>}] + b_h)$                                                    \r\n","\r\n","Which is another way of writing this:         \r\n","\r\n","$h^{<t>}=g(W_{hh}h^{<t-1>} \\oplus W_{hx}x^{<t>} + b_h)$                                        \r\n","\r\n","Where \r\n","\r\n","- $W_{h}$ in the first formula is denotes the *horizontal* concatenation of $W_{hh}$ and $W_{hx}$ from the second formula.\r\n","\r\n","- $W_{h}$ in the first formula is then multiplied by $[h^{<t-1>},x^{<t>}]$, another concatenation of parameters from the second formula but this time in a different direction, i.e *vertical*!\r\n","\r\n","Let us see what this means computationally.\r\n","\r\n","## Imports"]},{"cell_type":"code","metadata":{"id":"dJZShz4wMGtf"},"source":[""],"execution_count":null,"outputs":[]}]}