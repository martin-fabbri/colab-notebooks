{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c3_w1_assigment_deep_neural_networks.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/nlp/c3_w1_assigment_deep_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"jPVkpoAe8BBC"},"source":["# Assignment 1:  Sentiment with Deep Neural Networks\r\n","\r\n","Welcome to the first assignment of course 3. In this assignment, you will explore sentiment analysis using deep neural networks. \r\n","## Outline\r\n","- [Part 1:  Import libraries and try out Trax](#1)\r\n","- [Part 2:  Importing the data](#2)\r\n","    - [2.1  Loading in the data](#2.1)\r\n","    - [2.2  Building the vocabulary](#2.2)\r\n","    - [2.3  Converting a tweet to a tensor](#2.3)\r\n","        - [Exercise 01](#ex01)\r\n","    - [2.4  Creating a batch generator](#2.4)\r\n","        - [Exercise 02](#ex02)\r\n","- [Part 3:  Defining classes](#3)\r\n","    - [3.1  ReLU class](#3.1)\r\n","        - [Exercise 03](#ex03)\r\n","    - [3.2  Dense class ](#3.2)\r\n","        - [Exercise 04](#ex04)\r\n","    - [3.3  Model](#3.3)\r\n","        - [Exercise 05](#ex05)\r\n","- [Part 4:  Training](#4)\r\n","    - [4.1  Training the model](#4.1)\r\n","        - [Exercise 06](#ex06)\r\n","    - [4.2  Practice Making a prediction](#4.2)\r\n","- [Part 5:  Evaluation  ](#5)\r\n","    - [5.1  Computing the accuracy on a batch](#5.1)\r\n","        - [Exercise 07](#ex07)\r\n","    - [5.2  Testing your model on Validation Data](#5.2)\r\n","        - [Exercise 08](#ex08)\r\n","- [Part 6:  Testing with your own input](#6)"]},{"cell_type":"markdown","metadata":{"id":"r52dOO5s8FZp"},"source":["In course 1, you implemented Logistic regression and Naive Bayes for sentiment analysis. However if you were to give your old models an example like:\r\n","\r\n","<center> <font size=\"3\" color=\"darkgreen\"><b>This movie was almost good.</b> </font> </center>\r\n","\r\n","Your model would have predicted a positive sentiment for that review. However, that sentence has a negative sentiment and indicates that the movie was not good. To solve those kinds of misclassifications, you will write a program that uses deep neural networks to identify sentiment in text. By completing this assignment, you will: \r\n","\r\n","- Understand how you can build/design a model using layers\r\n","- Train a model using a training loop\r\n","- Use a binary cross-entropy loss function\r\n","- Compute the accuracy of your model\r\n","- Predict using your own input\r\n","\r\n","As you can tell, this model follows a similar structure to the one you previously implemented in the second course of this specialization. \r\n","- Indeed most of the deep nets you will be implementing will have a similar structure. The only thing that changes is the model architecture, the inputs, and the outputs. Before starting the assignment, we will introduce you to the Google library `trax` that we use for building and training models.\r\n","\r\n","\r\n","Now we will show you how to compute the gradient of a certain function `f` by just using `  .grad(f)`. \r\n","\r\n","- Trax source code can be found on Github: [Trax](https://github.com/google/trax)\r\n","- The Trax code also uses the JAX library: [JAX](https://jax.readthedocs.io/en/latest/index.html)"]},{"cell_type":"markdown","metadata":{"id":"gT988hwEHlo-"},"source":["<a name=\"1\"></a>\r\n","# Part 1:  Import libraries and try out Trax\r\n","\r\n","- Let's import libraries and look at an example of using the Trax library."]},{"cell_type":"code","metadata":{"id":"EIcgkqcEHroB","executionInfo":{"status":"ok","timestamp":1611557278855,"user_tz":480,"elapsed":3820,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["%%capture\r\n","!pip install trax==1.3.1"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vAJr628k7l-V","executionInfo":{"status":"ok","timestamp":1611557279539,"user_tz":480,"elapsed":4483,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"4e6a0078-89e8-41a7-92f9-ab3dab4012a6"},"source":["import random as rnd\r\n","\r\n","# import relevant libraries\r\n","import trax\r\n","\r\n","# set random seeds to make this notebook easier to replicate\r\n","trax.supervised.trainer_lib.init_random_number_generators(31)\r\n","\r\n","import os\r\n","import re\r\n","\r\n","# import Layer from the utils.py file\r\n","import string\r\n","\r\n","import nltk\r\n","\r\n","# import trax.fastmath.numpy\r\n","import trax.fastmath.numpy as np\r\n","\r\n","# import trax.layers\r\n","from trax import layers as tl\r\n","\r\n","nltk.download('twitter_samples')\r\n","nltk.download('stopwords')\r\n","from nltk.corpus import stopwords, twitter_samples\r\n","from nltk.tokenize import TweetTokenizer\r\n","\r\n","!pip list | grep 'trax\\|nltk\\|jax'"],"execution_count":20,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Package twitter_samples is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","jax                           0.2.7                \n","jaxlib                        0.1.57+cuda101       \n","nltk                          3.2.5                \n","trax                          1.3.1                \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yg1j5wBTIDYV","executionInfo":{"status":"ok","timestamp":1611557279540,"user_tz":480,"elapsed":4481,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# Stop words are messy and not that compelling; \r\n","# \"very\" and \"not\" are considered stop words, but they are obviously expressing sentiment\r\n","\r\n","# The porter stemmer lemmatizes \"was\" to \"wa\".  Seriously???\r\n","\r\n","# I'm not sure we want to get into stop words\r\n","stopwords_english = stopwords.words('english')\r\n","\r\n","# Also have my doubts about stemming...\r\n","from nltk.stem import PorterStemmer\r\n","stemmer = PorterStemmer()\r\n","\r\n","def process_tweet(tweet):\r\n","    '''\r\n","    Input: \r\n","        tweet: a string containing a tweet\r\n","    Output:\r\n","        tweets_clean: a list of words containing the processed tweet\r\n","    \r\n","    '''\r\n","    # remove stock market tickers like $GE\r\n","    tweet = re.sub(r'\\$\\w*', '', tweet)\r\n","    # remove old style retweet text \"RT\"\r\n","    tweet = re.sub(r'^RT[\\s]+', '', tweet)\r\n","    # remove hyperlinks\r\n","    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\r\n","    # remove hashtags\r\n","    # only removing the hash # sign from the word\r\n","    tweet = re.sub(r'#', '', tweet)\r\n","    # tokenize tweets\r\n","    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\r\n","    tweet_tokens = tokenizer.tokenize(tweet)\r\n","    ### START CODE HERE ###\r\n","    tweets_clean = []\r\n","    for word in tweet_tokens:\r\n","        if (word not in stopwords_english and # remove stopwords\r\n","            word not in string.punctuation): # remove punctuation\r\n","            #tweets_clean.append(word)\r\n","            stem_word = stemmer.stem(word) # stemming word\r\n","            tweets_clean.append(stem_word)\r\n","    ### END CODE HERE ###\r\n","    return tweets_clean\r\n","\r\n","\r\n","# let's not reuse variables\r\n","#all_positive_tweets = twitter_samples.strings('positive_tweets.json')\r\n","#all_negative_tweets = twitter_samples.strings('negative_tweets.json')\r\n","\r\n","def load_tweets():\r\n","    all_positive_tweets = twitter_samples.strings('positive_tweets.json')\r\n","    all_negative_tweets = twitter_samples.strings('negative_tweets.json')  \r\n","    return all_positive_tweets, all_negative_tweets\r\n","    \r\n","# Layers have weights and a foward function.\r\n","# They create weights when layer.initialize is called and use them.\r\n","# remove this or make it optional \r\n","\r\n","class Layer(object):\r\n","    \"\"\"Base class for layers.\"\"\"\r\n","    def __init__(self):\r\n","        self.weights = None\r\n","\r\n","    def forward(self, x):\r\n","        raise NotImplementedError\r\n","  \r\n","    def init_weights_and_state(self, input_signature, random_key):\r\n","        pass\r\n","\r\n","    def init(self, input_signature, random_key):\r\n","        self.init_weights_and_state(input_signature, random_key)\r\n","        return self.weights\r\n","    \r\n","    def __call__(self, x):\r\n","        return self.forward(x)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":50},"id":"kehO6khUIKLF","executionInfo":{"status":"ok","timestamp":1611557279543,"user_tz":480,"elapsed":4475,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"fcd954e8-1d3d-47c1-e55c-22f2ee83e18e"},"source":["# Create an array using trax.fastmath.numpy\r\n","a = np.array(5.0)\r\n","\r\n","# View the returned array\r\n","display(a)\r\n","\r\n","print(type(a))"],"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":["DeviceArray(5., dtype=float32)"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["<class 'jax.interpreters.xla._DeviceArray'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_cE5IP4aNZjv"},"source":["Notice that trax.fastmath.numpy returns a DeviceArray from the jax library."]},{"cell_type":"code","metadata":{"id":"6RH0zy0UIKHp","executionInfo":{"status":"ok","timestamp":1611557279544,"user_tz":480,"elapsed":4472,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# Define a function that will use the trax.fastmath.numpy array\r\n","def f(x):\r\n","    \r\n","    # f = x^2\r\n","    return (x**2)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mt7r9-GlIKDl","executionInfo":{"status":"ok","timestamp":1611557279546,"user_tz":480,"elapsed":4466,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"f6010262-a5c7-44bc-d6f5-962cc25ff78d"},"source":["# Call the function\r\n","print(f\"f(a) for a={a} is {f(a)}\")"],"execution_count":24,"outputs":[{"output_type":"stream","text":["f(a) for a=5.0 is 25.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EEURb2zqNxdk"},"source":["The gradient (derivative) of function `f` with respect to its input `x` is the derivative of $x^2$.\r\n","- The derivative of $x^2$ is $2x$.  \r\n","- When x is 5, then $2x=10$.\r\n","\r\n","You can calculate the gradient of a function by using `trax.fastmath.grad(fun=)` and passing in the name of the function.\r\n","- In this case the function you want to take the gradient of is `f`.\r\n","- The object returned (saved in `grad_f` in this example) is a function that can calculate the gradient of f for a given trax.fastmath.numpy array."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AL_Bk01zNyOT","executionInfo":{"status":"ok","timestamp":1611557279547,"user_tz":480,"elapsed":4460,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"93da237b-81b3-4428-823e-f52567c4677f"},"source":["grad_f = trax.fastmath.grad(fun=f)\r\n","type(grad_f)"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["function"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"Mov9AurcNzGc","executionInfo":{"status":"ok","timestamp":1611557279549,"user_tz":480,"elapsed":4455,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"9e1bfe6c-34be-4b17-cfa5-4ae3657f4fdc"},"source":["grad_calculation = grad_f(a)\r\n","display(grad_calculation)"],"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":["DeviceArray(10., dtype=float32)"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"eF3C87cSPGMa"},"source":["The function returned by trax.fastmath.grad takes in x=5 and calculates the gradient of f, which is 2*x, which is 10. The value is also stored as a DeviceArray from the jax library."]},{"cell_type":"markdown","metadata":{"id":"AK-UyQ11TbsE"},"source":["<a name=\"2\"></a>\r\n","# Part 2:  Importing the data\r\n","\r\n","<a name=\"2.1\"></a>\r\n","## 2.1  Loading in the data\r\n","\r\n","Import the data set.  \r\n","- You may recognize this from earlier assignments in the specialization.\r\n","- Details of process_tweet function are available in utils.py file"]},{"cell_type":"code","metadata":{"id":"rAzrSwK6NzDS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611557375260,"user_tz":480,"elapsed":1190,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"0db8fb4e-ba6c-4c81-d3a7-7953f9f2f3f5"},"source":["## DO NOT EDIT THIS CELL\r\n","\r\n","# Import functions from the utils.py file\r\n","\r\n","import numpy as np\r\n","\r\n","# Load positive and negative tweets\r\n","all_positive_tweets, all_negative_tweets = load_tweets()\r\n","\r\n","# View the total number of positive and negative tweets.\r\n","print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\r\n","print(f\"The number of negative tweets: {len(all_negative_tweets)}\")\r\n","\r\n","# Split positive set into validation and training\r\n","val_pos   = all_positive_tweets[4000:] # generating validation set for positive tweets\r\n","train_pos  = all_positive_tweets[:4000]# generating training set for positive tweets\r\n","\r\n","# Split negative set into validation and training\r\n","val_neg   = all_negative_tweets[4000:] # generating validation set for negative tweets\r\n","train_neg  = all_negative_tweets[:4000] # generating training set for nagative tweets\r\n","\r\n","# Combine training data into one set\r\n","train_x = train_pos + train_neg \r\n","\r\n","# Combine validation data into one set\r\n","val_x  = val_pos + val_neg\r\n","\r\n","# Set the labels for the training set (1 for positive, 0 for negative)\r\n","train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\r\n","\r\n","# Set the labels for the validation set (1 for positive, 0 for negative)\r\n","val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\r\n","\r\n","print(f\"length of train_x {len(train_x)}\")\r\n","print(f\"length of val_x {len(val_x)}\")"],"execution_count":27,"outputs":[{"output_type":"stream","text":["The number of positive tweets: 5000\n","The number of negative tweets: 5000\n","length of train_x 8000\n","length of val_x 2000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QpuX1_U2Tr5n"},"source":["Now import a function that processes tweets (we've provided this in the utils.py file).\r\n","- `process_tweets' removes unwanted characters e.g. hashtag, hyperlinks, stock tickers from tweet.\r\n","- It also returns a list of words (it tokenizes the original string)."]},{"cell_type":"code","metadata":{"id":"PRdVxoIKNy_X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611557464953,"user_tz":480,"elapsed":595,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"71a462eb-245c-4b67-c65c-bad4d5130866"},"source":["# Import a function that processes the tweets\r\n","# from utils import process_tweet\r\n","\r\n","# Try out function that processes tweets\r\n","print(\"original tweet at training position 0\")\r\n","print(train_pos[0])\r\n","\r\n","print(\"Tweet at training position 0 after processing:\")\r\n","process_tweet(train_pos[0])"],"execution_count":28,"outputs":[{"output_type":"stream","text":["original tweet at training position 0\n","#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n","Tweet at training position 0 after processing:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"E3RDjV1INy5i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611557559640,"user_tz":480,"elapsed":573,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"9465c182-e960-4d48-f6b5-1bf62148743d"},"source":["# Try out function that processes tweets\r\n","print(\"original tweet at training position 1\")\r\n","print(train_pos[1])\r\n","\r\n","print(\"Tweet at training position 0 after processing:\")\r\n","process_tweet(train_pos[1])"],"execution_count":29,"outputs":[{"output_type":"stream","text":["original tweet at training position 1\n","@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n","Tweet at training position 0 after processing:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['hey',\n"," 'jame',\n"," 'odd',\n"," ':/',\n"," 'pleas',\n"," 'call',\n"," 'contact',\n"," 'centr',\n"," '02392441234',\n"," 'abl',\n"," 'assist',\n"," ':)',\n"," 'mani',\n"," 'thank']"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"yID_07iYUcu6"},"source":["Notice that the function `process_tweet` keeps key words, removes the hash # symbol, and ignores usernames (words that begin with '@').  It also returns a list of the words."]},{"cell_type":"markdown","metadata":{"id":"w7Nznu6yYW-I"},"source":["<a name=\"2.2\"></a>\r\n","## 2.2  Building the vocabulary\r\n","\r\n","Now build the vocabulary.\r\n","- Map each word in each tweet to an integer (an \"index\"). \r\n","- The following code does this for you, but please read it and understand what it's doing.\r\n","- Note that you will build the vocabulary based on the training data. \r\n","- To do so, you will assign an index to everyword by iterating over your training set.\r\n","\r\n","The vocabulary will also include some special tokens\r\n","- `__PAD__`: padding\r\n","- `</e>`: end of line\r\n","- `__UNK__`: a token representing any word that is not in the vocabulary."]},{"cell_type":"code","metadata":{"id":"zYE7ebdDNyv-","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1611558677307,"user_tz":480,"elapsed":2311,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"13444364-fa97-4810-fb44-72f648984810"},"source":["# Build the vocabulary\r\n","# Unit Test Note - There is no test set here only train/val\r\n","\r\n","# Include special tokens \r\n","# started with pad, end of line and unk tokens\r\n","Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \r\n","\r\n","# Note that we build vocab using training data\r\n","for tweet in train_x: \r\n","    processed_tweet = process_tweet(tweet)\r\n","    for word in processed_tweet:\r\n","        if word not in Vocab: \r\n","            Vocab[word] = len(Vocab)\r\n","    \r\n","print(\"Total words in vocab are\",len(Vocab))\r\n","display(Vocab)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Total words in vocab are 9092\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["{'__PAD__': 0,\n"," '__</e>__': 1,\n"," '__UNK__': 2,\n"," 'followfriday': 3,\n"," 'top': 4,\n"," 'engag': 5,\n"," 'member': 6,\n"," 'commun': 7,\n"," 'week': 8,\n"," ':)': 9,\n"," 'hey': 10,\n"," 'jame': 11,\n"," 'odd': 12,\n"," ':/': 13,\n"," 'pleas': 14,\n"," 'call': 15,\n"," 'contact': 16,\n"," 'centr': 17,\n"," '02392441234': 18,\n"," 'abl': 19,\n"," 'assist': 20,\n"," 'mani': 21,\n"," 'thank': 22,\n"," 'listen': 23,\n"," 'last': 24,\n"," 'night': 25,\n"," 'bleed': 26,\n"," 'amaz': 27,\n"," 'track': 28,\n"," 'scotland': 29,\n"," 'congrat': 30,\n"," 'yeaaah': 31,\n"," 'yipppi': 32,\n"," 'accnt': 33,\n"," 'verifi': 34,\n"," 'rqst': 35,\n"," 'succeed': 36,\n"," 'got': 37,\n"," 'blue': 38,\n"," 'tick': 39,\n"," 'mark': 40,\n"," 'fb': 41,\n"," 'profil': 42,\n"," '15': 43,\n"," 'day': 44,\n"," 'one': 45,\n"," 'irresist': 46,\n"," 'flipkartfashionfriday': 47,\n"," 'like': 48,\n"," 'keep': 49,\n"," 'love': 50,\n"," 'custom': 51,\n"," 'wait': 52,\n"," 'long': 53,\n"," 'hope': 54,\n"," 'enjoy': 55,\n"," 'happi': 56,\n"," 'friday': 57,\n"," 'lwwf': 58,\n"," 'second': 59,\n"," 'thought': 60,\n"," '’': 61,\n"," 'enough': 62,\n"," 'time': 63,\n"," 'dd': 64,\n"," 'new': 65,\n"," 'short': 66,\n"," 'enter': 67,\n"," 'system': 68,\n"," 'sheep': 69,\n"," 'must': 70,\n"," 'buy': 71,\n"," 'jgh': 72,\n"," 'go': 73,\n"," 'bayan': 74,\n"," ':D': 75,\n"," 'bye': 76,\n"," 'act': 77,\n"," 'mischiev': 78,\n"," 'etl': 79,\n"," 'layer': 80,\n"," 'in-hous': 81,\n"," 'wareh': 82,\n"," 'app': 83,\n"," 'katamari': 84,\n"," 'well': 85,\n"," '…': 86,\n"," 'name': 87,\n"," 'impli': 88,\n"," ':p': 89,\n"," 'influenc': 90,\n"," 'big': 91,\n"," '...': 92,\n"," 'juici': 93,\n"," 'selfi': 94,\n"," 'follow': 95,\n"," 'perfect': 96,\n"," 'alreadi': 97,\n"," 'know': 98,\n"," \"what'\": 99,\n"," 'great': 100,\n"," 'opportun': 101,\n"," 'junior': 102,\n"," 'triathlet': 103,\n"," 'age': 104,\n"," '12': 105,\n"," '13': 106,\n"," 'gatorad': 107,\n"," 'seri': 108,\n"," 'get': 109,\n"," 'entri': 110,\n"," 'lay': 111,\n"," 'greet': 112,\n"," 'card': 113,\n"," 'rang': 114,\n"," 'print': 115,\n"," 'today': 116,\n"," 'job': 117,\n"," ':-)': 118,\n"," \"friend'\": 119,\n"," 'lunch': 120,\n"," 'yummm': 121,\n"," 'nostalgia': 122,\n"," 'tb': 123,\n"," 'ku': 124,\n"," 'id': 125,\n"," 'conflict': 126,\n"," 'help': 127,\n"," \"here'\": 128,\n"," 'screenshot': 129,\n"," 'work': 130,\n"," 'hi': 131,\n"," 'liv': 132,\n"," 'hello': 133,\n"," 'need': 134,\n"," 'someth': 135,\n"," 'u': 136,\n"," 'fm': 137,\n"," 'twitter': 138,\n"," '—': 139,\n"," 'sure': 140,\n"," 'thing': 141,\n"," 'dm': 142,\n"," 'x': 143,\n"," \"i'v\": 144,\n"," 'heard': 145,\n"," 'four': 146,\n"," 'season': 147,\n"," 'pretti': 148,\n"," 'dope': 149,\n"," 'penthous': 150,\n"," 'obv': 151,\n"," 'gobigorgohom': 152,\n"," 'fun': 153,\n"," \"y'all\": 154,\n"," 'yeah': 155,\n"," 'suppos': 156,\n"," 'lol': 157,\n"," 'chat': 158,\n"," 'bit': 159,\n"," 'youth': 160,\n"," '💅': 161,\n"," '🏽': 162,\n"," '💋': 163,\n"," 'seen': 164,\n"," 'year': 165,\n"," 'rest': 166,\n"," 'goe': 167,\n"," 'quickli': 168,\n"," 'bed': 169,\n"," 'music': 170,\n"," 'fix': 171,\n"," 'dream': 172,\n"," 'spiritu': 173,\n"," 'ritual': 174,\n"," 'festiv': 175,\n"," 'népal': 176,\n"," 'begin': 177,\n"," 'line-up': 178,\n"," 'left': 179,\n"," 'see': 180,\n"," 'sarah': 181,\n"," 'send': 182,\n"," 'us': 183,\n"," 'email': 184,\n"," 'bitsy@bitdefender.com': 185,\n"," \"we'll\": 186,\n"," 'asap': 187,\n"," 'kik': 188,\n"," 'hatessuc': 189,\n"," '32429': 190,\n"," 'kikm': 191,\n"," 'lgbt': 192,\n"," 'tinder': 193,\n"," 'nsfw': 194,\n"," 'akua': 195,\n"," 'cumshot': 196,\n"," 'come': 197,\n"," 'hous': 198,\n"," 'nsn_supplement': 199,\n"," 'effect': 200,\n"," 'press': 201,\n"," 'releas': 202,\n"," 'distribut': 203,\n"," 'result': 204,\n"," 'link': 205,\n"," 'remov': 206,\n"," 'pressreleas': 207,\n"," 'newsdistribut': 208,\n"," 'bam': 209,\n"," 'bestfriend': 210,\n"," 'lot': 211,\n"," 'warsaw': 212,\n"," '<3': 213,\n"," 'x46': 214,\n"," 'everyon': 215,\n"," 'watch': 216,\n"," 'documentari': 217,\n"," 'earthl': 218,\n"," 'youtub': 219,\n"," 'support': 220,\n"," 'buuut': 221,\n"," 'oh': 222,\n"," 'look': 223,\n"," 'forward': 224,\n"," 'visit': 225,\n"," 'next': 226,\n"," 'letsgetmessi': 227,\n"," 'jo': 228,\n"," 'make': 229,\n"," 'feel': 230,\n"," 'better': 231,\n"," 'never': 232,\n"," 'anyon': 233,\n"," 'kpop': 234,\n"," 'flesh': 235,\n"," 'good': 236,\n"," 'girl': 237,\n"," 'best': 238,\n"," 'wish': 239,\n"," 'reason': 240,\n"," 'epic': 241,\n"," 'soundtrack': 242,\n"," 'shout': 243,\n"," 'ad': 244,\n"," 'video': 245,\n"," 'playlist': 246,\n"," 'would': 247,\n"," 'dear': 248,\n"," 'jordan': 249,\n"," 'okay': 250,\n"," 'fake': 251,\n"," 'gameplay': 252,\n"," ';)': 253,\n"," 'haha': 254,\n"," 'im': 255,\n"," 'kid': 256,\n"," 'stuff': 257,\n"," 'exactli': 258,\n"," 'product': 259,\n"," 'line': 260,\n"," 'etsi': 261,\n"," 'shop': 262,\n"," 'check': 263,\n"," 'vacat': 264,\n"," 'recharg': 265,\n"," 'normal': 266,\n"," 'charger': 267,\n"," 'asleep': 268,\n"," 'talk': 269,\n"," 'sooo': 270,\n"," 'someon': 271,\n"," 'text': 272,\n"," 'ye': 273,\n"," 'bet': 274,\n"," \"he'll\": 275,\n"," 'fit': 276,\n"," 'hear': 277,\n"," 'speech': 278,\n"," 'piti': 279,\n"," 'green': 280,\n"," 'garden': 281,\n"," 'midnight': 282,\n"," 'sun': 283,\n"," 'beauti': 284,\n"," 'canal': 285,\n"," 'dasvidaniya': 286,\n"," 'till': 287,\n"," 'scout': 288,\n"," 'sg': 289,\n"," 'futur': 290,\n"," 'wlan': 291,\n"," 'pro': 292,\n"," 'confer': 293,\n"," 'asia': 294,\n"," 'chang': 295,\n"," 'lollipop': 296,\n"," '🍭': 297,\n"," 'nez': 298,\n"," 'agnezmo': 299,\n"," 'oley': 300,\n"," 'mama': 301,\n"," 'stand': 302,\n"," 'stronger': 303,\n"," 'god': 304,\n"," 'misti': 305,\n"," 'babi': 306,\n"," 'cute': 307,\n"," 'woohoo': 308,\n"," \"can't\": 309,\n"," 'sign': 310,\n"," 'yet': 311,\n"," 'still': 312,\n"," 'think': 313,\n"," 'mka': 314,\n"," 'liam': 315,\n"," 'access': 316,\n"," 'welcom': 317,\n"," 'stat': 318,\n"," 'arriv': 319,\n"," '1': 320,\n"," 'unfollow': 321,\n"," 'via': 322,\n"," 'surpris': 323,\n"," 'figur': 324,\n"," 'happybirthdayemilybett': 325,\n"," 'sweet': 326,\n"," 'talent': 327,\n"," '2': 328,\n"," 'plan': 329,\n"," 'drain': 330,\n"," 'gotta': 331,\n"," 'timezon': 332,\n"," 'parent': 333,\n"," 'proud': 334,\n"," 'least': 335,\n"," 'mayb': 336,\n"," 'sometim': 337,\n"," 'grade': 338,\n"," 'al': 339,\n"," 'grand': 340,\n"," 'manila_bro': 341,\n"," 'chosen': 342,\n"," 'let': 343,\n"," 'around': 344,\n"," '..': 345,\n"," 'side': 346,\n"," 'world': 347,\n"," 'eh': 348,\n"," 'take': 349,\n"," 'care': 350,\n"," 'final': 351,\n"," 'fuck': 352,\n"," 'weekend': 353,\n"," 'real': 354,\n"," 'x45': 355,\n"," 'join': 356,\n"," 'hushedcallwithfraydo': 357,\n"," 'gift': 358,\n"," 'yeahhh': 359,\n"," 'hushedpinwithsammi': 360,\n"," 'event': 361,\n"," 'might': 362,\n"," 'luv': 363,\n"," 'realli': 364,\n"," 'appreci': 365,\n"," 'share': 366,\n"," 'wow': 367,\n"," 'tom': 368,\n"," 'gym': 369,\n"," 'monday': 370,\n"," 'invit': 371,\n"," 'scope': 372,\n"," 'friend': 373,\n"," 'nude': 374,\n"," 'sleep': 375,\n"," 'birthday': 376,\n"," 'want': 377,\n"," 't-shirt': 378,\n"," 'cool': 379,\n"," 'haw': 380,\n"," 'phela': 381,\n"," 'mom': 382,\n"," 'obvious': 383,\n"," 'princ': 384,\n"," 'charm': 385,\n"," 'stage': 386,\n"," 'luck': 387,\n"," 'tyler': 388,\n"," 'hipster': 389,\n"," 'glass': 390,\n"," 'marti': 391,\n"," 'glad': 392,\n"," 'done': 393,\n"," 'afternoon': 394,\n"," 'read': 395,\n"," 'kahfi': 396,\n"," 'finish': 397,\n"," 'ohmyg': 398,\n"," 'yaya': 399,\n"," 'dub': 400,\n"," 'stalk': 401,\n"," 'ig': 402,\n"," 'gondooo': 403,\n"," 'moo': 404,\n"," 'tologooo': 405,\n"," 'becom': 406,\n"," 'detail': 407,\n"," 'zzz': 408,\n"," 'xx': 409,\n"," 'physiotherapi': 410,\n"," 'hashtag': 411,\n"," '💪': 412,\n"," 'monica': 413,\n"," 'miss': 414,\n"," 'sound': 415,\n"," 'morn': 416,\n"," \"that'\": 417,\n"," 'x43': 418,\n"," 'definit': 419,\n"," 'tri': 420,\n"," 'tonight': 421,\n"," 'took': 422,\n"," 'advic': 423,\n"," 'treviso': 424,\n"," 'concert': 425,\n"," 'citi': 426,\n"," 'countri': 427,\n"," \"i'll\": 428,\n"," 'start': 429,\n"," 'fine': 430,\n"," 'gorgeou': 431,\n"," 'xo': 432,\n"," 'oven': 433,\n"," 'roast': 434,\n"," 'garlic': 435,\n"," 'oliv': 436,\n"," 'oil': 437,\n"," 'dri': 438,\n"," 'tomato': 439,\n"," 'basil': 440,\n"," 'centuri': 441,\n"," 'tuna': 442,\n"," 'right': 443,\n"," 'back': 444,\n"," 'atchya': 445,\n"," 'even': 446,\n"," 'almost': 447,\n"," 'chanc': 448,\n"," 'cheer': 449,\n"," 'po': 450,\n"," 'ice': 451,\n"," 'cream': 452,\n"," 'agre': 453,\n"," '100': 454,\n"," 'heheheh': 455,\n"," 'that': 456,\n"," 'point': 457,\n"," 'stay': 458,\n"," 'home': 459,\n"," 'soon': 460,\n"," 'promis': 461,\n"," 'web': 462,\n"," 'whatsapp': 463,\n"," 'volta': 464,\n"," 'funcionar': 465,\n"," 'com': 466,\n"," 'iphon': 467,\n"," 'jailbroken': 468,\n"," 'later': 469,\n"," '34': 470,\n"," 'min': 471,\n"," 'leia': 472,\n"," 'appear': 473,\n"," 'hologram': 474,\n"," 'r2d2': 475,\n"," 'w': 476,\n"," 'messag': 477,\n"," 'obi': 478,\n"," 'wan': 479,\n"," 'sit': 480,\n"," 'luke': 481,\n"," 'inter': 482,\n"," '3': 483,\n"," 'ucl': 484,\n"," 'arsen': 485,\n"," 'small': 486,\n"," 'team': 487,\n"," 'pass': 488,\n"," '🚂': 489,\n"," 'dewsburi': 490,\n"," 'railway': 491,\n"," 'station': 492,\n"," 'dew': 493,\n"," 'west': 494,\n"," 'yorkshir': 495,\n"," '430': 496,\n"," 'smh': 497,\n"," '9:25': 498,\n"," 'live': 499,\n"," 'strang': 500,\n"," 'imagin': 501,\n"," 'megan': 502,\n"," 'masaantoday': 503,\n"," 'a4': 504,\n"," 'shweta': 505,\n"," 'tripathi': 506,\n"," '5': 507,\n"," '20': 508,\n"," 'kurta': 509,\n"," 'half': 510,\n"," 'number': 511,\n"," 'wsalelov': 512,\n"," 'ah': 513,\n"," 'larri': 514,\n"," 'anyway': 515,\n"," 'kinda': 516,\n"," 'goood': 517,\n"," 'life': 518,\n"," 'enn': 519,\n"," 'could': 520,\n"," 'warmup': 521,\n"," '15th': 522,\n"," 'bath': 523,\n"," 'dum': 524,\n"," 'andar': 525,\n"," 'ram': 526,\n"," 'sampath': 527,\n"," 'sona': 528,\n"," 'mohapatra': 529,\n"," 'samantha': 530,\n"," 'edward': 531,\n"," 'mein': 532,\n"," 'tulan': 533,\n"," 'razi': 534,\n"," 'wah': 535,\n"," 'josh': 536,\n"," 'alway': 537,\n"," 'smile': 538,\n"," 'pictur': 539,\n"," '16.20': 540,\n"," 'giveitup': 541,\n"," 'given': 542,\n"," 'ga': 543,\n"," 'subsidi': 544,\n"," 'initi': 545,\n"," 'propos': 546,\n"," 'delight': 547,\n"," 'yesterday': 548,\n"," 'x42': 549,\n"," 'lmaoo': 550,\n"," 'song': 551,\n"," 'ever': 552,\n"," 'shall': 553,\n"," 'littl': 554,\n"," 'throwback': 555,\n"," 'outli': 556,\n"," 'island': 557,\n"," 'cheung': 558,\n"," 'chau': 559,\n"," 'mui': 560,\n"," 'wo': 561,\n"," 'total': 562,\n"," 'differ': 563,\n"," 'kfckitchentour': 564,\n"," 'kitchen': 565,\n"," 'clean': 566,\n"," \"i'm\": 567,\n"," 'cusp': 568,\n"," 'test': 569,\n"," 'water': 570,\n"," 'reward': 571,\n"," 'arummzz': 572,\n"," \"let'\": 573,\n"," 'drive': 574,\n"," 'travel': 575,\n"," 'yogyakarta': 576,\n"," 'jeep': 577,\n"," 'indonesia': 578,\n"," 'instamood': 579,\n"," 'wanna': 580,\n"," 'skype': 581,\n"," 'may': 582,\n"," 'nice': 583,\n"," 'friendli': 584,\n"," 'pretend': 585,\n"," 'film': 586,\n"," 'congratul': 587,\n"," 'winner': 588,\n"," 'cheesydelight': 589,\n"," 'contest': 590,\n"," 'address': 591,\n"," 'guy': 592,\n"," 'market': 593,\n"," '24/7': 594,\n"," '14': 595,\n"," 'hour': 596,\n"," 'leav': 597,\n"," 'without': 598,\n"," 'delay': 599,\n"," 'actual': 600,\n"," 'easi': 601,\n"," 'guess': 602,\n"," 'train': 603,\n"," 'wd': 604,\n"," 'shift': 605,\n"," 'engin': 606,\n"," 'etc': 607,\n"," 'sunburn': 608,\n"," 'peel': 609,\n"," 'blog': 610,\n"," 'huge': 611,\n"," 'warm': 612,\n"," '☆': 613,\n"," 'complet': 614,\n"," 'triangl': 615,\n"," 'northern': 616,\n"," 'ireland': 617,\n"," 'sight': 618,\n"," 'smthng': 619,\n"," 'fr': 620,\n"," 'hug': 621,\n"," 'xoxo': 622,\n"," 'uu': 623,\n"," 'jaann': 624,\n"," 'topnewfollow': 625,\n"," 'connect': 626,\n"," 'wonder': 627,\n"," 'made': 628,\n"," 'fluffi': 629,\n"," 'insid': 630,\n"," 'pirouett': 631,\n"," 'moos': 632,\n"," 'trip': 633,\n"," 'philli': 634,\n"," 'decemb': 635,\n"," \"i'd\": 636,\n"," 'dude': 637,\n"," 'x41': 638,\n"," 'question': 639,\n"," 'flaw': 640,\n"," 'pain': 641,\n"," 'negat': 642,\n"," 'strength': 643,\n"," 'went': 644,\n"," 'solo': 645,\n"," 'move': 646,\n"," 'fav': 647,\n"," 'nirvana': 648,\n"," 'smell': 649,\n"," 'teen': 650,\n"," 'spirit': 651,\n"," 'rip': 652,\n"," 'ami': 653,\n"," 'winehous': 654,\n"," 'coupl': 655,\n"," 'tomhiddleston': 656,\n"," 'elizabetholsen': 657,\n"," 'yaytheylookgreat': 658,\n"," 'goodnight': 659,\n"," 'vid': 660,\n"," 'wake': 661,\n"," 'gonna': 662,\n"," 'shoot': 663,\n"," 'itti': 664,\n"," 'bitti': 665,\n"," 'teeni': 666,\n"," 'bikini': 667,\n"," 'much': 668,\n"," '4th': 669,\n"," 'togeth': 670,\n"," 'end': 671,\n"," 'xfile': 672,\n"," 'content': 673,\n"," 'rain': 674,\n"," 'fabul': 675,\n"," 'fantast': 676,\n"," '♡': 677,\n"," 'jb': 678,\n"," 'forev': 679,\n"," 'belieb': 680,\n"," 'nighti': 681,\n"," 'bug': 682,\n"," 'bite': 683,\n"," 'bracelet': 684,\n"," 'idea': 685,\n"," 'foundri': 686,\n"," 'game': 687,\n"," 'sens': 688,\n"," 'pic': 689,\n"," 'ef': 690,\n"," 'phone': 691,\n"," 'woot': 692,\n"," 'derek': 693,\n"," 'use': 694,\n"," 'parkshar': 695,\n"," 'gloucestershir': 696,\n"," 'aaaahhh': 697,\n"," 'man': 698,\n"," 'traffic': 699,\n"," 'stress': 700,\n"," 'reliev': 701,\n"," \"how'r\": 702,\n"," 'arbeloa': 703,\n"," 'turn': 704,\n"," '17': 705,\n"," 'omg': 706,\n"," 'say': 707,\n"," 'europ': 708,\n"," 'rise': 709,\n"," 'find': 710,\n"," 'hard': 711,\n"," 'believ': 712,\n"," 'uncount': 713,\n"," 'coz': 714,\n"," 'unlimit': 715,\n"," 'cours': 716,\n"," 'teamposit': 717,\n"," 'aldub': 718,\n"," '☕': 719,\n"," 'rita': 720,\n"," 'info': 721,\n"," \"we'd\": 722,\n"," 'way': 723,\n"," 'boy': 724,\n"," 'x40': 725,\n"," 'true': 726,\n"," 'sethi': 727,\n"," 'high': 728,\n"," 'exe': 729,\n"," 'skeem': 730,\n"," 'saam': 731,\n"," 'peopl': 732,\n"," 'polit': 733,\n"," 'izzat': 734,\n"," 'wese': 735,\n"," 'trust': 736,\n"," 'khawateen': 737,\n"," 'k': 738,\n"," 'sath': 739,\n"," 'mana': 740,\n"," 'kar': 741,\n"," 'deya': 742,\n"," 'sort': 743,\n"," 'smart': 744,\n"," 'hair': 745,\n"," 'tbh': 746,\n"," 'jacob': 747,\n"," 'g': 748,\n"," 'upgrad': 749,\n"," 'tee': 750,\n"," 'famili': 751,\n"," 'person': 752,\n"," 'two': 753,\n"," 'convers': 754,\n"," 'onlin': 755,\n"," 'mclaren': 756,\n"," 'fridayfeel': 757,\n"," 'tgif': 758,\n"," 'squar': 759,\n"," 'enix': 760,\n"," 'bissmillah': 761,\n"," 'ya': 762,\n"," 'allah': 763,\n"," \"we'r\": 764,\n"," 'socent': 765,\n"," 'startup': 766,\n"," 'drop': 767,\n"," 'your': 768,\n"," 'arnd': 769,\n"," 'town': 770,\n"," 'basic': 771,\n"," 'piss': 772,\n"," 'cup': 773,\n"," 'also': 774,\n"," 'terribl': 775,\n"," 'complic': 776,\n"," 'discuss': 777,\n"," 'snapchat': 778,\n"," 'lynettelow': 779,\n"," 'kikmenow': 780,\n"," 'snapm': 781,\n"," 'hot': 782,\n"," 'amazon': 783,\n"," 'kikmeguy': 784,\n"," 'defin': 785,\n"," 'grow': 786,\n"," 'sport': 787,\n"," 'rt': 788,\n"," 'rakyat': 789,\n"," 'write': 790,\n"," 'sinc': 791,\n"," 'mention': 792,\n"," 'fli': 793,\n"," 'fish': 794,\n"," 'promot': 795,\n"," 'post': 796,\n"," 'cyber': 797,\n"," 'ourdaughtersourprid': 798,\n"," 'mypapamyprid': 799,\n"," 'papa': 800,\n"," 'coach': 801,\n"," 'posit': 802,\n"," 'kha': 803,\n"," 'atleast': 804,\n"," 'x39': 805,\n"," 'mango': 806,\n"," \"lassi'\": 807,\n"," \"monty'\": 808,\n"," 'marvel': 809,\n"," 'though': 810,\n"," 'suspect': 811,\n"," 'meant': 812,\n"," '24': 813,\n"," 'hr': 814,\n"," 'touch': 815,\n"," 'kepler': 816,\n"," '452b': 817,\n"," 'chalna': 818,\n"," 'hai': 819,\n"," 'thankyou': 820,\n"," 'hazel': 821,\n"," 'food': 822,\n"," 'brooklyn': 823,\n"," 'pta': 824,\n"," 'awak': 825,\n"," 'okayi': 826,\n"," 'awww': 827,\n"," 'ha': 828,\n"," 'doc': 829,\n"," 'splendid': 830,\n"," 'spam': 831,\n"," 'folder': 832,\n"," 'amount': 833,\n"," 'nigeria': 834,\n"," 'claim': 835,\n"," 'rted': 836,\n"," 'leg': 837,\n"," 'hurt': 838,\n"," 'bad': 839,\n"," 'mine': 840,\n"," 'saturday': 841,\n"," 'thaaank': 842,\n"," 'puhon': 843,\n"," 'happinesss': 844,\n"," 'tnc': 845,\n"," 'prior': 846,\n"," 'notif': 847,\n"," 'fat': 848,\n"," 'co': 849,\n"," 'probabl': 850,\n"," 'ate': 851,\n"," 'yuna': 852,\n"," 'tamesid': 853,\n"," '´': 854,\n"," 'googl': 855,\n"," 'account': 856,\n"," 'scouser': 857,\n"," 'everyth': 858,\n"," 'zoe': 859,\n"," 'mate': 860,\n"," 'liter': 861,\n"," \"they'r\": 862,\n"," 'samee': 863,\n"," 'edgar': 864,\n"," 'updat': 865,\n"," 'log': 866,\n"," 'bring': 867,\n"," 'abe': 868,\n"," 'meet': 869,\n"," 'x38': 870,\n"," 'sigh': 871,\n"," 'dreamili': 872,\n"," 'pout': 873,\n"," 'eye': 874,\n"," 'quacketyquack': 875,\n"," 'funni': 876,\n"," 'happen': 877,\n"," 'phil': 878,\n"," 'em': 879,\n"," 'del': 880,\n"," 'rodder': 881,\n"," 'els': 882,\n"," 'play': 883,\n"," 'newest': 884,\n"," 'gamejam': 885,\n"," 'irish': 886,\n"," 'literatur': 887,\n"," 'inaccess': 888,\n"," \"kareena'\": 889,\n"," 'fan': 890,\n"," 'brain': 891,\n"," 'dot': 892,\n"," 'braindot': 893,\n"," 'fair': 894,\n"," 'rush': 895,\n"," 'either': 896,\n"," 'brandi': 897,\n"," '18': 898,\n"," 'carniv': 899,\n"," 'men': 900,\n"," 'put': 901,\n"," 'mask': 902,\n"," 'xavier': 903,\n"," 'forneret': 904,\n"," 'jennif': 905,\n"," 'site': 906,\n"," 'free': 907,\n"," '50.000': 908,\n"," '8': 909,\n"," 'ball': 910,\n"," 'pool': 911,\n"," 'coin': 912,\n"," 'edit': 913,\n"," 'trish': 914,\n"," '♥': 915,\n"," 'grate': 916,\n"," 'three': 917,\n"," 'comment': 918,\n"," 'wakeup': 919,\n"," 'besid': 920,\n"," 'dirti': 921,\n"," 'sex': 922,\n"," 'lmaooo': 923,\n"," '😤': 924,\n"," 'loui': 925,\n"," \"he'\": 926,\n"," 'throw': 927,\n"," 'caus': 928,\n"," 'inspir': 929,\n"," 'ff': 930,\n"," 'twoof': 931,\n"," 'gr8': 932,\n"," 'wkend': 933,\n"," 'kind': 934,\n"," 'exhaust': 935,\n"," 'word': 936,\n"," 'cheltenham': 937,\n"," 'area': 938,\n"," 'kale': 939,\n"," 'crisp': 940,\n"," 'ruin': 941,\n"," 'x37': 942,\n"," 'open': 943,\n"," 'worldwid': 944,\n"," 'outta': 945,\n"," 'sfvbeta': 946,\n"," 'vantast': 947,\n"," 'xcylin': 948,\n"," 'bundl': 949,\n"," 'show': 950,\n"," 'internet': 951,\n"," 'price': 952,\n"," 'realisticli': 953,\n"," 'pay': 954,\n"," 'net': 955,\n"," 'educ': 956,\n"," 'power': 957,\n"," 'weapon': 958,\n"," 'nelson': 959,\n"," 'mandela': 960,\n"," 'recent': 961,\n"," 'j': 962,\n"," 'chenab': 963,\n"," 'flow': 964,\n"," 'pakistan': 965,\n"," 'incredibleindia': 966,\n"," 'teenchoic': 967,\n"," 'choiceinternationalartist': 968,\n"," 'superjunior': 969,\n"," 'caught': 970,\n"," 'first': 971,\n"," 'salmon': 972,\n"," 'super-blend': 973,\n"," 'project': 974,\n"," 'youth@bipolaruk.org.uk': 975,\n"," 'awesom': 976,\n"," 'stream': 977,\n"," 'alma': 978,\n"," 'mater': 979,\n"," 'highschoolday': 980,\n"," 'clientvisit': 981,\n"," 'faith': 982,\n"," 'christian': 983,\n"," 'school': 984,\n"," 'lizaminnelli': 985,\n"," 'upcom': 986,\n"," 'uk': 987,\n"," '😄': 988,\n"," 'singl': 989,\n"," 'hill': 990,\n"," 'everi': 991,\n"," 'beat': 992,\n"," 'wrong': 993,\n"," 'readi': 994,\n"," 'natur': 995,\n"," 'pefumeri': 996,\n"," 'workshop': 997,\n"," 'neal': 998,\n"," 'yard': 999,\n"," ...}"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"UkKEsBRMYj6G"},"source":["The dictionary `Vocab` will look like this:\r\n","```CPP\r\n","{'__PAD__': 0,\r\n"," '__</e>__': 1,\r\n"," '__UNK__': 2,\r\n"," 'followfriday': 3,\r\n"," 'top': 4,\r\n"," 'engag': 5,\r\n"," ...\r\n","```\r\n","\r\n","- Each unique word has a unique integer associated with it.\r\n","- The total number of words in Vocab: 9088"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k-5SOYONYsne"},"source":["<a name=\"2.3\"></a>\r\n","## 2.3  Converting a tweet to a tensor\r\n","\r\n","Write a function that will convert each tweet to a tensor (a list of unique integer IDs representing the processed tweet).\r\n","- Note, the returned data type will be a **regular Python `list()`**\r\n","    - You won't use TensorFlow in this function\r\n","    - You also won't use a numpy array\r\n","    - You also won't use trax.fastmath.numpy array\r\n","- For words in the tweet that are not in the vocabulary, set them to the unique ID for the token `__UNK__`.\r\n","\r\n","##### Example\r\n","Input a tweet:\r\n","```CPP\r\n","'@happypuppy, is Maria happy?'\r\n","```\r\n","\r\n","The tweet_to_tensor will first conver the tweet into a list of tokens (including only relevant words)\r\n","```CPP\r\n","['maria', 'happi']\r\n","```\r\n","\r\n","Then it will convert each word into its unique integer\r\n","\r\n","```CPP\r\n","[2, 56]\r\n","```\r\n","- Notice that the word \"maria\" is not in the vocabulary, so it is assigned the unique integer associated with the `__UNK__` token, because it is considered \"unknown.\""]},{"cell_type":"markdown","metadata":{"id":"iBw_mVmIY6GL"},"source":["<a name=\"ex01\"></a>\r\n","### Exercise 01\r\n","**Instructions:** Write a program `tweet_to_tensor` that takes in a tweet and converts it to an array of numbers. You can use the `Vocab` dictionary you just found to help create the tensor. \r\n","\r\n","- Use the vocab_dict parameter and not a global variable.\r\n","- Do not hard code the integer value for the `__UNK__` token."]},{"cell_type":"markdown","metadata":{"id":"_BxNig8sY-2S"},"source":["<details>    \r\n","<summary>\r\n","    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\r\n","</summary>\r\n","<p>\r\n","<ul>\r\n","    <li>Map each word in tweet to corresponding token in 'Vocab'</li>\r\n","    <li>Use Python's Dictionary.get(key,value) so that the function returns a default value if the key is not found in the dictionary.</li>\r\n","</ul>\r\n","</p>"]},{"cell_type":"code","metadata":{"id":"wABXOZbzYrsc","executionInfo":{"status":"ok","timestamp":1611559349290,"user_tz":480,"elapsed":919,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\r\n","# GRADED FUNCTION: tweet_to_tensor\r\n","def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\r\n","    '''\r\n","    Input: \r\n","        tweet - A string containing a tweet\r\n","        vocab_dict - The words dictionary\r\n","        unk_token - The special string for unknown tokens\r\n","        verbose - Print info durign runtime\r\n","    Output:\r\n","        tensor_l - A python list with\r\n","        \r\n","    '''  \r\n","    \r\n","    ### START CODE HERE (Replace instances of 'None' with your code) ###\r\n","    # Process the tweet into a list of words\r\n","    # where only important words are kept (stop words removed)\r\n","    word_l = process_tweet(tweet)\r\n","    \r\n","    if verbose:\r\n","        print(\"List of words from the processed tweet:\")\r\n","        print(word_l)\r\n","        \r\n","    # Initialize the list that will contain the unique integer IDs of each word\r\n","    tensor_l = []\r\n","    \r\n","    # Get the unique integer ID of the __UNK__ token\r\n","    unk_ID = vocab_dict.get(unk_token, 0)\r\n","    \r\n","    if verbose:\r\n","        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\r\n","        \r\n","    # for each word in the list:\r\n","    for word in word_l:\r\n","        \r\n","        # Get the unique integer ID.\r\n","        # If the word doesn't exist in the vocab dictionary,\r\n","        # use the unique ID for __UNK__ instead.\r\n","        word_ID = vocab_dict.get(word, unk_ID)\r\n","    ### END CODE HERE ###\r\n","        \r\n","        # Append the unique integer ID to the tensor list.\r\n","        tensor_l.append(word_ID) \r\n","    \r\n","    return tensor_l"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p-IaWq-sYrpj","executionInfo":{"status":"ok","timestamp":1611559351648,"user_tz":480,"elapsed":583,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"5e0d94c4-f640-430d-a57d-fe6659233bd3"},"source":["print(\"Actual tweet is\\n\", val_pos[0])\r\n","print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(val_pos[0], vocab_dict=Vocab))"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Actual tweet is\n"," Bro:U wan cut hair anot,ur hair long Liao bo\n","Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n","Bro:LOL Sibei xialan\n","\n","Tensor of tweet:\n"," [1065, 136, 479, 2351, 745, 8146, 1123, 745, 53, 2, 2672, 791, 2, 2, 349, 601, 2, 3489, 1017, 597, 4559, 9, 1065, 157, 2, 2]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x1gbwgdUaJln"},"source":["##### Expected output\r\n","\r\n","```CPP\r\n","Actual tweet is\r\n"," Bro:U wan cut hair anot,ur hair long Liao bo\r\n","Me:since ord liao,take it easy lor treat as save $ leave it longer :)\r\n","Bro:LOL Sibei xialan\r\n","\r\n","Tensor of tweet:\r\n","[1065, 136, 479, 2351, 745, 8148, 1123, 745, 53, 2, 2672, 791, 2, 2, 349, 601, 2, 3489, 1017, 597, 4559, 9, 1065, 157, 2, 2]\r\n","```"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xaw6wI_1bOJY","executionInfo":{"status":"ok","timestamp":1611559408562,"user_tz":480,"elapsed":706,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"01b5b00c-ed13-4ee8-b750-34d536df9775"},"source":["# test tweet_to_tensor\r\n","\r\n","def test_tweet_to_tensor():\r\n","    test_cases = [\r\n","        \r\n","        {\r\n","            \"name\":\"simple_test_check\",\r\n","            \"input\": [val_pos[1], Vocab],\r\n","            \"expected\":[444, 2, 304, 567, 56, 9],\r\n","            \"error\":\"The function gives bad output for val_pos[1]. Test failed\"\r\n","        },\r\n","        {\r\n","            \"name\":\"datatype_check\",\r\n","            \"input\":[val_pos[1], Vocab],\r\n","            \"expected\":type([]),\r\n","            \"error\":\"Datatype mismatch. Need only list not np.array\"\r\n","        },\r\n","        {\r\n","            \"name\":\"without_unk_check\",\r\n","            \"input\":[val_pos[1], Vocab],\r\n","            \"expected\":6,\r\n","            \"error\":\"Unk word check not done- Please check if you included mapping for unknown word\"\r\n","        }\r\n","    ]\r\n","    count = 0\r\n","    for test_case in test_cases:\r\n","        \r\n","        try:\r\n","            if test_case['name'] == \"simple_test_check\":\r\n","                assert test_case[\"expected\"] == tweet_to_tensor(*test_case['input'])\r\n","                count += 1\r\n","            if test_case['name'] == \"datatype_check\":\r\n","                assert isinstance(tweet_to_tensor(*test_case['input']), test_case[\"expected\"])\r\n","                count += 1\r\n","            if test_case['name'] == \"without_unk_check\":\r\n","                assert None not in tweet_to_tensor(*test_case['input'])\r\n","                count += 1\r\n","                \r\n","            \r\n","            \r\n","        except:\r\n","            print(test_case['error'])\r\n","    if count == 3:\r\n","        print(\"\\033[92m All tests passed\")\r\n","    else:\r\n","        print(count,\" Tests passed out of 3\")\r\n","test_tweet_to_tensor() "],"execution_count":34,"outputs":[{"output_type":"stream","text":["\u001b[92m All tests passed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zmCBvTeqbmzs"},"source":["<a name=\"2.4\"></a>\r\n","## 2.4  Creating a batch generator\r\n","\r\n","Most of the time in Natural Language Processing, and AI in general we use batches when training our data sets. \r\n","- If instead of training with batches of examples, you were to train a model with one example at a time, it would take a very long time to train the model. \r\n","- You will now build a data generator that takes in the positive/negative tweets and returns a batch of training examples. It returns the model inputs, the targets (positive or negative labels) and the weight for each target (ex: this allows us to can treat some examples as more important to get right than others, but commonly this will all be 1.0). \r\n","\r\n","Once you create the generator, you could include it in a for loop\r\n","\r\n","```CPP\r\n","for batch_inputs, batch_targets, batch_example_weights in data_generator:\r\n","    ...\r\n","```\r\n","\r\n","You can also get a single batch like this:\r\n","\r\n","```CPP\r\n","batch_inputs, batch_targets, batch_example_weights = next(data_generator)\r\n","```\r\n","The generator returns the next batch each time it's called. \r\n","- This generator returns the data in a format (tensors) that you could directly use in your model.\r\n","- It returns a triple: the inputs, targets, and loss weights:\r\n","- Inputs is a tensor that contains the batch of tweets we put into the model.\r\n","- Targets is the corresponding batch of labels that we train to generate.\r\n","- Loss weights here are just 1s with same shape as targets. Next week, you will use it to mask input padding."]},{"cell_type":"markdown","metadata":{"id":"RZzs_TYqbqSz"},"source":["<a name=\"ex02\"></a>\r\n","### Exercise 02\r\n","Implement `data_generator`."]},{"cell_type":"code","metadata":{"id":"jsu-N3CuYrmg","executionInfo":{"status":"ok","timestamp":1611561312027,"user_tz":480,"elapsed":732,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\r\n","# GRADED: Data generator\r\n","def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\r\n","    '''\r\n","    Input: \r\n","        data_pos - Set of posstive examples\r\n","        data_neg - Set of negative examples\r\n","        batch_size - number of samples per batch. Must be even\r\n","        loop - True or False\r\n","        vocab_dict - The words dictionary\r\n","        shuffle - Shuffle the data order\r\n","    Yield:\r\n","        inputs - Subset of positive and negative examples\r\n","        targets - The corresponding labels for the subset\r\n","        example_weights - An array specifying the importance of each example\r\n","        \r\n","    '''     \r\n","### START GIVEN CODE ###\r\n","    # make sure the batch size is an even number\r\n","    # to allow an equal number of positive and negative samples\r\n","    assert batch_size % 2 == 0\r\n","    \r\n","    # Number of positive examples in each batch is half of the batch size\r\n","    # same with number of negative examples in each batch\r\n","    n_to_take = batch_size // 2\r\n","    \r\n","    # Use pos_index to walk through the data_pos array\r\n","    # same with neg_index and data_neg\r\n","    pos_index = 0\r\n","    neg_index = 0\r\n","    \r\n","    len_data_pos = len(data_pos)\r\n","    len_data_neg = len(data_neg)\r\n","    \r\n","    # Get and array with the data indexes\r\n","    pos_index_lines = list(range(len_data_pos))\r\n","    neg_index_lines = list(range(len_data_neg))\r\n","    \r\n","    # shuffle lines if shuffle is set to True\r\n","    if shuffle:\r\n","        rnd.shuffle(pos_index_lines)\r\n","        rnd.shuffle(neg_index_lines)\r\n","        \r\n","    stop = False\r\n","    \r\n","    # Loop indefinitely\r\n","    while not stop:  \r\n","        \r\n","        # create a batch with positive and negative examples\r\n","        batch = []\r\n","        \r\n","        # First part: Pack n_to_take positive examples\r\n","        \r\n","        # Start from pos_index and increment i up to n_to_take\r\n","        for i in range(n_to_take):\r\n","                    \r\n","            # If the positive index goes past the positive dataset lenght,\r\n","            if pos_index >= len_data_pos: \r\n","                \r\n","                # If loop is set to False, break once we reach the end of the dataset\r\n","                if not loop:\r\n","                    stop = True;\r\n","                    break;\r\n","                \r\n","                # If user wants to keep re-using the data, reset the index\r\n","                pos_index = 0\r\n","                \r\n","                if shuffle:\r\n","                    # Shuffle the index of the positive sample\r\n","                    rnd.shuffle(pos_index_lines)\r\n","                    \r\n","            # get the tweet as pos_index\r\n","            tweet = data_pos[pos_index_lines[pos_index]]\r\n","            \r\n","            # convert the tweet into tensors of integers representing the processed words\r\n","            tensor = tweet_to_tensor(tweet, vocab_dict)\r\n","            \r\n","            # append the tensor to the batch list\r\n","            batch.append(tensor)\r\n","            \r\n","            # Increment pos_index by one\r\n","            pos_index = pos_index + 1\r\n","\r\n","### END GIVEN CODE ###\r\n","            \r\n","### START CODE HERE (Replace instances of 'None' with your code) ###\r\n","\r\n","        # Second part: Pack n_to_take negative examples\r\n","    \r\n","        # Using the same batch list, start from neg_index and increment i up to n_to_take\r\n","        for i in range(n_to_take):\r\n","            \r\n","            # If the negative index goes past the negative dataset length,\r\n","            if neg_index >= len_data_neg:\r\n","                \r\n","                # If loop is set to False, break once we reach the end of the dataset\r\n","                if not loop:\r\n","                    stop = True;\r\n","                    break;\r\n","                    \r\n","                # If user wants to keep re-using the data, reset the index\r\n","                neg_index = 0\r\n","                \r\n","                if shuffle:\r\n","                    # Shuffle the index of the negative sample\r\n","                    rnd.shuffle(neg_index_lines)\r\n","            # get the tweet as neg_index\r\n","            tweet = data_neg[neg_index_lines[neg_index]]\r\n","            \r\n","            # convert the tweet into tensors of integers representing the processed words\r\n","            tensor = tweet_to_tensor(tweet, vocab_dict)\r\n","            \r\n","            # append the tensor to the batch list\r\n","            batch.append(tensor)\r\n","            \r\n","            # Increment neg_index by one\r\n","            neg_index = neg_index + 1\r\n","\r\n","### END CODE HERE ###        \r\n","\r\n","### START GIVEN CODE ###\r\n","        if stop:\r\n","            break;\r\n","\r\n","        # Update the start index for positive data \r\n","        # so that it's n_to_take positions after the current pos_index\r\n","        pos_index += n_to_take\r\n","        \r\n","        # Update the start index for negative data \r\n","        # so that it's n_to_take positions after the current neg_index\r\n","        neg_index += n_to_take\r\n","        \r\n","        # Get the max tweet length (the length of the longest tweet) \r\n","        # (you will pad all shorter tweets to have this length)\r\n","        max_len = max([len(t) for t in batch]) \r\n","        \r\n","        \r\n","        # Initialize the input_l, which will \r\n","        # store the padded versions of the tensors\r\n","        tensor_pad_l = []\r\n","        # Pad shorter tweets with zeros\r\n","        for tensor in batch:\r\n","### END GIVEN CODE ###\r\n","\r\n","### START CODE HERE (Replace instances of 'None' with your code) ###\r\n","            # Get the number of positions to pad for this tensor so that it will be max_len long\r\n","            n_pad = max_len - len(tensor)\r\n","            \r\n","            # Generate a list of zeros, with length n_pad\r\n","            pad_l = [0] * n_pad\r\n","            \r\n","            # concatenate the tensor and the list of padded zeros\r\n","            tensor_pad = tensor + pad_l\r\n","            \r\n","            # append the padded tensor to the list of padded tensors\r\n","            tensor_pad_l.append(tensor_pad)\r\n","\r\n","        # convert the list of padded tensors to a numpy array\r\n","        # and store this as the model inputs\r\n","        inputs = np.array(tensor_pad_l)\r\n","  \r\n","        # Generate the list of targets for the positive examples (a list of ones)\r\n","        # The length is the number of positive examples in the batch\r\n","        target_pos = np.ones(n_to_take)\r\n","        \r\n","        # Generate the list of targets for the negative examples (a list of zeros)\r\n","        # The length is the number of negative examples in the batch\r\n","        target_neg = np.zeros(n_to_take)\r\n","        \r\n","        # Concatenate the positve and negative targets\r\n","        target_l = np.append(target_pos, target_neg)\r\n","        \r\n","        # Convert the target list into a numpy array\r\n","        targets = target_l\r\n","\r\n","        # Example weights: Treat all examples equally importantly.It should return an np.array. Hint: Use np.ones_like()\r\n","        example_weights = np.ones_like(targets)\r\n","        \r\n","\r\n","### END CODE HERE ###\r\n","\r\n","### GIVEN CODE ###\r\n","        # note we use yield and not return\r\n","        yield inputs, targets, example_weights"],"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RfjkH8K8fWDH"},"source":["Now you can use your data generator to create a data generator for the training data, and another data generator for the validation data.\r\n","\r\n","We will create a third data generator that does not loop, for testing the final accuracy of the model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0BLX0FtnYrjf","executionInfo":{"status":"ok","timestamp":1611561315019,"user_tz":480,"elapsed":604,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"3ce8c42a-c25c-4209-ea76-b933fc2ccdfc"},"source":["# Set the random number generator for the shuffle procedure\r\n","rnd.seed(30) \r\n","\r\n","# Create the training data generator\r\n","def train_generator(batch_size, shuffle = False):\r\n","    return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\r\n","\r\n","# Create the validation data generator\r\n","def val_generator(batch_size, shuffle = False):\r\n","    return data_generator(val_pos, val_neg, batch_size, True, Vocab, shuffle)\r\n","\r\n","# Create the validation data generator\r\n","def test_generator(batch_size, shuffle = False):\r\n","    return data_generator(val_pos, val_neg, batch_size, False, Vocab, shuffle)\r\n","\r\n","# Get a batch from the train_generator and inspect.\r\n","inputs, targets, example_weights = next(train_generator(4, shuffle=True))\r\n","\r\n","# this will print a list of 4 tensors padded with zeros\r\n","print(f'Inputs: {inputs}')\r\n","print(f'Targets: {targets}')\r\n","print(f'Example Weights: {example_weights}')"],"execution_count":55,"outputs":[{"output_type":"stream","text":["Inputs: [[2005 4451 3201    9    0    0    0    0    0    0    0]\n"," [4954  567 2000 1454 5174 3499  141 3499  130  459    9]\n"," [3761  109  136  583 2930 3969    0    0    0    0    0]\n"," [ 250 3761    0    0    0    0    0    0    0    0    0]]\n","Targets: [1. 1. 0. 0.]\n","Example Weights: [1. 1. 1. 1.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ncFgjEMwhvb0","executionInfo":{"status":"ok","timestamp":1611561318020,"user_tz":480,"elapsed":637,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"e1ac5f8f-db6f-44ee-a33f-0c48f562b81d"},"source":["# Test the train_generator\r\n","\r\n","# Create a data generator for training data,\r\n","# which produces batches of size 4 (for tensors and their respective targets)\r\n","tmp_data_gen = train_generator(batch_size = 4)\r\n","\r\n","# Call the data generator to get one batch and its targets\r\n","tmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)\r\n","\r\n","print(f\"The inputs shape is {tmp_inputs.shape}\")\r\n","print(f\"The targets shape is {tmp_targets.shape}\")\r\n","print(f\"The example weights shape is {tmp_example_weights.shape}\")\r\n","\r\n","for i,t in enumerate(tmp_inputs):\r\n","    print(f\"input tensor: {t}; target {tmp_targets[i]}; example weights {tmp_example_weights[i]}\")"],"execution_count":56,"outputs":[{"output_type":"stream","text":["The inputs shape is (4, 14)\n","The targets shape is (4,)\n","The example weights shape is (4,)\n","input tensor: [3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target 1.0; example weights 1.0\n","input tensor: [10 11 12 13 14 15 16 17 18 19 20  9 21 22]; target 1.0; example weights 1.0\n","input tensor: [5738 2901 3761    0    0    0    0    0    0    0    0    0    0    0]; target 0.0; example weights 1.0\n","input tensor: [ 858  256 3652 5739  307 4458  567 1230 2767  328 1202 3761    0    0]; target 0.0; example weights 1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M6SKXdXKhyZ-"},"source":["##### Expected output\r\n","\r\n","```CPP\r\n","The inputs shape is (4, 14)\r\n","The targets shape is (4,)\r\n","The example weights shape is (4,)\r\n","input tensor: [3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target 1; example weights 1\r\n","input tensor: [10 11 12 13 14 15 16 17 18 19 20  9 21 22]; target 1; example weights 1\r\n","input tensor: [5738 2901 3761    0    0    0    0    0    0    0    0    0    0    0]; target 0; example weights 1\r\n","input tensor: [ 858  256 3652 5739  307 4458  567 1230 2767  328 1202 3761    0    0]; target 0; example weights 1\r\n","```"]},{"cell_type":"markdown","metadata":{"id":"2YHPg0MjipB-"},"source":["Now that you have your train/val generators, you can just call them and they will return tensors which correspond to your tweets in the first column and their corresponding labels in the second column. Now you can go ahead and start building your neural network. "]},{"cell_type":"markdown","metadata":{"id":"Et5-10hXizY5"},"source":["<a name=\"3\"></a>\r\n","# Part 3:  Defining classes\r\n","\r\n","In this part, you will write your own library of layers. It will be very similar\r\n","to the one used in Trax and also in Keras and PyTorch. Writing your own small\r\n","framework will help you understand how they all work and use them effectively\r\n","in the future.\r\n","\r\n","Your framework will be based on the following `Layer` class from utils.py.\r\n","\r\n","```CPP\r\n","class Layer(object):\r\n","    \"\"\" Base class for layers.\r\n","    \"\"\"\r\n","      \r\n","    # Constructor\r\n","    def __init__(self):\r\n","        # set weights to None\r\n","        self.weights = None\r\n","\r\n","    # The forward propagation should be implemented\r\n","    # by subclasses of this Layer class\r\n","    def forward(self, x):\r\n","        raise NotImplementedError\r\n","\r\n","    # This function initializes the weights\r\n","    # based on the input signature and random key,\r\n","    # should be implemented by subclasses of this Layer class\r\n","    def init_weights_and_state(self, input_signature, random_key):\r\n","        pass\r\n","\r\n","    # This initializes and returns the weights, do not override.\r\n","    def init(self, input_signature, random_key):\r\n","        self.init_weights_and_state(input_signature, random_key)\r\n","        return self.weights\r\n"," \r\n","    # __call__ allows an object of this class\r\n","    # to be called like it's a function.\r\n","    def __call__(self, x):\r\n","        # When this layer object is called, \r\n","        # it calls its forward propagation function\r\n","        return self.forward(x)\r\n","```"]},{"cell_type":"markdown","metadata":{"id":"UkweJh0Pi6PT"},"source":[""]},{"cell_type":"code","metadata":{"id":"3uww3UchYrgk"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pZAz1RKIYrdw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4HTpqGVYrar"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQIe-UiMYrXA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TI22oLblYrOc"},"source":[""],"execution_count":null,"outputs":[]}]}