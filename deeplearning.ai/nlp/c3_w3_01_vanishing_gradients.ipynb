{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c3_w3_01_vanishing_gradients.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMFKfaf1ANPL2qDC3HYdhdF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AK3adqcIANOw"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\r\n","  <td>\r\n","    <a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/nlp/c3_w3_01_vanishing_gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>    \r\n","  </td>\r\n","  <td>\r\n","    <a href=\"https://github.com/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/nlp/c3_w3_01_vanishing_gradients.ipynb\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/martin-fabbri/colab-notebooks/master/assets/github.svg\" alt=\"View On Github\"/></a>  </td>\r\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"DjRj1H2aABmO"},"source":["# Vanishing Gradients : Ungraded Lecture Notebook\r\n","In this notebook you'll take another look at vanishing gradients, from an intuitive standpoint.\r\n","## Background\r\n","Adding layers to a neural network introduces multiplicative effects in both forward and backward propagation. The back prop in particular presents a problem as the gradient of activation functions can be very small. Multiplied together across many layers, their product can be vanishingly small! This results in weights not being updated in the front layers and training not progressing.\r\n","<br/><br/>\r\n","Gradients of the sigmoid function, for example, are in the range 0 to 0.25. To calculate gradients for the front layers of a neural network the chain rule is used. This means that these tiny values are multiplied starting at the last layer, working backwards to the first layer, with the gradients shrinking exponentially at each step.\r\n","## Imports"]},{"cell_type":"code","metadata":{"id":"1KAeXt0y_-7a"},"source":[""],"execution_count":null,"outputs":[]}]}