{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"c2_w4_model_architecture_relu_sigmoid.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"lyf5oCHLIksI"},"source":["# Word Embeddings: Intro to CBOW model, activation functions and working with Numpy\n","\n","In this lecture notebook you will be given an introduction to the continuous bag-of-words model, its activation functions and some considerations when working with Numpy. \n","\n","Let's dive into it!"]},{"cell_type":"code","metadata":{"id":"VUO1-Ax5IksQ"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YMxDsF3UIksR"},"source":["# The continuous bag-of-words model"]},{"cell_type":"markdown","metadata":{"id":"XWofcqv0IksS"},"source":["The CBOW model is based on a neural network, the architecture of which looks like the figure below, as you'll recall from the lecture.\n","\n","<img src='' alt=\"alternate text\" width=\"width\">\n"]},{"cell_type":"markdown","metadata":{"id":"oyD5hTcKIksS"},"source":["## Activation functions"]},{"cell_type":"markdown","metadata":{"id":"TGqNS2D0IksT"},"source":["Let's start by implementing the activation functions, ReLU and softmax."]},{"cell_type":"markdown","metadata":{"id":"stwES5DDIksT"},"source":["### ReLU"]},{"cell_type":"markdown","metadata":{"id":"05XjeLGYIksT"},"source":["ReLU is used to calculate the values of the hidden layer, in the following formulas:\n","\n","\\begin{align}\n"," \\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n"," \\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n","\\end{align}\n"]},{"cell_type":"markdown","metadata":{"id":"18XlKCQ0IksU"},"source":["Let's fix a value for $\\mathbf{z_1}$ as a working example."]},{"cell_type":"code","metadata":{"id":"EGE8Wz9VIksU"},"source":["# Define a random seed so all random outcomes can be reproduced\n","np.random.seed(10)\n","\n","# Define a 5X1 column vector using numpy\n","z_1 = 10*np.random.rand(5, 1)-5\n","\n","# Print the vector\n","z_1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M4UrootRIksV"},"source":["Notice that using numpy's `random.rand` function returns a numpy array filled with values taken from a uniform distribution over [0, 1). Numpy allows vectorization so each value is multiplied by 10 and then substracted 5."]},{"cell_type":"markdown","metadata":{"id":"gpK8HyY-IksV"},"source":["To get the ReLU of this vector, you want all the negative values to become zeros.\n","\n","First create a copy of this vector."]},{"cell_type":"code","metadata":{"id":"68qiU_9eIksV"},"source":["# Create copy of vector and save it in the 'h' variable\n","h = z_1.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jlfsmXWeIksW"},"source":["Now determine which of its values are negative."]},{"cell_type":"code","metadata":{"id":"VqKJTLdUIksW"},"source":["# Determine which values met the criteria (this is possible because of vectorization)\n","h < 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ItMhNVGQIksW"},"source":["You can now simply set all of the values which are negative to 0."]},{"cell_type":"code","metadata":{"id":"oNfDyG8jIksX"},"source":["# Slice the array or vector. This is the same as applying ReLU to it\n","h[h < 0] = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zSqCkZ3UIksX"},"source":["And that's it: you have the ReLU of $\\mathbf{z_1}$!"]},{"cell_type":"code","metadata":{"id":"aUQpgf7zIksX"},"source":["# Print the vector after ReLU\n","h"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Olh5B-B-IksY"},"source":["**Now implement ReLU as a function.**"]},{"cell_type":"code","metadata":{"id":"2RxxEucCIksY"},"source":["# Define the 'relu' function that will include the steps previously seen\n","def relu(z):\n","    result = z.copy()\n","    result[result < 0] = 0\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F4xKocYmIksY"},"source":["**And check that it's working.**"]},{"cell_type":"code","metadata":{"id":"5h2fB4eEIksY"},"source":["# Define a new vector and save it in the 'z' variable\n","z = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\n","\n","# Apply ReLU to it\n","relu(z)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qJZYIPw6IksZ"},"source":["Expected output:\n","\n","    array([[0.        ],\n","           [4.50714306],\n","           [2.31993942],\n","           [0.98658484],\n","           [0.        ]])"]},{"cell_type":"markdown","metadata":{"id":"4zPb_B2uIksZ"},"source":["### Softmax"]},{"cell_type":"markdown","metadata":{"id":"ZXGDzCRZIksZ"},"source":["The second activation function that you need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n","\n","\\begin{align}\n"," \\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n"," \\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n","\\end{align}\n","\n","To calculate softmax of a vector $\\mathbf{z}$, the $i$-th component of the resulting vector is given by:\n","\n","$$ \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} $$\n","\n","Let's work through an example."]},{"cell_type":"code","metadata":{"id":"ejx3QSi1Iksa"},"source":["# Define a new vector and save it in the 'z' variable\n","z = np.array([9, 8, 11, 10, 8.5])\n","\n","# Print the vector\n","z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Eh7goy4Iksa"},"source":["You'll need to calculate the exponentials of each element, both for the numerator and for the denominator."]},{"cell_type":"code","metadata":{"id":"5nftqymLIksa"},"source":["# Save exponentials of the values in a new vector\n","e_z = np.exp(z)\n","\n","# Print the vector with the exponential values\n","e_z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bI4D9C-MIksb"},"source":["The denominator is equal to the sum of these exponentials."]},{"cell_type":"code","metadata":{"id":"HBVgJ9OqIksb"},"source":["# Save the sum of the exponentials\n","sum_e_z = np.sum(e_z)\n","\n","# Print sum of exponentials\n","sum_e_z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tQclDWX9Iksb"},"source":["And the value of the first element of $\\textrm{softmax}(\\textbf{z})$ is given by:"]},{"cell_type":"code","metadata":{"id":"l8heHNMFIksb"},"source":["# Print softmax value of the first element in the original vector\n","e_z[0]/sum_e_z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L92c-75yIksc"},"source":["This is for one element. You can use numpy's vectorized operations to calculate the values of all the elements of the $\\textrm{softmax}(\\textbf{z})$ vector in one go.\n","\n","**Implement the softmax function.**"]},{"cell_type":"code","metadata":{"id":"N5Oovt_LIksc"},"source":["# Define the 'softmax' function that will include the steps previously seen\n","def softmax(z):\n","    e_z = np.exp(z)\n","    sum_e_z = np.sum(e_z)\n","    return e_z / sum_e_z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cs9OWwBfIksc"},"source":["**Now check that it works.**"]},{"cell_type":"code","metadata":{"id":"mVUKqomKIksc"},"source":["# Print softmax values for original vector\n","softmax([9, 8, 11, 10, 8.5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pQu-NssjIksd"},"source":["Expected output:\n","\n","    array([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])"]},{"cell_type":"markdown","metadata":{"id":"ZEzIAaysIksd"},"source":["Notice that the sum of all these values is equal to 1."]},{"cell_type":"code","metadata":{"id":"uN83JlaHIksd"},"source":["# Assert that the sum of the softmax values is equal to 1\n","np.sum(softmax([9, 8, 11, 10, 8.5])) == 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LeRtP2ToIksd"},"source":["## Dimensions: 1-D arrays vs 2-D column vectors\n","\n","Before moving on to implement forward propagation, backpropagation, and gradient descent in the next lecture notebook, let's have a look at the dimensions of the vectors you've been handling until now.\n","\n","Create a vector of length $V$ filled with zeros."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"iwfG1cilIkse"},"source":["# Define V. Remember this was the size of the vocabulary in the previous lecture notebook\n","V = 5\n","\n","# Define vector of length V filled with zeros\n","x_array = np.zeros(V)\n","\n","# Print vector\n","x_array"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sk92uj1wIkse"},"source":["This is a 1-dimensional array, as revealed by the `.shape` property of the array."]},{"cell_type":"code","metadata":{"id":"XIGc-qOrIkse"},"source":["# Print vector's shape\n","x_array.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s0v42IqPIksf"},"source":["To perform matrix multiplication in the next steps, you actually need your column vectors to be represented as a matrix with one column. In numpy, this matrix is represented as a 2-dimensional array.\n","\n","The easiest way to convert a 1D vector to a 2D column matrix is to set its `.shape` property to the number of rows and one column, as shown in the next cell."]},{"cell_type":"code","metadata":{"id":"WzLniXcIIksf"},"source":["# Copy vector\n","x_column_vector = x_array.copy()\n","\n","# Reshape copy of vector\n","x_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\n","\n","# Print vector\n","x_column_vector"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kat0gIg3Iksg"},"source":["The shape of the resulting \"vector\" is:"]},{"cell_type":"code","metadata":{"id":"FHm_KAudIksg"},"source":["# Print vector's shape\n","x_column_vector.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yzUtMjoKIksg"},"source":["So you now have a 5x1 matrix that you can use to perform standard matrix multiplication."]},{"cell_type":"markdown","metadata":{"id":"1i0kjl29Iksh"},"source":["**Congratulations on finishing this lecture notebook!** Hopefully you now have a better understanding of the activation functions used in the continuous bag-of-words model, as well as a clearer idea of how to leverage Numpy's power for these types of mathematical computations.\n","\n","In the next lecture notebook you will get a comprehensive dive into:\n","\n","- Forward propagation.\n","\n","- Cross-entropy loss.\n","\n","- Backpropagation.\n","\n","- Gradient descent.\n","\n","**See you next time!**"]}]}