{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"c2_w4_model_architecture_relu_sigmoid.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"lyf5oCHLIksI"},"source":["# Word Embeddings: Intro to CBOW model, activation functions and working with Numpy\n","\n","In this lecture notebook you will be given an introduction to the continuous bag-of-words model, its activation functions and some considerations when working with Numpy. \n","\n","Let's dive into it!"]},{"cell_type":"code","metadata":{"id":"VUO1-Ax5IksQ"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YMxDsF3UIksR"},"source":["# The continuous bag-of-words model"]},{"cell_type":"markdown","metadata":{"id":"XWofcqv0IksS"},"source":["The CBOW model is based on a neural network, the architecture of which looks like the figure below, as you'll recall from the lecture.\n","\n","<img src='https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/nlp/images/cbow_model_architecture.png' alt=\"alternate text\" width=\"600px\">\n"]},{"cell_type":"markdown","metadata":{"id":"oyD5hTcKIksS"},"source":["## Activation functions"]},{"cell_type":"markdown","metadata":{"id":"TGqNS2D0IksT"},"source":["Let's start by implementing the activation functions, ReLU and softmax."]},{"cell_type":"markdown","metadata":{"id":"stwES5DDIksT"},"source":["### ReLU"]},{"cell_type":"markdown","metadata":{"id":"05XjeLGYIksT"},"source":["ReLU is used to calculate the values of the hidden layer, in the following formulas:\n","\n","\\begin{align}\n"," \\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n"," \\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n","\\end{align}\n"]},{"cell_type":"markdown","metadata":{"id":"18XlKCQ0IksU"},"source":["Let's fix a value for $\\mathbf{z_1}$ as a working example."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EGE8Wz9VIksU","executionInfo":{"status":"ok","timestamp":1611186563925,"user_tz":480,"elapsed":1047,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"9714013a-5071-49cc-9a03-286691cb0e4d"},"source":["np.random.seed(10)\n","\n","z_1 = 10 * np.random.rand(5, 1) - 5\n","\n","z_1"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 2.71320643],\n","       [-4.79248051],\n","       [ 1.33648235],\n","       [ 2.48803883],\n","       [-0.01492988]])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"M4UrootRIksV"},"source":["Notice that using numpy's `random.rand` function returns a numpy array filled with values taken from a uniform distribution over [0, 1). Numpy allows vectorization so each value is multiplied by 10 and then substracted 5."]},{"cell_type":"markdown","metadata":{"id":"gpK8HyY-IksV"},"source":["To get the ReLU of this vector, you want all the negative values to become zeros.\n","\n","First create a copy of this vector."]},{"cell_type":"code","metadata":{"id":"68qiU_9eIksV"},"source":["h = z_1.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jlfsmXWeIksW"},"source":["Now determine which of its values are negative."]},{"cell_type":"code","metadata":{"id":"VqKJTLdUIksW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611186580622,"user_tz":480,"elapsed":510,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"f46577f2-7921-428d-932f-90a5a843c7d1"},"source":["# Determine which values met the criteria (this is possible because of vectorization)\n","h < 0"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[False],\n","       [ True],\n","       [False],\n","       [False],\n","       [ True]])"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"ItMhNVGQIksW"},"source":["You can now simply set all of the values which are negative to 0."]},{"cell_type":"code","metadata":{"id":"oNfDyG8jIksX"},"source":["# Slice the array or vector. This is the same as applying ReLU to it\n","h[h < 0] = 0  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zSqCkZ3UIksX"},"source":["And that's it: you have the ReLU of $\\mathbf{z_1}$!"]},{"cell_type":"code","metadata":{"id":"aUQpgf7zIksX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611186679940,"user_tz":480,"elapsed":397,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"ddc449e2-adf1-4279-8f20-f02abe4cfaef"},"source":["# Print the vector after ReLU\n","h"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2.71320643],\n","       [0.        ],\n","       [1.33648235],\n","       [2.48803883],\n","       [0.        ]])"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"Olh5B-B-IksY"},"source":["**Now implement ReLU as a function.**"]},{"cell_type":"code","metadata":{"id":"2RxxEucCIksY"},"source":["# Define the 'relu' function that will include the steps previously seen\n","def relu(z):\n","    result = z.copy()\n","    result[result < 0] = 0\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F4xKocYmIksY"},"source":["**And check that it's working.**"]},{"cell_type":"code","metadata":{"id":"5h2fB4eEIksY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611187139293,"user_tz":480,"elapsed":312,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"50edb5f3-ae85-48a4-cb08-03e3dc6f7383"},"source":["# Define a new vector and save it in the 'z' variable\n","z = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\n","\n","# Apply ReLU to it\n","relu(z)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        ],\n","       [4.50714306],\n","       [2.31993942],\n","       [0.98658484],\n","       [0.        ]])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"qJZYIPw6IksZ"},"source":["Expected output:\n","\n","    array([[0.        ],\n","           [4.50714306],\n","           [2.31993942],\n","           [0.98658484],\n","           [0.        ]])"]},{"cell_type":"markdown","metadata":{"id":"4zPb_B2uIksZ"},"source":["### Softmax"]},{"cell_type":"markdown","metadata":{"id":"ZXGDzCRZIksZ"},"source":["The second activation function that you need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n","\n","\\begin{align}\n"," \\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n"," \\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n","\\end{align}\n","\n","To calculate softmax of a vector $\\mathbf{z}$, the $i$-th component of the resulting vector is given by:\n","\n","$$ \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} $$\n","\n","Let's work through an example."]},{"cell_type":"code","metadata":{"id":"ejx3QSi1Iksa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611187606809,"user_tz":480,"elapsed":482,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"76592e87-4a16-475f-b17b-ea5b7e8f757e"},"source":["# Define a new vector and save it in the 'z' variable\n","z = np.array([9, 8, 11, 10, 8.5])\n","\n","# Print the vector\n","z"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 9. ,  8. , 11. , 10. ,  8.5])"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"2Eh7goy4Iksa"},"source":["You'll need to calculate the exponentials of each element, both for the numerator and for the denominator."]},{"cell_type":"code","metadata":{"id":"5nftqymLIksa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611187642966,"user_tz":480,"elapsed":413,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"6fdb42a2-3a15-4e0f-9bcd-2cf997bfe413"},"source":["# Save exponentials of the values in a new vector\n","e_z = np.exp(z)\n","# Print the vector with the exponential values\n","e_z"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 8103.08392758,  2980.95798704, 59874.1417152 , 22026.46579481,\n","        4914.7688403 ])"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"bI4D9C-MIksb"},"source":["The denominator is equal to the sum of these exponentials."]},{"cell_type":"code","metadata":{"id":"HBVgJ9OqIksb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611187864279,"user_tz":480,"elapsed":386,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"dc44d685-2063-4958-adc6-4f2df60ba4ef"},"source":["# Save the sum of the exponentials\n","sum_e_z = np.sum(e_z)\n","\n","# Print sum of exponentials\n","sum_e_z"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["97899.41826492078"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"tQclDWX9Iksb"},"source":["And the value of the first element of $\\textrm{softmax}(\\textbf{z})$ is given by:"]},{"cell_type":"code","metadata":{"id":"l8heHNMFIksb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611187922431,"user_tz":480,"elapsed":406,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"75ac208b-5631-4f25-9321-14fd7d75cfa6"},"source":["# Print softmax value of the first element in the original vector\n","e_z[0] / sum_e_z"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.08276947985173956"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"L92c-75yIksc"},"source":["This is for one element. You can use numpy's vectorized operations to calculate the values of all the elements of the $\\textrm{softmax}(\\textbf{z})$ vector in one go.\n","\n","**Implement the softmax function.**"]},{"cell_type":"code","metadata":{"id":"N5Oovt_LIksc","executionInfo":{"status":"ok","timestamp":1611188083708,"user_tz":480,"elapsed":487,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["# Define the 'softmax' function that will include the steps previously seen\n","def softmax(z):\n","    e_z = np.exp(z)\n","    sum_e_z = np.sum(e_z)\n","    return e_z / sum_e_z"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cs9OWwBfIksc"},"source":["**Now check that it works.**"]},{"cell_type":"code","metadata":{"id":"mVUKqomKIksc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611188093418,"user_tz":480,"elapsed":378,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"827f7032-7a96-4f71-b439-8b4156be1ecd"},"source":["# Print softmax values for original vector\n","softmax([9, 8, 11, 10, 8.5])"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"pQu-NssjIksd"},"source":["Expected output:\n","\n","    array([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])"]},{"cell_type":"markdown","metadata":{"id":"ZEzIAaysIksd"},"source":["Notice that the sum of all these values is equal to 1."]},{"cell_type":"code","metadata":{"id":"uN83JlaHIksd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611188128750,"user_tz":480,"elapsed":396,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"68aee065-4309-443a-d6ed-2bd20777b55f"},"source":["# Assert that the sum of the softmax values is equal to 1\n","sum(softmax([9, 8, 11, 10, 8.5]))"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"LeRtP2ToIksd"},"source":["## Dimensions: 1-D arrays vs 2-D column vectors\n","\n","Before moving on to implement forward propagation, backpropagation, and gradient descent in the next lecture notebook, let's have a look at the dimensions of the vectors you've been handling until now.\n","\n","Create a vector of length $V$ filled with zeros."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"iwfG1cilIkse","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611195414513,"user_tz":480,"elapsed":658,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"498f28e8-a615-4f5f-839d-a8ef3bf5b429"},"source":["# Define V. Remember this was the size of the vocabulary in the previous lecture notebook\n","V = 5\n","\n","# Define vector of length V filled with zeros\n","x_array = np.zeros(V)\n","\n","# Print vector\n","x_array"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"Sk92uj1wIkse"},"source":["This is a 1-dimensional array, as revealed by the `.shape` property of the array."]},{"cell_type":"code","metadata":{"id":"XIGc-qOrIkse","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611195430086,"user_tz":480,"elapsed":387,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"c41e9364-bdf7-4db0-b707-997d1d1b32d3"},"source":["# Print vector's shape\n","x_array.shape"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5,)"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"s0v42IqPIksf"},"source":["To perform matrix multiplication in the next steps, you actually need your column vectors to be represented as a matrix with one column. In numpy, this matrix is represented as a 2-dimensional array.\n","\n","The easiest way to convert a 1D vector to a 2D column matrix is to set its `.shape` property to the number of rows and one column, as shown in the next cell."]},{"cell_type":"code","metadata":{"id":"WzLniXcIIksf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611195528592,"user_tz":480,"elapsed":513,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"60b61c70-8bfe-4c62-97df-3deb88cc9c7f"},"source":["# Copy vector\n","x_column_vector = x_array.copy()\n","\n","# Reshape copy of vector\n","x_column_vector.shape = (V, 1)\n","\n","# Print vector\n","x_column_vector"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.]])"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"kat0gIg3Iksg"},"source":["The shape of the resulting \"vector\" is:"]},{"cell_type":"code","metadata":{"id":"FHm_KAudIksg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611195540716,"user_tz":480,"elapsed":634,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"1d07b3f5-9281-492a-e363-1c79b4e6d411"},"source":["# Print vector's shape\n","x_column_vector.shape"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5, 1)"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"yzUtMjoKIksg"},"source":["So you now have a 5x1 matrix that you can use to perform standard matrix multiplication."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pwu-eRy5vPyv","executionInfo":{"status":"ok","timestamp":1611195661129,"user_tz":480,"elapsed":398,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"e433038d-5f85-49bd-8032-f9098ce51977"},"source":["x_expand_dim_column_vec = np.expand_dims(x_array, axis=1)\r\n","print(x_expand_dim_column_vec)\r\n","print(x_expand_dim_column_vec.shape)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["[[0.]\n"," [0.]\n"," [0.]\n"," [0.]\n"," [0.]]\n","(5, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lff4hugHvuxy","executionInfo":{"status":"ok","timestamp":1611195682501,"user_tz":480,"elapsed":355,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"cc2a71c0-be9d-48b2-873e-829ba1201048"},"source":["x_array.shape"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5,)"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"1i0kjl29Iksh"},"source":["**Congratulations on finishing this lecture notebook!** Hopefully you now have a better understanding of the activation functions used in the continuous bag-of-words model, as well as a clearer idea of how to leverage Numpy's power for these types of mathematical computations.\n","\n","In the next lecture notebook you will get a comprehensive dive into:\n","\n","- Forward propagation.\n","\n","- Cross-entropy loss.\n","\n","- Backpropagation.\n","\n","- Gradient descent.\n","\n","**See you next time!**"]}]}