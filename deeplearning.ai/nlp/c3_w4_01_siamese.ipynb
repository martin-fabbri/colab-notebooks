{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c3_w4_01_siamese.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNsGu02pQL7PrHmUG58IQqf"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"r-WiHdK2A7-x"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\r\n","  <td>\r\n","    <a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/nlp/c3_w4_01_siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>    \r\n","  </td>\r\n","  <td>\r\n","    <a href=\"https://github.com/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/nlp/c3_w4_01_siamese.ipynb\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/martin-fabbri/colab-notebooks/master/assets/github.svg\" alt=\"View On Github\"/></a>  </td>\r\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"hi01zk3I4PcF"},"source":["# Creating a Siamese model using Trax: Ungraded Lecture Notebook"]},{"cell_type":"code","metadata":{"id":"Wm79XLmo4X9_","executionInfo":{"status":"ok","timestamp":1611902728128,"user_tz":480,"elapsed":27540,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["%%capture\r\n","!pip install trax==1.3.1"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gtYPYCFF4im8","executionInfo":{"status":"ok","timestamp":1611902763275,"user_tz":480,"elapsed":987,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"7d1c4158-aafa-4778-9f9e-26920e47f891"},"source":["!pip list | grep trax"],"execution_count":3,"outputs":[{"output_type":"stream","text":["trax                          1.3.1                \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ugnjS69F4N6k","executionInfo":{"status":"ok","timestamp":1611903262337,"user_tz":480,"elapsed":397,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["import trax\r\n","from trax import layers as tl\r\n","import trax.fastmath.numpy as np\r\n","import numpy\r\n","\r\n","trax.supervised.trainer_lib.init_random_number_generators(10)\r\n","numpy.random.seed(10)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mjG0ZAbY7ACB"},"source":["## L2 Normalization\r\n","\r\n","Before building the model you will need to define a function that applies L2 normalization to a tensor. This is very important because in this week's assignment you will create a custom loss function which expects the tensors it receives to be normalized. Luckily this is pretty straightforward:"]},{"cell_type":"code","metadata":{"id":"hfcUR9x04VLa","executionInfo":{"status":"ok","timestamp":1611903390174,"user_tz":480,"elapsed":334,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["def normalize(x):\r\n","    return x / np.sqrt(np.sum(x * x, axis=-1, keepdims=True))"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lFM4OQZi7g3M"},"source":["Notice that the denominator can be replaced by `np.linalg.norm(x, axis=-1, keepdims=True)` to achieve the same results and that Trax's numpy is being used within the function."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7qWtsHea7dBA","executionInfo":{"status":"ok","timestamp":1611903503581,"user_tz":480,"elapsed":655,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"2539e6e6-4a2f-4888-a51d-b9d49307d6dd"},"source":["tensor = numpy.random.random((2, 5))\r\n","print(f\"The tensor is of type: {type(tensor)}\\n\\nAnd looks like this:\\n\\n {tensor}\")"],"execution_count":11,"outputs":[{"output_type":"stream","text":["The tensor is of type: <class 'numpy.ndarray'>\n","\n","And looks like this:\n","\n"," [[0.77132064 0.02075195 0.63364823 0.74880388 0.49850701]\n"," [0.22479665 0.19806286 0.76053071 0.16911084 0.08833981]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ospEn3gS71oj","executionInfo":{"status":"ok","timestamp":1611904581726,"user_tz":480,"elapsed":541,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"2ebdbf9e-9e9f-47f3-ee43-c46259f9e2fd"},"source":["norm_tensor = normalize(tensor)\r\n","print(f\"The normalized tensor is of type: {type(norm_tensor)}\\n\\nAnd looks like this:\\n\\n {norm_tensor}\")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["The normalized tensor is of type: <class 'jax.interpreters.xla._DeviceArray'>\n","\n","And looks like this:\n","\n"," [[0.57393795 0.01544148 0.4714962  0.55718327 0.37093794]\n"," [0.26781026 0.23596111 0.9060541  0.20146926 0.10524315]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V8HiQ0pRAdnW"},"source":["Notice that the initial tensor was converted from a numpy array to a jax array in the process."]},{"cell_type":"markdown","metadata":{"id":"sm_MupzoApj-"},"source":["## Siamese Model\r\n","\r\n","To create a `Siamese` model you will first need to create a LSTM model using the `Serial` combinator layer and then use another combinator layer called `Parallel` to create the Siamese model. You should be familiar with the following layers (notice each layer can be clicked to go to the docs):\r\n","   - [`Serial`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial) A combinator layer that allows to stack layers serially using function composition.\r\n","   - [`Embedding`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding) Maps discrete tokens to vectors. It will have shape `(vocabulary length X dimension of output vectors)`. The dimension of output vectors (also called `d_feature`) is the number of elements in the word embedding.\r\n","   - [`LSTM`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM) The LSTM layer. It leverages another Trax layer called [`LSTMCell`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTMCell). The number of units should be specified and should match the number of elements in the word embedding.\r\n","   - [`Mean`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean) Computes the mean across a desired axis. Mean uses one tensor axis to form groups of values and replaces each group with the mean value of that group.\r\n","   - [`Fn`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn) Layer with no weights that applies the function f, which should be specified using a lambda syntax. \r\n","   - [`Parallel`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Parallel) It is a combinator layer (like `Serial`) that applies a list of layers in parallel to its inputs.\r\n","\r\n","Putting everything together the Siamese model will look like this:"]},{"cell_type":"code","metadata":{"id":"EukzRk5q__3Z"},"source":[""],"execution_count":null,"outputs":[]}]}