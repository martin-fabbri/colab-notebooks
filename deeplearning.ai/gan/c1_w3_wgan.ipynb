{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "c1_w3_wgan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvQbiDitdtjDiCDGwDLjMO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/gan/c1_w3_wgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLXnaX0HjdGm"
      },
      "source": [
        "# Wasserstein GAN with Gradient Penalty (WGAN-GP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac9JCGC3DqXu"
      },
      "source": [
        "## Motivation\n",
        "\n",
        "Training GANs is hard. Models may never converge and mode collapses are common. To move forward, we need to make incremental improvements or embrace optimal cost functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1SqQDbm-3Q-"
      },
      "source": [
        "### WGAN-GP improvements\n",
        "\n",
        "WGAN-GP enhances training stability. As shown below, when the model design is less optimal, WGAN-GP can still create good results while the original GAN cost function fails.\n",
        "\n",
        "<img src=\"https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/gan/images/wgan-gp-1.png\" width=600>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tumH9yRkBQ89"
      },
      "source": [
        "Below is the inception score using different methods. The experiment from WGAN-GP paper demostrates better image quality and convergence comparing with WGAN. However, DCGAN demostrates slightly better image quality and it converges faster. But the inception score for WGAN-GP is more stable when it starts converting.\n",
        "\n",
        "<img src=\"https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/gan/images/wgan-gp-2.png\" width=500>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwVqXBntCL5D"
      },
      "source": [
        "> ### So what is the benefit of WGAN-GP if it cannot beat DCGAN?\n",
        "> - `The major advantage of WGAN-GP is its convergency`. It makes training more stable and therefore easier to train. As WGAN-GP helps model to converge better, we can use a more complex model lie deep ResNet for generation and the discriminator.  \n",
        "> - When using the Wasserstein loss function, we should train the critic to `converge to ensure that the gradients for the generator are accurate`. This is `in contrast to a standard GAN`, where it is important `not to let the discrimininator get too strong, to avoid vanishing gradients`. Helping to reduce one of the main difficulties of training GANs - How to balance the training of the discriminator and generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn25o_qOFEOj"
      },
      "source": [
        "## Wasserstein Distance\n",
        "\n",
        "Wasserstein GAN (WGAN) proposes a new cost function using Wasserstein distance that has a smoother gradient everyware. WGAN learns regarless of the generator performance. The diagram below repeats a similar plot on the value $D(X)$ fro both GAN and WGAN. For GAN (the red line), it fills with areas with diminishing or exploiding gradient. For WANG (the blue line), the gradient is smoother everywhere and learns better even when the generator is not producing good images.\n",
        "\n",
        "<img src=\"https://github.com/martin-fabbri/colab-notebooks/raw/master/deeplearning.ai/gan/images/wgan-4.png\" width=300>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mdcDjoEqEry"
      },
      "source": [
        "GAN `discriminator loss` minimization $$min_D−(\\mathbb{E}_{x∼pX}[log\\,D(x)] + \\mathbb{E}_{z∼pZ}[log\\,(1−D(G(z)))])$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1-O788J36SB"
      },
      "source": [
        "To train the GAN `generator` G, we calculate the loss when comparing predictions for generated images $p_i=D(G(zi))$ to the response y_i=1. Therefore for the GAN generator, minimizing the loss function can be written as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVhlSKwZ5OLs"
      },
      "source": [
        "GAN `generator loss` minimization:\n",
        "$$min_G−(\\mathbb{E}_{z∼pZ}[log\\,D(G(z))])$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0oNosCjpA9L"
      },
      "source": [
        "### Wasserstein loss function\n",
        "\n",
        "First, the Wasserstein loss requires that we use $y_i=1$ and $y_i=-1$ as labels, rather than 1 and 0. We also remove the sigmoid activation from the final layer of the **discriminator(critic)**, so that predictions $p_i$ are no longer constrained to fall in the range $[0,1]$, but instead can now be any number in the range $[–∞, ∞]$. For this reason, the discriminator in a WGAN is usually referred to as a critic.The Wasserstein loss function is then defined as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zjnNxZt6EEx"
      },
      "source": [
        "Wasserstein loss:\n",
        "$$-\\frac{1}{n}\\sum_{i=1}^{n}{(y_i\\,p_i)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKAwr_EQ8XI6"
      },
      "source": [
        "To train the WGAN critic $D$, we calculate the loss when comparing predictions for a real images $p_i=D(x_i)$ to the response $y_i=1$ and predictions for generated images $p_i=D(G(z_i))$ to the response $y_i=-1$. Therefore for the WGAN critic, minimizing the loss function can be written as follows:\n",
        "\n",
        "WGAN critic loss minimization:\n",
        "\n",
        "$$min_D−(\\mathbb{E}_{x∼pX}[D(x)]−\\mathbb{E}_{z∼pZ}[D(G(z))])$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woV06Ldn-SlJ"
      },
      "source": [
        "### The Lipschitz Constraint\n",
        "\n",
        "It may surprise you that we are now allowing the critic to output any number in the range $[–∞, ∞]$, rather than applying a sigmoid function to restrict the output to the usual $[0, 1]$ range. The Wasserstein loss can therefore be very large, which is unsettling—usually, large numbers in neural networks are to be avoided!\n",
        "\n",
        "In fact, the authors of the WGAN paper show that for the Wasserstein loss function to work, we also need to place an additional constraint on the critic. Specifically, it is required that the critic is a `1-Lipschitz continuous` function. Let’s pick this apart to understand what it means in more detail.\n",
        "\n",
        "The critic is a function $D$ that converts an image into a prediction. We say that this function is 1-Lipschitz if it satisfies the following inequality for any two input images x1 and x2:\n",
        "\n",
        "$$\\frac{|D(x_1)−D(x_2)|}{|x_1−x_2|}\\leq1$$\n",
        "\n",
        "Here, $x_1 – x_2$ is the average pixelwise absolute difference between two images and $|D(x_1)−D(x_2)||D(x_1)-D(x_2)|$ is the absolute difference between the critic predictions. Essentially, we require a limit on the rate at which the predictions of the critic can change between two images (i.e., the absolute value of the gradient must be at most 1 everywhere)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfgS9e4mIiIu"
      },
      "source": [
        "## Key differences between GANs and WGANs\n",
        "\n",
        "- A WGAN uses the Wasserstein loss.\n",
        "\n",
        "- The WGAN is trained using labels of `1 for real` and `–1 for fake`.\n",
        "\n",
        "- There is `no need for the sigmoid activation` in the final layer of the WGAN critic.\n",
        "\n",
        "- `Clip the weights` of the critic after each update.\n",
        "\n",
        "- Train the critic multiple times for each update of the generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-YXTGG3jZTa"
      },
      "source": [
        "### Goals\n",
        "In this notebook, you're going to build a Wasserstein GAN with Gradient Penalty (WGAN-GP) that solves some of the stability issues with the GANs that you have been using up until this point. Specifically, you'll use a special kind of loss function known as the W-loss, where W stands for Wasserstein, and gradient penalties to prevent mode collapse.\n",
        "\n",
        "*Fun Fact: Wasserstein is named after a mathematician at Penn State, Leonid Vaseršteĭn. You'll see it abbreviated to W (e.g. WGAN, W-loss, W-distance).*\n",
        "\n",
        "### Learning Objectives\n",
        "1.   Get hands-on experience building a more stable GAN: Wasserstein GAN with Gradient Penalty (WGAN-GP).\n",
        "2.   Train the more advanced WGAN-GP model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRF5GktQje09"
      },
      "source": [
        "## Generator and Critic\n",
        "\n",
        "You will begin by importing some useful packages, defining visualization functions, building the generator, and building the critic. Since the changes for WGAN-GP are done to the loss function during training, you can simply reuse your previous GAN code for the generator and critic class. Remember that in WGAN-GP, you no longer use a discriminator that classifies fake and real as 0 and 1 but rather a critic that scores images with real numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJvvIvaWjLNu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}