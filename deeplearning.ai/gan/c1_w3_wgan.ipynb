{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "c1_w3_wgan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhzDwBolVUzEfEWSW2+Gi+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/deeplearning.ai/gan/c1_w3_wgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLXnaX0HjdGm"
      },
      "source": [
        "# Wasserstein GAN with Gradient Penalty (WGAN-GP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-YXTGG3jZTa"
      },
      "source": [
        "### Goals\n",
        "In this notebook, you're going to build a Wasserstein GAN with Gradient Penalty (WGAN-GP) that solves some of the stability issues with the GANs that you have been using up until this point. Specifically, you'll use a special kind of loss function known as the W-loss, where W stands for Wasserstein, and gradient penalties to prevent mode collapse.\n",
        "\n",
        "*Fun Fact: Wasserstein is named after a mathematician at Penn State, Leonid Vaseršteĭn. You'll see it abbreviated to W (e.g. WGAN, W-loss, W-distance).*\n",
        "\n",
        "### Learning Objectives\n",
        "1.   Get hands-on experience building a more stable GAN: Wasserstein GAN with Gradient Penalty (WGAN-GP).\n",
        "2.   Train the more advanced WGAN-GP model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0oNosCjpA9L"
      },
      "source": [
        "### Wasserstein loss function\n",
        "\n",
        "First, the Wasserstein loss requires that we use $y_i=1$ and $y_i=-1$ as labels, rather than 1 and 0. We also remove the sigmoid activation from the final layer of the **discriminator(critic)**, so that predictions $p_i$ are no longer constrained to fall in the range $[0,1]$, but instead can now be any number in the range $[–∞, ∞]$. For this reason, the discriminator in a WGAN is usually referred to as a critic.The Wasserstein loss function is then defined as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mdcDjoEqEry"
      },
      "source": [
        "GAN `discriminator loss` minimization $$min_D−(\\mathbb{E}_{x∼pX}[log\\,D(x)] + \\mathbb{E}_{z∼pZ}[log\\,(1−D(G(z)))])$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1-O788J36SB"
      },
      "source": [
        "To train the GAN `generator` G, we calculate the loss when comparing predictions for generated images $p_i=D(G(zi))$ to the response yi=1. Therefore for the GAN generator, minimizing the loss function can be written as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRF5GktQje09"
      },
      "source": [
        "## Generator and Critic\n",
        "\n",
        "You will begin by importing some useful packages, defining visualization functions, building the generator, and building the critic. Since the changes for WGAN-GP are done to the loss function during training, you can simply reuse your previous GAN code for the generator and critic class. Remember that in WGAN-GP, you no longer use a discriminator that classifies fake and real as 0 and 1 but rather a critic that scores images with real numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJvvIvaWjLNu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}