{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "data_acquisition_preprocessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-fabbri/colab-notebooks/blob/master/satellite_imagery/data_acquisition_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J2O_ugj1awX"
      },
      "source": [
        "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
        "\n",
        "<tr style=\"vertical-align: top; padding: 0; margin: 0;background-color: #ffffff\">\n",
        "        <td style=\"vertical-align: top; padding: 0; margin: 0; padding-right: 15px;\">\n",
        "    <p style=\"background: #182AEB; color:#ffffff; text-align:justify; padding: 10px 25px;\">\n",
        "        <strong style=\"font-size: 1.0em;\"><span style=\"font-size: 1.2em;\"><span style=\"color: #ffffff;\">Deep Learning </span> for Satellite Image Classification (Manning Publications)</span><br/>by <em>Daniel Buscombe</em> </strong><br/><br/>\n",
        "        <strong>> Chapter 2: Data Acquisition and Pre-Processing </strong><br/>\n",
        "    </p>           \n",
        "        \n",
        "<p style=\"border: 1px solid #ff5733; border-left: 15px solid #ff5733; padding: 10px; text-align:justify;\">\n",
        "    <strong style=\"color: #ff5733\">What you will learn in this Part.</strong>  \n",
        "    <br/>In this Part you will learn a workflow to generate datasets that you will use to eventually train and test your semantic segmentation model to detect water pixels in images of lakes. You will be training your models using the lakes category in the NWPU-RESISC45 dataset, and applying them to Sentinel-2 imagery of some of Earth's critical lakes. Like in many situations in industry, you will make your own label imagery (but you'll get a head-start with some labels already prepared for you). You will also learn how to augment image data sets for improving model training. <br/><br/>Optionally, you will be presented a workflow for automatically downloading imagery programmatically using the sentinelsat package, filering them by type, date, tile, and finally merging adjacent tiles to generate time-series of imagery over whole lakes. Following this workflow is strongly recommended for those learning how to work with satellite and other geospatial data, and also those who want to challenge themselves further by using a customized dataset over any lake on Earth.\n",
        "    </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5-sp-hW1awZ"
      },
      "source": [
        "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
        "\n",
        "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
        "    </small><br/>Data set 1: a time-series <br/> of cloudless sentinel-2 imagery</h1>\n",
        "<br/>\n",
        "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
        "    Use the <a href=\"https://s2maps.eu/\">Sentinel-2 cloudless</a> image service (\"option 1\") as a means to easily acquire cloudless mosaiced imagery from some of Earth's critical lakes that collectively provide drinking water to countless millions of people. You can use the website to explore and compare changes visually between 2016 and 2018 everywhere, and also 2017 for Europe only. This is the data you will be attempting to test your model (trained on another dataset that is introduced later).\n",
        "</p>\n",
        "<p style=\"border-left: 15px solid #6019D6; padding: 0 10px; text-align:justify;\">\n",
        "    <strong style=\"color: #6019D6;\">Tip.</strong> \n",
        "If you use the download tool to obtain an image tile, you will see some structure in the URL. For example, \n",
        "<br/>\n",
        "<a href=\"https://tiles.maps.eox.at/wms?service=wms&request=getmap&version=1.1.1&layers=s2cloudless-2018&bbox=10.232263646327286,-0.670557747166987,10.334917149745255,-0.5679042437490183&width=4096&height=4096&srs=epsg:4326\">https://tiles.maps.eox.at/wms?service=wms&request=getmap&version=1.1.1&layers=s2cloudless-2018&bbox=10.232263646327286,-0.670557747166987,10.334917149745255,-0.5679042437490183&width=4096&height=4096&srs=epsg:4326</a>\n",
        "<br/><br/>\n",
        "    Let's break this down a little:\n",
        "<ul>\n",
        "    <li><b>https://tiles.maps.eox.at/wms?service=wms&request=getmap&version=1.1.1</b>: The root of the service that always stays the same. WMS stands for <a href=\"https://en.wikipedia.org/wiki/Web_Map_Service\">Web Map Service</a>.</li>    \n",
        "    <li><b>layers=s2cloudless2018</b>: The layer to get. Other options are <em>\"s2cloudless\"</em> for 2016 imagery, and <em>\"s2cloudless2017\"</em> for 2017 imagery (Europe only).</li>\n",
        "  <li><b>bbox=10.232263646327286,-0.670557747166987,10.334917149745255,-0.5679042437490183</b>: the bounding box coordinates (region of interest) in the form south Latitude, west Longitude, north Latitude, east Longitude.</li>\n",
        "  <li><b>width=4096&height=4096</b>: the width and height of the image in number of pixels</li>  \n",
        "    <li><b>srs=epsg:4326</b>: the <a href=\"http://www.epsg-registry.org/\">epsg</a> code (coordinate projection system) that pertains to the bounding box coordinates. In this example the epsg code is <a href=\"https://epsg.io/4326\">4326</a>, which is GPS coordinates (WGS 84).</li>  \n",
        "</ul>  \n",
        "</p>\n",
        "        </tr>\n",
        "        </table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QJcX7Z31awb"
      },
      "source": [
        "##### Building a web scraping script to download imagery from internet servers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d6xYVNq1awb"
      },
      "source": [
        "Now we understand the URL, we can use a program to download image time-series at a number of places programmatically. This is an example of <a href=\"https://en.wikipedia.org/wiki/Web_scraping\">web scraping</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETlNlzzg1awc"
      },
      "source": [
        "# define all the parameters\n",
        "root = \"https://tiles.maps.eox.at/wms?service=wms&request=getmap&version=1.1.1&\"\n",
        "layers = 'layers=s2cloudless-2018&'\n",
        "bbox = 'bbox=10.232263646327286,-0.670557747166987,10.334917149745255,-0.5679042437490183&'\n",
        "width = 'width=4096&'\n",
        "height = 'height=4096&'\n",
        "srs = 'srs=epsg:4326'\n",
        "\n",
        "# define the output\n",
        "destination = 'example_image.jpg'\n",
        "\n",
        "#construct and print the url\n",
        "url = root+layers+bbox+width+height+srs\n",
        "print(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC4rDU091awd"
      },
      "source": [
        "Use the built-in package <a href=\"https://docs.python.org/3/library/urllib.request.html#module-urllib.request\">urllib.request</a> to download the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlvP5Vhi1awd"
      },
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve(url, destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qgYSBZu1awe"
      },
      "source": [
        "To remove the image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsbarbIg1awe"
      },
      "source": [
        "import os\n",
        "os.remove('example_image.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lklnuFgh1awf"
      },
      "source": [
        "Download time-series of images from 17 critically endangered lake systems:\n",
        "1. <a href=\"https://en.wikipedia.org/wiki/Lake_Poop%C3%B3\">Lake Poopo, Bolivia</a>\n",
        "\n",
        "2. <a href=\"https://en.wikipedia.org/wiki/Lake_Urmia\">Lake Urmia, Iran</a> \n",
        " \n",
        "3. <a href=\"https://en.wikipedia.org/wiki/Lake_Mead\">Lake Mead</a>/<a href=\"https://en.wikipedia.org/wiki/Lake_Mohave\">Lake Mojave</a>, USA\n",
        "\n",
        "4. <a href=\"https://en.wikipedia.org/wiki/Aral_Sea\">Aral sea, Kazahkstan</a>\n",
        "\n",
        "5. <a href=\"https://en.wikipedia.org/wiki/Lake_Copais\">Lake Copais, Greece</a>\n",
        "\n",
        "6. <a href=\"https://en.wikipedia.org/wiki/Ramganga_Dam\">Lake Ramganga, India</a>\n",
        " \n",
        "7. <a href=\"https://en.wikipedia.org/wiki/Qinghai_Lake\">Qinghai Lake, China</a> \n",
        "\n",
        "8. <a href=\"https://en.wikipedia.org/wiki/Salton_Sea\">Salton Sea, USA</a> \n",
        "\n",
        "9. <a href=\"https://earthobservatory.nasa.gov/images/8991/drying-of-lake-faguibine-mali\">Lake Faguibine, Mali</a> \n",
        " \n",
        "10. <a href=\"https://en.wikipedia.org/wiki/Mono_Lake\">Mono Lake, USA</a> \n",
        "\n",
        "11. <a href=\"https://en.wikipedia.org/wiki/Walker_Lake_(Nevada)\">Walker Lake, USA</a>\n",
        "\n",
        "12. <a href=\"https://en.wikipedia.org/wiki/Lake_Balaton\">Lake Balaton, Hungary</a>\n",
        "\n",
        "13. <a href=\"https://en.wikipedia.org/wiki/Lake_Koroneia\">Lake Koroneia, Greece</a>\n",
        "\n",
        "14. <a href=\"https://en.wikipedia.org/wiki/Lake_Salda\">Lake Salda, Turkey</a>\n",
        "\n",
        "15. <a href=\"https://en.wikipedia.org/wiki/Lake_Burdur\">Lake Burdur, Turkey</a>\n",
        "\n",
        "16. <a href=\"https://en.wikipedia.org/wiki/Lake_Mendocino\">Lake Mendocino, USA</a>\n",
        "\n",
        "17. <a href=\"https://en.wikipedia.org/wiki/Elephant_Butte_Reservoir\">Elephant Butte Reservoir, USA</a>\n",
        "\n",
        "You can read about some of these sites and their problems in a <a href=\"https://www.nationalgeographic.com/magazine/2018/03/drying-lakes-climate-change-global-warming-drought/\">2018 National Geographic article</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n31KxLm1awg"
      },
      "source": [
        "#Lake Poopo, Bolivia\n",
        "site='poopo'\n",
        "bbox = 'bbox=-68.66848367000537,-19.687928531849003,-66.67924128546656,-17.8774477409051&'\n",
        "width = 'width=4096&'\n",
        "height = 'height=4096&'\n",
        "layers = ['layers=s2cloudless&', 'layers=s2cloudless-2018&']\n",
        "srs = 'srs=epsg:4326'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYDJzvd01awg"
      },
      "source": [
        "The following function can be used to download all layers given a list of layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_P-T3Po1awh"
      },
      "source": [
        "def download_all_layers(root,layers,bbox,width,height,srs,site):\n",
        "    #cycle through each layer to download\n",
        "    for layer in layers:\n",
        "        # construct a URL string \n",
        "        url = root+layer+bbox+width+height+srs\n",
        "        #download into a filename constructed as \"site_layer.jpg\"\n",
        "        urllib.request.urlretrieve(url, site+'_'+layer.split('=')[-1].split('&')[0]+'.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNVlLRTq1awh"
      },
      "source": [
        "If the above function doesn't function correctly, you might find the alternative solution below useful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUydY3Xl1awi"
      },
      "source": [
        "# import requests, shutil\n",
        "# def download_all_layers(root,layers,bbox,width,height,srs,site):\n",
        "#     #cycle through each layer to download\n",
        "#     for layer in layers:\n",
        "#         # construct a URL string \n",
        "#         url = root+layer+bbox+width+height+srs\n",
        "#         #download into a filename constructed as \"site_layer.jpg\"\n",
        "#         r=requests.get(url, stream=True)\n",
        "#         if r.status_code == 200:\n",
        "#             with open(site+'_'+layer.split('=')[-1].split('&')[0]+'.jpg', 'wb') as f:\n",
        "#                 r.raw.decode_content = True\n",
        "#                 shutil.copyfileobj(r.raw, f)    \n",
        "#                 del r             "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq16XX_01awi"
      },
      "source": [
        "Note that if the file doesn’t download due to an error, it is likely because of an unstable internet connection, you will need to run it again if it fails"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zctegm3C1awj"
      },
      "source": [
        "#Use the function to download all layers at Lake Poopo\n",
        "download_all_layers(root,layers,bbox,width,height,srs,site)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJc9QGtA1awj"
      },
      "source": [
        "Now you can download imagery at the other 16 sites in turn. This will download 40 images and a total of 104 MB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYwXD5JQ1awj"
      },
      "source": [
        "#Lake Urmia, Iran\n",
        "bbox = 'bbox=44.587725529095295,36.86436828406643,46.230181583782795,38.50682433875393&'\n",
        "layers = ['layers=s2cloudless&', 'layers=s2cloudless-2017&', 'layers=s2cloudless-2018&']\n",
        "download_all_layers(root,layers,bbox,width,height,srs,'urmia')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAeVq-Xc1awk"
      },
      "source": [
        "#Lake Mead/ Lake Mojave, USA\n",
        "bbox = 'bbox=-115.42507235769445,34.94273489926993,-113.78261630300695,36.58519095395743&'\n",
        "layers = ['layers=s2cloudless&','layers=s2cloudless-2018&']\n",
        "download_all_layers(root,layers,bbox,width,height,srs,'mead_mojave')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUMrYBtz1awk"
      },
      "source": [
        "#Aral Sea, Kazahkstan\n",
        "bbox = 'bbox=58.032853536637845,43.955292007325,61.317765646012845,47.2402041167&'\n",
        "download_all_layers(root,layers,bbox,width,height,srs,'aral_sea')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuaQxbrr1awk"
      },
      "source": [
        "#Copais Lake, Greece\n",
        "bbox = 'bbox=23.214268551013436,38.36067354565393,23.350911007068124,38.4633270490719&'\n",
        "layers = ['layers=s2cloudless&', 'layers=s2cloudless-2017&', 'layers=s2cloudless-2018&']\n",
        "download_all_layers(root,layers,bbox,width,height,srs,'copais')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unfEErwu1awl"
      },
      "source": [
        "#Ramganga Lake, India\n",
        "bbox = 'bbox=78.63716910452058,29.468167756293038,78.91045401662996,29.673474763128976&'\n",
        "layers = ['layers=s2cloudless&','layers=s2cloudless-2018&']\n",
        "download_all_layers(root,layers,bbox,width,height,srs,'ramganga')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKSsZDG61awl"
      },
      "source": [
        "#Qinghai Lake, China \n",
        "bbox = 'bbox=99.56471967977474,35.70094613663666,101.20168257039974,37.34340219132416&'\n",
        "download_all_layers(root,layers,bbox,width,height,srs,'qinghai')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTE6GwUX1awl"
      },
      "source": [
        "#Salton Sea, USA \n",
        "bbox = 'bbox=-116.21928854749297,32.903320741965295,-115.40080710218047,33.724548769309045&'\n",
        "download_all_layers(root,layers,bbox,width,height,srs,'salton_sea')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zA1_9yl1awm"
      },
      "source": [
        "# Lake Faguibine, Mali\n",
        "bbox = 'bbox=-4.656089323623854,14.959306189862815,-3.0191264329988536,16.601762244550315&'\n",
        "width = 'width=4082&'\n",
        "download_all_layers(root,layers,bbox,width,height,srs,'faguibine')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7MMhVmn1awm"
      },
      "source": [
        "# Mono Lake, USA\n",
        "bbox = 'bbox=-119.22357779576633,37.797192162208084,-118.81433707311008,38.20780617587996&'\n",
        "download_all_layers(root,layers,bbox,width,height,srs,'mono')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wshFI6d21awm"
      },
      "source": [
        "# Walker Lake, USA\n",
        "bbox = 'bbox=-118.82841344222616,38.58649115903216,-118.62379308089804,38.791798165868094&'\n",
        "download_all_layers(root,layers,bbox,width,height,srs,'walker')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPYrMt5_1awn"
      },
      "source": [
        "# Lake Balaton, Hungary\n",
        "width = 'width=4096&'\n",
        "height = 'height=3084&'\n",
        "layers = ['layers=s2cloudless&', 'layers=s2cloudless-2017&', 'layers=s2cloudless-2018&']\n",
        "bbox = 'bbox=17.138306471163226,46.438781725159295,18.231446119600726,47.260009752503045&'\n",
        "download_all_layers(root,layers,bbox,width,height,srs,'balaton')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbmuMc431awn"
      },
      "source": [
        "# Lake Koroneia, Greece\n",
        "bbox = 'bbox=23.34167375183668,40.56321556296156,23.614958663946055,40.7685225697975&'\n",
        "download_all_layers(root,layers,bbox,width,height,srs,'koroneia')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKq4uwCW1awn"
      },
      "source": [
        "# Lake Salda, Turkey\n",
        "width = 'width=4082&'\n",
        "height = 'height=4096&'\n",
        "bbox = 'bbox=29.6337346568769,37.49763563391751,29.736044837540963,37.60028913733548&'\n",
        "download_all_layers(root,layers,bbox,width,height,srs,'salda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WQXAXg_1awo"
      },
      "source": [
        "# Lake Burdur, Turkey\n",
        "width = 'width=4096&'\n",
        "height = 'height=2312&'\n",
        "bbox = 'bbox=30.008970333642345,37.6345355820376,30.373579098290783,37.83984258887354&'\n",
        "download_all_layers(root,layers,bbox,width,height,srs,'burdur')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFUFDDmt1awo"
      },
      "source": [
        "# Lake Mendocino, USA\n",
        "width = 'width=4082&'\n",
        "height = 'height=4096&'\n",
        "bbox = 'bbox=-123.19613698824355,39.189371563341254,-123.14498189791152,39.24069831505024&'\n",
        "layers = ['layers=s2cloudless&','layers=s2cloudless-2018&']\n",
        "download_all_layers(root,layers,bbox,width,height,srs,'mendocino')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zcxkIVx1awp"
      },
      "source": [
        "# Elephant Butte Reservoir, USA\n",
        "bbox = 'bbox=-107.23803910498334,33.14614806405013,-107.13572892431928,33.2488015674681&'\n",
        "download_all_layers(root,layers,bbox,width,height,srs,'elephant_butte')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krnolhYy1awp"
      },
      "source": [
        "##### Organizing the downloaded files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivtPpQU61awp"
      },
      "source": [
        "Now you have downloaded imagery from every site, use the ```glob``` and ```shutil``` libraries to clean up. Change the name of the 2016 imagery so it has the ```-2016``` string in the filename.\n",
        "\n",
        "It will become clear why we put the imagery into the subfolder ```data``` in Part 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70_q5Qeq1awp"
      },
      "source": [
        "## create a directory to move the images into. It is wrapped in a \"try:except\" loop \n",
        "## in case you have run this cell before and want to avoid errors\n",
        "try:\n",
        "    os.mkdir('s2cloudless_imagery')\n",
        "    os.mkdir('s2cloudless_imagery'+os.sep+'data')\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1zlSr1A1awq"
      },
      "source": [
        "import shutil, glob, os\n",
        "# cycle through each jpg image in the current directory\n",
        "try:\n",
        "    for f in glob.glob('*.jpg'):\n",
        "        #move to the new directory\n",
        "        if f.endswith('s2cloudless.jpg'): #2016 imagery\n",
        "            shutil.move(f, 's2cloudless_imagery'+os.sep+'data'+os.sep+f.replace('s2cloudless.jpg','s2cloudless-2016.jpg'))\n",
        "        else: #2017 or 2018 imagery\n",
        "            shutil.move(f, 's2cloudless_imagery'+os.sep+'data')\n",
        "except:\n",
        "    pass        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoORau581awq"
      },
      "source": [
        "##### Reading and displaying the files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EMz_QWH1awq"
      },
      "source": [
        "Let's take a look at the images. First read the images into a list organized by site"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A70VGNB1awr"
      },
      "source": [
        "# get a sorted list of all jpg files\n",
        "files = sorted(glob.glob('s2cloudless_imagery'+os.sep+'data'+os.sep+'*.jpg'))\n",
        "# define a list of sites\n",
        "sites = ['aral_sea','balaton','burdur','copais','elephant_butte',\n",
        "         'faguibine','koroneia','mead_mojave','mendocino','mono',\n",
        "         'poopo','qinghai','ramganga','salda','urmia','walker']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDd7uZS-1awr"
      },
      "source": [
        "#allocate F which will contain a list of file name lists\n",
        "F = []; F_arrays = []\n",
        "#cycle through each of the 17 sites\n",
        "for site in sites:\n",
        "    # append those files at each site\n",
        "    F.append([f for f in files if f.startswith('s2cloudless_imagery'+os.sep+'data'+os.sep+site)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5zLywST1awr"
      },
      "source": [
        "Use rasterio to import the image data into a new list of image arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHMIqNFw1aws"
      },
      "source": [
        "import rasterio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2BAHsxs1aws"
      },
      "source": [
        "F_arrays = [] #initialize empty list\n",
        "for f in F: #cycle through sites\n",
        "    tmp = [] #make a temporary empty list\n",
        "    for i in f: #cycle through files at each site\n",
        "        with rasterio.open(i) as dataset: #open the file\n",
        "            tmp.append(dataset.read().T) #read in and transpose channels\n",
        "    F_arrays.append(tmp) #add to F_arrays"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugOqrwo_1aws"
      },
      "source": [
        "Plot the first and last images at each site, side by side"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhp8q5Z51aws"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JszYhba1awt"
      },
      "source": [
        "Show the first 5 sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqyfDYNm1awt"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# visualize some of the examples\n",
        "fig = plt.figure(figsize=(8,24))\n",
        "for i in range(1,10,2):\n",
        "    plt.subplot(5,2,i)\n",
        "    plt.imshow(F_arrays[i-1][0])\n",
        "    plt.title(F[i-1][0].split(os.sep)[-1])\n",
        "    plt.axis('off')\n",
        "    plt.subplot(5,2,i+1)\n",
        "    plt.imshow(F_arrays[i-1][-1])\n",
        "    plt.title(F[i-1][-1].split(os.sep)[-1])\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuSioUEH1awt"
      },
      "source": [
        "Plot the next 5 sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB9uwdt-1awt"
      },
      "source": [
        "F_arrays = F_arrays[5:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XC3of0Z1awu"
      },
      "source": [
        "fig = plt.figure(figsize=(8,24))\n",
        "for i in range(1,10,2):\n",
        "    plt.subplot(5,2,i)\n",
        "    plt.imshow(F_arrays[i-1][0])\n",
        "    plt.title(F[i-1][0].split(os.sep)[-1])\n",
        "    plt.axis('off')\n",
        "    plt.subplot(5,2,i+1)\n",
        "    plt.imshow(F_arrays[i-1][-1])\n",
        "    plt.title(F[i-1][-1].split(os.sep)[-1])\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOH7TNBF1awu"
      },
      "source": [
        "As you can see, there is a considerable amount of variability in color, hue, and texture in water and surrounding land pixels. \n",
        "\n",
        "> Going further: how many other lakes can you find that show a noticeable change between 2016 and 2018?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUVx72kG1awu"
      },
      "source": [
        "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
        "\n",
        "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
        "    </small><br/>Data set 2: NWPU-RESISC45 <br/> dataset of land covers and uses</h1>\n",
        "<br/>\n",
        "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
        "The Northwestern Polytechnical University (NWPU) REmote Sensing Image Scene Classification (RESISC) project has created <a href=\"http://www.drweiscience.com/people/JunweiHan/NWPU-RESISC45.html\">NWPU-RESISC45</a>, a publicly available benchmark dataset containing 31,500 high-resolution images from Google Earth imagery, in 45 scene classes with 700 images in each class. \n",
        "</p>\n",
        "<p style=\"border-left: 15px solid #6019D6; padding: 0 10px; text-align:justify;\">\n",
        "    <strong style=\"color: #6019D6;\">Tip.</strong> \n",
        "    The dataset is described in <a href=\"https://arxiv.org/abs/1703.00121\">this paper</a> and is also available as a tensorflow <a href=\"https://www.tensorflow.org/datasets/catalog/resisc45\">built-in dataset</a>\n",
        "</p>\n",
        "        </tr>\n",
        "        </table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsMNGovI1awu"
      },
      "source": [
        "##### Download the dataset from google drive. \n",
        "\n",
        "Warning, this will download 415 MB. We saw this function for downloading a file in the previous Part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sywIAHA1awv"
      },
      "source": [
        "# from https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    \"\"\"\n",
        "    response = filename for input\n",
        "    destination = filename for output\n",
        "    \"\"\"    \n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn7iUR1G1awv"
      },
      "source": [
        "Download the google drive file into a zipped folder on your computer called ```NWPU_images.zip```. This should be 405 MB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z3tEVFY1awv"
      },
      "source": [
        "file_id = '14kkcuU6wd9UMvjaDrg3PNI-e_voCi8HL'\n",
        "destination = 'NWPU_images.zip'\n",
        "download_file_from_google_drive(file_id, destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjWFtKJN1aww"
      },
      "source": [
        "#### Using system commands to work with the files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2t25FvU1aww"
      },
      "source": [
        "Unzip the folder (this may take a few minutes) as a new folder called ```images```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fxx4ygM1aww"
      },
      "source": [
        "import zipfile\n",
        "def unzip_nwpu(f):\n",
        "    \"\"\"\n",
        "    f = file to be unzipped\n",
        "    \"\"\"    \n",
        "    with zipfile.ZipFile(f, 'r') as zip_ref:\n",
        "        zip_ref.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PC2jUNi1aww"
      },
      "source": [
        "unzip_nwpu(destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR1yUnCP1awx"
      },
      "source": [
        "Load file sysem utilities for moving and deleting files (```os``` and ```shutil```)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yK-eEof1awx"
      },
      "source": [
        "import shutil, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmaYV_YP1awx"
      },
      "source": [
        "Rename the ```images``` directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUu3pIR31awx"
      },
      "source": [
        "try:\n",
        "    os.rename('images','nwpu_images')\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqfqzsOu1awy"
      },
      "source": [
        "Remove non-lake directories that we won't need. First find all subdirectories (except the first, which is the parent directory)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu99U5aN1awy"
      },
      "source": [
        "subdirecs = [x[0] for x in os.walk('nwpu_images')][1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1GUoTit1awy"
      },
      "source": [
        "then get a list of all subdirectories that do not contain the word \"lake\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-Kixycs1awy"
      },
      "source": [
        "to_delete = [s for s in subdirecs if 'lake' not in s]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcyD4dqQ1awz"
      },
      "source": [
        "Use ```shutil.rmtree``` to delete the imagery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EwcD46O1awz"
      },
      "source": [
        "for k in to_delete:\n",
        "    shutil.rmtree(k, ignore_errors=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRqcGw7k1awz"
      },
      "source": [
        "Finally, rename the subdirectory, for consistency with dataset 1. It will become apparent why we use the ```data``` subdirectory in Part 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg9rxNe61awz"
      },
      "source": [
        "os.rename('nwpu_images'+os.sep+'lake','nwpu_images'+os.sep+'data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOfigaje1awz"
      },
      "source": [
        "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
        "\n",
        "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
        "    </small><br/>Creating your own  <br/> label data</h1>\n",
        "<br/>\n",
        "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
        "Label images can be time-consuming to create but creation of accurate pixelwise label images is necessary for any semantic segmentation project. The <a href+\"https://www.makesense.ai/\">https://www.makesense.ai/</a> utility is an efficient and relatively simple way to generate image labels (for various applications; not just semantic segmantation). A <a href=\"https://www.json.org/\">json</a> format file is provided that contains labels for each of the 40 sentinel-2 cloudless images created earlier in this Part, intended for testing the deep learning water classifier. \n",
        "</p>\n",
        "<p style=\"border-left: 15px solid #6019D6; padding: 0 10px; text-align:justify;\">\n",
        "    <strong style=\"color: #6019D6;\">Tip.</strong> \n",
        "You will be expected to create your own labels for the training data set (NWPU-RESISC45, and any other data sets you create by following the \"Going further\" stream), perhaps in groups for time-efficiency. The following describes workflows for reading, writing, editing and merging json format files created by using <a href+\"https://www.makesense.ai/\">https://www.makesense.ai/</a>. Select \"Get Started\" >> Load your images (best not to load too many at once) >> Select \"Object Detection\" >> create a label \"water\" >> Select \"start project then \"I'm going on my own\". Now you should see your uploaded images and the labeling tools. Remember to use the \"Polygon\" tool, and to \"export labels\" when you are done.\n",
        "</p>\n",
        "        </tr>\n",
        "        </table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQqA7EOR1aw0"
      },
      "source": [
        "##### Importing a json label file with one image annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmhtflfT1aw0"
      },
      "source": [
        "Import the json label file and extract the polygon coordinates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQPFjv1D1aw0"
      },
      "source": [
        "import json, os\n",
        "import numpy as np\n",
        "json_file = 's2cloudless_labels'+os.sep+'balaton_s2cloudless-2018.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi0NDm391aw1"
      },
      "source": [
        "The VGG json format is simply a text format for containing image annotation data in a human-readable and consistent way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZRhNjA31aw1"
      },
      "source": [
        "data = json.load(open(json_file))\n",
        "data = data['balaton_s2cloudless-2018.jpg']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttT0Uib41aw1"
      },
      "source": [
        "x = data['regions']['0']['shape_attributes']['all_points_x']\n",
        "y = data['regions']['0']['shape_attributes']['all_points_y']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt4u8CXn1aw1"
      },
      "source": [
        "Read the image, transpose its channels and print the shape to screen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "089TMIFu1aw1"
      },
      "source": [
        "import rasterio\n",
        "with rasterio.open('s2cloudless_imagery'+os.sep+'data'+os.sep+'balaton_s2cloudless-2018.jpg') as dataset:\n",
        "    im = dataset.read().T\n",
        "print(np.shape(im))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOZlwDBE1aw2"
      },
      "source": [
        "The `.T` transpose is required because the channels are ordered BGR wheras our visual cortex responds to RGB ordering, which we call the visible band spectrum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSYhyJ2P1aw2"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeH2BbIg1aw2"
      },
      "source": [
        "Plot the polygon over the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SQOKki91aw2"
      },
      "source": [
        "%matplotlib inline\n",
        "plt.imshow(im)\n",
        "plt.plot(y,x,'r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu_CPUAY1aw3"
      },
      "source": [
        "##### Loading json files with multiple polygons for one image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3bPNSPQ1aw3"
      },
      "source": [
        "data = json.load(open('s2cloudless_labels'+os.sep+'aral_sea_s2cloudless-2018.json'))\n",
        "data = data['aral_sea_s2cloudless-2018.jpg']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWttWGGB1aw3"
      },
      "source": [
        "x1 = data['regions']['0']['shape_attributes']['all_points_x']\n",
        "y1 = data['regions']['0']['shape_attributes']['all_points_y']\n",
        "\n",
        "x2 = data['regions']['1']['shape_attributes']['all_points_x']\n",
        "y2 = data['regions']['1']['shape_attributes']['all_points_y']\n",
        "\n",
        "x3 = data['regions']['2']['shape_attributes']['all_points_x']\n",
        "y3 = data['regions']['2']['shape_attributes']['all_points_y']\n",
        "\n",
        "x4 = data['regions']['3']['shape_attributes']['all_points_x']\n",
        "y4 = data['regions']['3']['shape_attributes']['all_points_y']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1gK0xsW1aw3"
      },
      "source": [
        "with rasterio.open('s2cloudless_imagery'+os.sep+'data'+os.sep+'aral_sea_s2cloudless-2018.jpg') as dataset:\n",
        "    im = dataset.read().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZdNMxw21aw4"
      },
      "source": [
        "%matplotlib inline\n",
        "plt.imshow(im)\n",
        "plt.plot(y1,x1,'r')\n",
        "plt.plot(y2,x2,'k')\n",
        "plt.plot(y3,x3,'b')\n",
        "plt.plot(y4,x4,'m')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nuq3KDON1aw4"
      },
      "source": [
        "##### Working with VGG-json format data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuVQPHW91aw4"
      },
      "source": [
        "Create a list containing the contents of two json files that you wish to merge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXAoEUb_1aw4"
      },
      "source": [
        "data = []\n",
        "data.append(json.load(open('s2cloudless_labels'+os.sep+'aral_sea_s2cloudless-2018.json')))\n",
        "data.append(json.load(open('s2cloudless_labels'+os.sep+'urmia_s2cloudless-2016.json')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StcuBKaq1aw5"
      },
      "source": [
        "Merge the list of dictionaries into a single dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29ezOgTE1aw5"
      },
      "source": [
        "data_merged = {}\n",
        "for d in data:\n",
        "    data_merged.update(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3vTVlHF1aw5"
      },
      "source": [
        "Verify by printing the image names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uVYzHha1aw5"
      },
      "source": [
        "images = sorted(data_merged.keys())\n",
        "print(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWNbUJab1aw6"
      },
      "source": [
        "Let's say the image names changed and you had to update the json information with the new image names. One potential solution is illustrated below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWFIAKBd1aw6"
      },
      "source": [
        "new_images = []\n",
        "for image in images:\n",
        "    if image.startswith('aral'):\n",
        "        image = image.replace('aral', 'Aral')\n",
        "        new_images.append(image)\n",
        "    elif image.startswith('balaton'):\n",
        "        image = image.replace('balaton', 'Site0001')\n",
        "        new_images.append(image)\n",
        "    else:\n",
        "        new_images.append(image)\n",
        "print(new_images)        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzhcGiuC1aw6"
      },
      "source": [
        "Then you could replace the old image names (keys) with the new ones "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__D29V5W1aw6"
      },
      "source": [
        "for old_key, new_key in zip(images, new_images):\n",
        "    data_merged[new_key] = data_merged.pop(old_key)\n",
        "print(data_merged.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqH29WTj1aw7"
      },
      "source": [
        "The following code writes the merged data back out to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2egVvw961aw7"
      },
      "source": [
        "with open('s2cloudless_labels'+os.sep+'merged.json','w') as f:\n",
        "    json.dump(data_merged, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiaIibqM1aw7"
      },
      "source": [
        "##### Loading json files with multiple polygons for several images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9yjqeJL1aw7"
      },
      "source": [
        "data = json.load(open('s2cloudless_labels'+os.sep+'all_labels.json'))\n",
        "images = sorted(data.keys())\n",
        "print(images)\n",
        "print(len(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lUVydXU1aw8"
      },
      "source": [
        "Load all images into memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0BS2PiC1aw8"
      },
      "source": [
        "all_images = []\n",
        "for image in images:\n",
        "    with rasterio.open('s2cloudless_imagery'+os.sep+'data'+os.sep+image) as dataset:\n",
        "        all_images.append(dataset.read().T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWjkY_5P1aw8"
      },
      "source": [
        "Create a function to show an image and make/show polygon vectors on top"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK7pUfs71aw8"
      },
      "source": [
        "def make_plot(data, images, i):\n",
        "    X = []; Y = [] #pre-allocate lists to fill in a for loop\n",
        "    for k in data[images[i]]['regions']: #cycle through each polygon\n",
        "        # get the x and y points from the dictionary\n",
        "        X.append(data[images[i]]['regions'][k]['shape_attributes']['all_points_x'])\n",
        "        Y.append(data[images[i]]['regions'][k]['shape_attributes']['all_points_y'])\n",
        "    #make a plot of the image, create a title (the image filename)    \n",
        "    plt.imshow(all_images[i])\n",
        "    plt.title(images[i])\n",
        "    # plot each polygon and turn the axes off\n",
        "    for k in range(len(X)):\n",
        "        plt.plot(Y[k],X[k])\n",
        "    plt.axis('off')\n",
        "    return Y,X #image coordinates are flipped relative to json coordinates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wI43aGH1aw9"
      },
      "source": [
        "Make a plot of the \"0th\" lake in the list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuVc1x-V1aw9"
      },
      "source": [
        "X,Y = make_plot(data, images, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb_-vO6x1aw9"
      },
      "source": [
        "Make a plot of the \"10th\" lake in the list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh4LZfr91aw-"
      },
      "source": [
        "X,Y = make_plot(data, images, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1zgPv5F1aw-"
      },
      "source": [
        "i = 11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V2inLPA1aw-"
      },
      "source": [
        "X,Y = make_plot(data, images, i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USzc1G7S1aw-"
      },
      "source": [
        "##### Create a label image or 'mask' from label polygons "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJl0BJLc1aw-"
      },
      "source": [
        "# import some drawing tools from PIL\n",
        "from PIL import Image, ImageDraw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ123PgZ1aw_"
      },
      "source": [
        "# get the dimensions of the image\n",
        "nx, ny, nz = np.shape(all_images[i])\n",
        "mask = np.zeros((ny,nx))\n",
        "    \n",
        "for x,y in zip(X,Y):\n",
        "    # the ImageDraw.Draw().polygon function we will use to create the mask\n",
        "    # requires the x's and y's are interweaved, which is what the following\n",
        "    # one-liner does    \n",
        "    polygon = np.vstack((x,y)).reshape((-1,),order='F').tolist()\n",
        "\n",
        "    # create a mask image of the right size and infill according to the polygon\n",
        "    if nx>ny:\n",
        "        x,y = y,x \n",
        "        img = Image.new('L', (ny, nx), 0)\n",
        "    #elif ny>nx:\n",
        "    #    x,y = y,x \n",
        "    #    img = Image.new('L', (ny, nx), 0)            \n",
        "    else:\n",
        "        img = Image.new('L', (nx, ny), 0)\n",
        "\n",
        "    ImageDraw.Draw(img).polygon(polygon, outline=1, fill=1)\n",
        "    # turn into a numpy array\n",
        "    m = np.array(img)\n",
        "    mask = mask + m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD8EwCM91aw_"
      },
      "source": [
        "Take a look!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2JAjsUq1aw_"
      },
      "source": [
        "plt.imshow(all_images[i])\n",
        "plt.title(images[i])\n",
        "plt.imshow(mask, cmap='bwr_r', alpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMTeO5Vw1axA"
      },
      "source": [
        "Matplotlib can be used to save the jpg image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjXSAzrt1axA"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.image.imsave(\"mask.jpg\", mask.astype('uint8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLSWdp_T1axA"
      },
      "source": [
        "Rasterio is what we have been using. Note that you need to rotate the transposed image so the label and image are in the same image coordinate system. Then taking only the zeroth channel with give you a 2D mask "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cSETMvt1axA"
      },
      "source": [
        "with rasterio.open('mask.jpg') as dataset:\n",
        "     mask = np.flipud(np.rot90(dataset.read().T))[:,:,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpkO4S4l1axB"
      },
      "source": [
        "Show the image and mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSERua9I1axB"
      },
      "source": [
        "plt.imshow(all_images[i])\n",
        "plt.title(images[i])\n",
        "plt.imshow(mask, cmap='gray', alpha=0.75)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds4om1st1axB"
      },
      "source": [
        "##### NWPU image labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTcFXshy1axB"
      },
      "source": [
        "To get you started on the NWPU lakes dataset for model training, three files are provided with labels are provided for a total of 70 lake images (10% of the total number). These files are ```nwpu_labels\\nwpu_lakes_30samples.json```, ```nwpu_labels\\nwpu_lakes_20samplesA.json```, and ```nwpu_labels\\nwpu_lakes_20samplesB.json```. You may use these as, respectively, train, test and validation sets, or alternatively in any way you choose. You are strongly recommended to create more of your own label images. Deep neural networks tend to work well when they have lots (and lots!) of examples to learn from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JqhLpfs1axC"
      },
      "source": [
        "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
        "<p style=\"border: 1px solid #ff5733; border-left: 15px solid #ff5733; padding: 10px; text-align:justify;\">\n",
        "    <strong style=\"color: #ff5733\">Deliverable</strong>  \n",
        "    <br/>The deliverable for Part 2 is a jupyter notebook showing a workflow to create test and training datasets, consisting of folders of imagery and corresponding label imagery, ready for training a semantic segmentation model in keras. This will mostly test your understanding the generic workflow for preparing a dataset to train and test a deep learning model, which is an essential component of the remaining Parts.\n",
        "    </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRd0WRMr1axC"
      },
      "source": [
        "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
        "\n",
        "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
        "    </small><br/> Going further: develop your own <br/> datasets using the sentinelsat package</h1>\n",
        "<br/>\n",
        "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
        "In this section, you will create and use a geoJSON format file to A) Download satellite imagery from a specific region; B) Plot a time-series of these images; C) Merge rasters with rasterio; and D) read the merged imagery in and display side by side \n",
        "</p>\n",
        "<p style=\"border-left: 15px solid #6019D6; padding: 0 10px; text-align:justify;\">\n",
        "    <strong style=\"color: #6019D6;\">Tip.</strong> \n",
        "This section follows from the workflow introduced in \"Going further:\n",
        "Introduction to satellite imagery\" section in Part_1_GettingStarted.ipynb\n",
        "</p>\n",
        "<p style=\"border-left: 15px solid #4E9317; padding: 0 10px; text-align:justify;\">\n",
        "    <strong style=\"color: #4E9317;\">More Resources.</strong> \n",
        "<ul>\n",
        "  <li><a href=\"https://krstn.eu/download-Sentinel-2-images/\">Another guide to using sentinselsat</a></li>    \n",
        "  <li><a href=\"https://github.com/binder-examples/getting-data/blob/master/Sentinel2.ipynb\">An alternative workflow </a> to that demonstrated here is to use <a href=\"https://pypi.org/project/rio-tiler/\">rio-tiler </a></li>\n",
        "</ul>\n",
        "        </tr>\n",
        "        </table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSyKiNti1axC"
      },
      "source": [
        "#### geoJSON\n",
        "\n",
        "Create an account at <a href=\"http://geojson.io/#map=2/20.0/0.0\">geojson.io</a> and log in. This is similar to what you should see:\n",
        "![](notebook_images/Picture1.png)\n",
        "\n",
        "Example: zoom into the Aral sea, on the border of Kazakhstan and Uzbekistan\n",
        "![](notebook_images/Picture2.png)\n",
        "\n",
        "Select a shape tool (polygon or rectangle) and define your area of interest. You should see JSON code on the right hand pane\n",
        "![](notebook_images/Picture3.png)\n",
        "\n",
        "In the save menu, save in geoJSON format. In the above example, your filename might be something like ```aral_sea.geojson```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVxYIkbD1axC"
      },
      "source": [
        "#### SciHub\n",
        "\n",
        "[Use this guide to self-registration](https://scihub.copernicus.eu/userguide/SelfRegistration) to get the access credentials you'll need to access SciHub data services for free"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM_2Jdt91axD"
      },
      "source": [
        "#### Set up your SentinelAPI oject with your SciHub credentials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeVlDD691axD"
      },
      "source": [
        "from sentinelsat.sentinel import SentinelAPI\n",
        "from sentinelsat import read_geojson, geojson_to_wkt\n",
        "import os\n",
        "\n",
        "s2_api = SentinelAPI(\n",
        "    user=\"yourusername\",\n",
        "    password=\"yourpassword\",\n",
        "    api_url=\"https://scihub.copernicus.eu/apihub/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntf8sm4h1axD"
      },
      "source": [
        "If you get download or server connection issues, try the alternative url: \n",
        "``` api_url=\"https://scihub.copernicus.eu/dhus/\"```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDsYtzpK1axD"
      },
      "source": [
        "#### Download imagery using your geoJSON file as an area of interest \n",
        "\n",
        "Download all Level 2A images collected over August 2019, with less than 10% cloud cover, and within the vicinty of the area defined in the geoJSON file\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFnGLcEQ1axE"
      },
      "source": [
        "products = s2_api.query(\n",
        "    area = geojson_to_wkt(read_geojson('poopo.geojson')),\n",
        "    date = (\"20190801\", \"20190901\"),\n",
        "    platformname = \"Sentinel-2\",\n",
        "    processinglevel = 'Level-2A',\n",
        "    cloudcoverpercentage=(0,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3xF8zG81axE"
      },
      "source": [
        "The code below is a more detailed and specific search. Use ```s2_api.query?``` to see what these additional options mean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPh6Rwsv1axE"
      },
      "source": [
        "#s2_api.query?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlBxZPdh1axF"
      },
      "source": [
        "#products = s2_api.query(\n",
        "#    area = geojson_to_wkt(read_geojson('poopo.geojson')),\n",
        "#    date = (\"20190801\", \"20191201\"),\n",
        "#    platformname = \"Sentinel-2\",\n",
        "#    processinglevel = 'Level-2A',\n",
        "#    cloudcoverpercentage=(0,10),\n",
        "#    orbitdirection='DESCENDING',\n",
        "#    sensoroperationalmode='IW',\n",
        "#    polarisationmode='VV')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9huGoNLw1axF"
      },
      "source": [
        "#### Download all imagery\n",
        "\n",
        "Warning: This will download a lot of data: 6 Sentinel-2 tiles, each over 1 GB "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBNnqYn61axF"
      },
      "source": [
        "s2_api.download_all(products)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOPiFtJD1axF"
      },
      "source": [
        "#### Work with the imagery\n",
        "\n",
        "First load libraries for finding files by pattern matching (```glob```), zipping and unzipping files (```zipfile```), and file sysem utilities for moving and deleting files (```shutil```)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3hIk2631axG"
      },
      "source": [
        "import glob, zipfile, shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uVfMIuo1axG"
      },
      "source": [
        "Use glob to find all zip files in the current directory, and print those files to screen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maHGsYVA1axG"
      },
      "source": [
        "files = glob.glob('*.zip'); print(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7__dWWx1axG"
      },
      "source": [
        "The following will unzip each zipped file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UspAuKoS1axH"
      },
      "source": [
        "def unzip_satfile(f):\n",
        "    \"\"\"\n",
        "    f = zipped file to be extracted\n",
        "    \"\"\"    \n",
        "    with zipfile.ZipFile(f, 'r') as zip_ref:\n",
        "        zip_ref.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVob5x0T1axH"
      },
      "source": [
        "Use list comprehension to unzip all files in one line of code. Warning this takes a while. Ignore a message consisting of `None` : this is normal and expected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VucgtY621axH"
      },
      "source": [
        "[unzip_satfile(f) for f in files]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbA9AFe21axH"
      },
      "source": [
        "# you can remove all the zipped folders to save disk space, if you have no further use of them\n",
        "#[os.remove(f) for f in files]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGHgR9a71axI"
      },
      "source": [
        "The following function extracts only the 10-m imagery folder, moves it to the root directory, and then delete the rest of the folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrU_GICM1axI"
      },
      "source": [
        "def tidy_folders(file):\n",
        "    \"\"\"\n",
        "    file = root directory\n",
        "    \"\"\"    \n",
        "    # search for the subdirectory within the GRANULE folder\n",
        "    subdirec = glob.glob(file+os.sep+'GRANULE'+os.sep+'*')[0].split(os.sep)[-1]\n",
        "    # the directory to move (R10m) is constructed \n",
        "    direc = file+os.sep+'GRANULE'+os.sep+subdirec+os.sep+'IMG_DATA'+os.sep+'R10m'\n",
        "    # move to the root directory\n",
        "    shutil.move(direc, '.')\n",
        "    # rename it\n",
        "    shutil.move('./R10m', file+'_R10m')\n",
        "    # delete the folder\n",
        "    shutil.rmtree(file, ignore_errors=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Blql3gl1axI"
      },
      "source": [
        "Finally, cycle through each of the folders and extract only the 10m imagery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR4PrIP01axI"
      },
      "source": [
        "for k in range(len(files)):\n",
        "    #construct a file string \n",
        "    file = os.getcwd()+os.sep+files[k].split('.zip')[0]+'.SAFE'\n",
        "    # call tidy_folders to remove unwanted imagery\n",
        "    tidy_folders(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKy-LmnO1axJ"
      },
      "source": [
        "The demonstrations in this project uses only 3-band visible (RGB) imagery, so to keep only that imagery and move all 6 images to 1 folder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ASXdemz1axJ"
      },
      "source": [
        "#create a new directory with an appropriate name\n",
        "os.mkdir('poopo_sentinel_RGB_20190801_20190901')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z4XnUdk1axJ"
      },
      "source": [
        "for k in range(len(files)):\n",
        "    file = os.getcwd()+os.sep+files[k].split('.zip')[0]+'.SAFE'\n",
        "    shutil.move(glob.glob(file+'_R10m'+os.sep+'*TC*')[0], 'poopo_sentinel_RGB_20190801_20190901')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6sOcgT71axK"
      },
      "source": [
        "Now we can delete all the remaining image folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mmtV6id1axK"
      },
      "source": [
        "for k in range(len(files)):\n",
        "    shutil.rmtree(os.getcwd()+os.sep+files[k].split('.zip')[0]+'.SAFE_R10m', ignore_errors=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWkdM6zk1axK"
      },
      "source": [
        "##### Plot a time-series of these images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFtXNMvk1axL"
      },
      "source": [
        "import rasterio\n",
        "from rasterio.merge import merge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOjaoZkG1axL"
      },
      "source": [
        "# get a list of all the jp2 files and print them to the screen\n",
        "files = sorted(glob.glob('poopo_sentinel_RGB_20190801_20190901/*.jp2'))\n",
        "print(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQvHDhr51axL"
      },
      "source": [
        "You can see 3 tile codes in the list: ```KFV```, ```KGU```, and ```KFV```. For visualizing a time-series, we'll want to separate those 3 groups of files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv105OwW1axL"
      },
      "source": [
        "# the following syntax is called 'list comprehension'\n",
        "kgv = [f for f in files if f.startswith('poopo_sentinel_RGB_20190801_20190901'+os.sep+'T19KGV')]\n",
        "kgu = [f for f in files if f.startswith('poopo_sentinel_RGB_20190801_20190901'+os.sep+'T19KGU')]\n",
        "kfv = [f for f in files if f.startswith('poopo_sentinel_RGB_20190801_20190901'+os.sep+'T19KFV')]\n",
        "print(kgv)\n",
        "print(kgu)\n",
        "print(kfv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "513sZc9N1axM"
      },
      "source": [
        "Cycle through the files and read them into an array. First we'll take a look at the size of the imagery by printing the profile of the first image in the first list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J561MtQc1axM"
      },
      "source": [
        "with rasterio.open(kgv[0], driver='JP2OpenJPEG') as dataset:\n",
        "    print(dataset.profile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FijqquRg1axM"
      },
      "source": [
        "# initialize empty arrays for kgv arrays and times\n",
        "kgv_arrays = [] ; kgv_times = []\n",
        "# cycle through each file, open file with rasterio, read it in transposed, \n",
        "# and extract the time from the filename\n",
        "for k in range(len(kgv)):\n",
        "    with rasterio.open(kgv[k], driver='JP2OpenJPEG') as dataset:\n",
        "        # append images and image times to their respective lists\n",
        "        kgv_arrays.append(dataset.read().T)\n",
        "        kgv_times.append(kgv[k].split(os.sep)[-1].split('_')[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JwYxiEt1axN"
      },
      "source": [
        "import numpy as np\n",
        "np.shape(kgv_arrays)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJaKk1wa1axN"
      },
      "source": [
        "Now we have an array of images corresponding to the same area at different times, we can make our plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdN0h2_i1axN"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk45a09A1axN"
      },
      "source": [
        "# call matplotlib inline to show plots in this cell output\n",
        "%matplotlib inline\n",
        "# visualize some of the examples\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, constrained_layout=False, figsize=(12,4))\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.imshow(kgv_arrays[i], cmap=\"gray\")\n",
        "    ax.set_title(kgv_times[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_3i_GfL1axO"
      },
      "source": [
        "The second tile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0rflRHL1axO"
      },
      "source": [
        "# initialize empty arrays for kgu arrays and times\n",
        "kgu_arrays = [] ; kgu_times = []\n",
        "for k in range(len(kgu)):\n",
        "    with rasterio.open(kgu[k], driver='JP2OpenJPEG') as dataset:\n",
        "        kgu_arrays.append(dataset.read().T)\n",
        "        kgu_times.append(kgu[k].split(os.sep)[-1].split('_')[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2abo-dAr1axO"
      },
      "source": [
        "%matplotlib inline\n",
        "# visualize some of the examples\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, constrained_layout=False, figsize=(12,4))\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.imshow(kgu_arrays[i], cmap=\"gray\")\n",
        "    ax.set_title(kgu_times[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SisHksJh1axO"
      },
      "source": [
        "And the final tile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUyI6tVb1axP"
      },
      "source": [
        "# initialize empty arrays for kfv arrays and times\n",
        "kfv_arrays = [] ; kfv_times = []\n",
        "for k in range(len(kfv)):\n",
        "    with rasterio.open(kfv[k], driver='JP2OpenJPEG') as dataset:\n",
        "        kfv_arrays.append(dataset.read().T)\n",
        "        kfv_times.append(kfv[k].split(os.sep)[-1].split('_')[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITdeiUY21axP"
      },
      "source": [
        "%matplotlib inline\n",
        "# visualize some of the examples\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, constrained_layout=False, figsize=(12,4))\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.imshow(kfv_arrays[i], cmap=\"gray\")\n",
        "    ax.set_title(kfv_times[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIH-uuWt1axP"
      },
      "source": [
        "##### Merge rasters with rasterio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hhV0nJ01axP"
      },
      "source": [
        "def merge_all(use, outfile):\n",
        "    \"\"\"\n",
        "    Use = list of filenames of RGB rasters\n",
        "    outfile = filename for a 8-bit image, with 3 layers, and LZW compression\n",
        "              containing the merged raster\n",
        "    \"\"\"\n",
        "\n",
        "    #use rasterio to open all images\n",
        "    sources = [rasterio.open(path) for path in use]\n",
        "\n",
        "    # merge the 3 images together\n",
        "    dest, out_transform = merge(sources, bounds=None, res=None, precision=7)\n",
        "\n",
        "    #save as an 8-bit image, with 3 layers, and LZW compression\n",
        "    with rasterio.open(files[0]) as src:\n",
        "        profile = src.profile\n",
        "    profile.update({'dtype':rasterio.uint8, 'count':3, 'compress':'lzw'})\n",
        "\n",
        "    #write the merged raster out to file\n",
        "    with rasterio.open(outfile, 'w', **profile) as dst:\n",
        "        dst.write(dest.astype(rasterio.uint8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krjZ1-8X1axQ"
      },
      "source": [
        "# merge the first image in the sequence for each of the 3 tiles\n",
        "use = list((kgv[0],kgu[0],kfv[0]))\n",
        "merge_all(use, 'wholescene_'+kgu_times[0]+'.tif')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwBkt4LW1axQ"
      },
      "source": [
        "# merge the second image in the sequence for each of the 3 tiles\n",
        "use = list((kgv[1],kgu[1],kfv[1]))\n",
        "merge_all(use, 'wholescene_'+kgu_times[1]+'.tif')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVsiGxyp1axQ"
      },
      "source": [
        "# deleting unnecessary variables is a good way to manage limited memory \n",
        "del dst, src, \n",
        "del kfv_arrays, kgu_arrays, kgv_arrays"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99MfbkeR1axR"
      },
      "source": [
        "##### Read the merged imagery in and display side by side"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbgOeY3Y1axR"
      },
      "source": [
        "# read both datasets in as rasterio dataset objects\n",
        "t1 = rasterio.open('wholescene_'+kgu_times[0]+'.tif')\n",
        "t2 = rasterio.open('wholescene_'+kgu_times[1]+'.tif')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxK555hN1axR"
      },
      "source": [
        "#make a list of the arrays read in and transpose to flip the channels to RGB\n",
        "arrays = [t1.read().T, t2.read().T]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WawK6klA1axR"
      },
      "source": [
        "%matplotlib inline\n",
        "# visualize \n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, constrained_layout=False, figsize=(12,4))\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.imshow(arrays[i], cmap=\"gray\")\n",
        "    ax.set_title(kgu_times[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fD3Q-V31axS"
      },
      "source": [
        "##### Finally, a note about labels\n",
        "If you have developed your own geospatial data sets in this way, you may find the [European Commission’s Global Surface Water Explorer](https://global-surface-water.appspot.com/) high-resolution label (“ground-truth”) data very useful. Using this dataset will likely require some familiarity with GIS such as [QGIS](https://qgis.org/en/site/) or geospatial processing such as [GDAL for python](https://pypi.org/project/GDAL/)"
      ]
    }
  ]
}