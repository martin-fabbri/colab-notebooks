{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tf_glue_tasks_bert.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPKRiYATl5NZjcilc556x3x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6xjncqQbllB6"},"source":["# Solving GLUE tasks using BERT"]},{"cell_type":"markdown","metadata":{"id":"yLEV7ljanHT-"},"source":["## What is the GLUE benchmark?\n","\n","The General Language Undertanding Evaluation benchmark (GLUE) is collection of datasets used for training, evaluating and analyzing NLP models. The collection consists of nine difficult and diverse tasks datasets.\n","\n","Dataset | Description | Data sample | Metric(s)  \n","--- | --- | --- | ---\n","CoLA | **Corpus of Linguistic Acceptability**<br>Is the sentence grammatically correct? | This building is than that one. <br> **= Ungrammatical** | Matthews\n","SST-2 | **Stanford Sentiment Treebank**<br>The task is to predict the sentiment of a given sentence. | The movie is funny, smart, and most of all alive.<br>**=.93056(very positive)** | Accuracy\n","MRPC | Microsoft Research Paraphrase Corpus<br>Determine whether a pair of sentences are semantically equivalent | a) Yesterday, Taiwan reported 35 new infections, briging the total number of cases to 418.<br>b) The island reported another 35 probable cases yesterday, taking its total to 418.<br>**A Paraphrase** | Accuracy/F1\n","QQP | **Quora Question Pairs2**<br>Are the two questions similar? | a) How to increase the speed of my internet connection while using a VPN?<br>b)How can Internet speed be increased by hacking through DNS?<br>**=Not similar** | Accuracy/F1\n","MNLI | **Multi-Genre Natural Language Inference**<br>Does sentence A entail or contradict sentence B?| a) Tourist information offices can be very helpful.<b>Tourist information offices are never of any help.<br>**=Contradiction** | Accuracy\n","WNLI | **Winograd Natural Language Inference**<br> Sentence B replaces sentence A's ambiguous pronoun with one of the nouns - is the corrent noun? | a) The thropy did not fit in the suitcase because it was too small.<br>b) The thropy did not fit in the suitcase because the suitcase was too small.<br>=Corrent Referent | Accuracy"]},{"cell_type":"markdown","metadata":{"id":"HBp5OcRLt6Ou"},"source":["##Setup the environment\n"]},{"cell_type":"code","metadata":{"id":"GXnp-FuYnGyJ","cellView":"form","executionInfo":{"status":"ok","timestamp":1609901262719,"user_tz":480,"elapsed":24971,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["#@title ###i. Install dependencies\n","#@markdown - tensorflow-text: text preprocessing\n","#@markdown - tf-models-official: pre-trained models hosted on tensorflow-hub\n","%%capture --no-stderr\n","!pip install -U pip\n","!pip install -Uqq tensorflow-text\n","!pip install -Uqq tf-models-official"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y1kuPVuIlY5l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609901265031,"user_tz":480,"elapsed":27275,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"439dce0e-0432-43db-c751-3fa14fd5467c"},"source":["#@title ###ii. Import packages\n","import os\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from google.colab import data_table\n","from official.nlp import optimization\n","from tensorflow.config.experimental import list_physical_devices\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import Dense, Dropout, Input\n","from tensorflow.keras.preprocessing import text_dataset_from_directory\n","\n","tf.get_logger().setLevel(\"ERROR\")\n","\n","print(\"tensorflow        \", tf.__version__)\n","print(\"tensorflow_hub    \", hub.__version__)\n","print(\"tensorflow_addons \", tfa.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["tensorflow         2.4.0\n","tensorflow_hub     0.10.0\n","tensorflow_addons  0.8.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jpLc0gnUw3KI","cellView":"form","executionInfo":{"status":"ok","timestamp":1609901276928,"user_tz":480,"elapsed":39164,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"91f2480c-0555-46c4-82ab-714f7f0466e6"},"source":["#@title ###iii. Choose Accelerator Strategy\n","#@markdown Ideally you should run this notebook using a TPU. \n","#@markdown In Colab, choose Runtime -> Change runtime type and verify that a \n","#@markdown TPU is selected.\n","accelerator_strategy = \"TPU\" #@param [\"TPU\", \"GPU\", \"None\"]\n","#@markdown The TPU strategy requires to read checkpoints directly from\n","#@markdown TFHub's cloud storage bucket. It's recommended to setup a GCP\n","#@markdown bucket store the uncompressed checkpoints.\n","tfhub_model_load_format = \"UNCOMPRESSED\"  # @param [\"UNCOMPRESSED\", \"GCP BUCKET\"]\n","os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = tfhub_model_load_format  # COLAB ONLY\n","\n","tpu_address = os.environ.get(\"COLAB_TPU_ADDR\", None)\n","#GCP TPU\n","#tpu_address = \"Insert TPU address here\"  # @param {type:\"string\"}\n","\n","if tpu_address:\n","    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n","        tpu=f\"grpc://{tpu_address}\"\n","    )\n","    tf.config.experimental_connect_to_cluster(cluster_resolver)\n","    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n","    strategy = tf.distribute.TPUStrategy(cluster_resolver)\n","    print(\"Using TPU\")\n","    print(\"TPU devices: \", tf.config.list_logical_devices(\"TPU\"))\n","elif tf.test.is_gpu_available():\n","    physical_gpus = list_physical_devices(\"GPU\")\n","    logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n","    print(\"Using GPU\")\n","    print(\n","        len(physical_gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\"\n","    )\n","else:\n","    raise ValueError(\"Running on CPU is not recommended.\")\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TPU\n","TPU devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU')]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UrdGBOIaqljp"},"source":["## Game plan\n","\n","1. Select a BERT model from the list of pre-trained models available. \n","2. Load the selected model from Tensorflow Hub.\n","3. Choose a GLUE task to solve and download the associated dataset.\n","4. Preprocess the dataset text according to the task seleted.\n","5. Fine-tune BERT.\n","6. Save the trained .\n","7. Test the saved model with our reserved ."]},{"cell_type":"markdown","metadata":{"id":"6FaXTwWDqET3"},"source":["## 1. BERT models overview\n","\n","Currently these is the family of BERT models available on Tensorflow Hub:\n","\n","- `BERT-Base`, `Uncased` the original BERT models.\n","- `Small BERTs` maintain the original architecture but with fewer and/or smaller Transformer blocks.\n","- `ALBERT` reduces the model size by sharing parameters between layers. Doesn't improve processing times.\n","- `BERT Experts` offer a choice of domain specific pre-trained models.\n","- `Electra` gets trained as a discriminator from GANs. (A must try!)\n","- `BERT with talking-heads Attention` has improved the core of the Transformers architecture."]},{"cell_type":"code","metadata":{"id":"KxunrSOg0PRD","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1609901276929,"user_tz":480,"elapsed":39158,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"d982b9b6-5979-4986-ec03-556a16e48c83"},"source":["#@title ###Choose a BERT model to fine-tune\n","#@markdown You might want to start with a small model first `sm BERT uncased` \n","#@markdown and then upgrader to a bigger model to improve accuracy.\n","\n","bert_model_name = \"sm BERT uncased\" #@param [\"sm BERT uncased\", \"albert\", \"sm electra\", \"talking heads\"]\n","\n","map_name_to_handle = {\n","    'sm BERT uncased':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n","    'albert':\n","        'https://tfhub.dev/tensorflow/albert_en_base/2',\n","    'sm electra':\n","        'https://tfhub.dev/google/electra_small/2',\n","    'talking heads':\n","        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n","}\n","\n","map_model_to_preprocess = {\n","    'sm BERT uncased':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n","    'albert':\n","        'https://tfhub.dev/tensorflow/albert_en_preprocess/2',\n","    'sm electra':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n","    'talking heads':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n","}\n","\n","tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n","tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n","\n","print(f'BERT model selected           : {tfhub_handle_encoder}')\n","print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ze7kqFa7vPnc"},"source":["## 2. Preprocess the dataset\n","\n","We will differenciate the preprocessing task in two stages.\n","\n","- During training, where we will preprocess all the dataset as part of the input pipeline before feeding this data to our model. It's a good practice to keep the CPU busy preprocessing data while the GPU/TPU is processing a batch.\n","- During inference, we will merge the processing task to our saved model to keep a consistent input performing inferences.\n","\n"]},{"cell_type":"code","metadata":{"id":"Ay2-Pc6K3JS2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609901281280,"user_tz":480,"elapsed":43502,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"d1c8e707-b602-45f7-e051-c31ac4435cc3"},"source":["#@title ###Testing text tokenization\n","text = \"Preprocessing a dataset is fun!\" #@param {type:\"string\"}\n","bert_preprocess = hub.load(tfhub_handle_preprocess)\n","tok = bert_preprocess.tokenize(tf.constant([text]))\n","print(tok)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["<tf.RaggedTensor [[[17463, 3217, 9623, 7741], [1037], [2951, 13462], [2003], [4569], [999]]]>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITmj2kvXmSeA","executionInfo":{"status":"ok","timestamp":1609901281434,"user_tz":480,"elapsed":43648,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}},"outputId":"baef9b18-69d5-41d2-fb7e-e4717db34ff9"},"source":["#@title ###Packing inputs (padding, masking, and sentence id)\n","max_sequence_length = 20 #@param {type:\"number\"}\n","\n","preprocessed_sentence = bert_preprocess.bert_pack_inputs(\n","    [tok, tok], tf.constant(max_sequence_length)\n",")\n","\n","print(f\"Keys       : {list(preprocessed_sentence.keys())}\")\n","#@markdown `input_word_ids` token ids truncated to max_sequence_length.<br>\n","print(f\"Shape      : {preprocessed_sentence['input_word_ids'].shape}\")\n","print(f\"Word Ids   : {preprocessed_sentence['input_word_ids'][0, :10]}\")\n","#@markdown `input_mask` differenciates content from padding. \n","print(f\"Input Mask : {preprocessed_sentence['input_mask'][0, :10]}\")\n","print(f\"Type Ids   : {preprocessed_sentence['input_type_ids'][0, :100]}\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Keys       : ['input_word_ids', 'input_mask', 'input_type_ids']\n","Shape      : (1, 20)\n","Word Ids   : [  101 17463  3217  9623  7741  1037  2951 13462  2003  4569]\n","Input Mask : [1 1 1 1 1 1 1 1 1 1]\n","Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SmuxZWyMwXa1"},"source":["BERT adds a \"position embedding\" to the token embedding of each input, and these come from a fixed-size lookup table. That imposes a max seq length of 512 (which is also a practival limit, due to quadratic growth of attention processing). We will default to 128."]},{"cell_type":"code","metadata":{"id":"pWk4_nMKwW5h","executionInfo":{"status":"ok","timestamp":1609902774754,"user_tz":480,"elapsed":480,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["MAX_SEQUENCE_LENGTH = 128\n","\n","\n","def make_bert_preprocess_model(\n","    sentence_features, seq_length=MAX_SEQUENCE_LENGTH\n","):\n","    \"\"\"\n","    Packs string features in the format expected by BERT.\n","\n","    Args:\n","        sentence_features: list with the names of string-values features.\n","        seq_lenght: defines the sequence length of BERT inputs.\n","\n","    Returns:\n","        A Keras model that can be called on a list or dict of string Tensors\n","        and returns a dict of tensors for input to BERT.\n","    \"\"\"\n","    input_segments = [\n","        Input(shape=(), dtype=tf.string, name=ft) for ft in sentence_features\n","    ]\n","\n","    # tokenize the text to word pieces\n","    bert_preprocess = hub.load(tfhub_handle_preprocess)\n","    tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name=\"tokenizer\")\n","    segments = [tokenizer(s) for s in input_segments]\n","\n","    # # optional: trim segments in a smart way to fit seq_length.\n","    # # simple cases: skip this step and let the next step apply a default\n","    # # truncation to approximate equal lengths.\n","    truncated_segments = segments\n","\n","    # pack inputs. The details (start/end token ids, dict of output tensors)\n","    # are model-dependent, so this gets loaded from SavedModel.\n","    packer = hub.KerasLayer(\n","        bert_preprocess.bert_pack_inputs,\n","        arguments=dict(seq_length=seq_length),\n","        name=\"packer\",\n","    )\n","\n","    model_inputs = packer(truncated_segments)\n","    return Model(input_segments, model_inputs)\n"],"execution_count":84,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bnT4Jxe-4Rz_"},"source":["Let's put our preprocessing model to the test."]},{"cell_type":"code","metadata":{"id":"HMlvWl6uqQWK","executionInfo":{"status":"ok","timestamp":1609902781722,"user_tz":480,"elapsed":3585,"user":{"displayName":"Martin Fabbri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimIntWE1SvxJeoWOj0PwvUuB_-simILm2JI8het08=s64","userId":"04899005791450850059"}}},"source":["test_preprocess_model = make_bert_preprocess_model([\"my_input1\", \"my_input2\"])"],"execution_count":85,"outputs":[]}]}